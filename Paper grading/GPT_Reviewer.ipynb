{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZ_cCzcY40gx",
        "outputId": "3b23f106-8d29-4227-a92a-61f445a64c4c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.3.7)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n"
          ]
        }
      ],
      "source": [
        "pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_key=\"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\"\n",
        "ass_id='asst_iB1itEdKVgdwlsKntqlXhbxx'"
      ],
      "metadata": {
        "id": "rB5vinZoCfq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "data = {\n",
        "    \"model\": \"gpt-3.5-turbo\",\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Say this is a test!\"}],\n",
        "    \"temperature\": 0.7\n",
        "}\n",
        "\n",
        "response = requests.post(\n",
        "    \"https://api.openai.com/v1/chat/completions\",\n",
        "    headers={\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": \"Bearer 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\"\n",
        "    },\n",
        "    json=data\n",
        ")\n",
        "\n",
        "print(response.json())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ns5HyOrd7RkP",
        "outputId": "b5aa0e79-5fce-4250-8541-80a1897ef0af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'id': 'chatcmpl-8TAjTFkjhVZAtyiZEzzgav9Dz6KFa', 'object': 'chat.completion', 'created': 1701963459, 'model': 'gpt-3.5-turbo-0613', 'choices': [{'index': 0, 'message': {'role': 'assistant', 'content': 'This is a test!'}, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 13, 'completion_tokens': 5, 'total_tokens': 18}, 'system_fingerprint': None}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=\"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\")\n",
        "\n",
        "assistant = client.beta.assistants.retrieve(\"asst_iB1itEdKVgdwlsKntqlXhbxx\")\n",
        "print(assistant)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDxPx6B796UG",
        "outputId": "959a2aa0-d25a-4d47-c2bd-8ed77ad111ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant(id='asst_iB1itEdKVgdwlsKntqlXhbxx', created_at=1701863156, description=None, file_ids=[], instructions=None, metadata={}, model='gpt-3.5-turbo-1106', name='GPT reviewer', object='assistant', tools=[ToolRetrieval(type='retrieval')])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "thread = client.beta.threads.create()\n"
      ],
      "metadata": {
        "id": "H1vrFCL7_AwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=\"I need to solve the equation `3x + 11 = 14`. Can you help me?\"\n",
        ")"
      ],
      "metadata": {
        "id": "GSEfzd3XAdPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run = client.beta.threads.runs.create(\n",
        "  thread_id=thread.id,\n",
        "  assistant_id=assistant.id,\n",
        "  instructions=\"Please address the user as Jane Doe. The user has a premium account.\"\n",
        ")"
      ],
      "metadata": {
        "id": "n4n2uMuAAiWb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = client.beta.threads.messages.list(\n",
        "  thread_id=thread.id\n",
        ")"
      ],
      "metadata": {
        "id": "ccFTF-c_A6gk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for msg in messages:\n",
        "    # 检查消息角色是助手还是用户\n",
        "    if msg.role == 'assistant':\n",
        "        print(\"助手回复:\", msg.content[0].text.value)\n",
        "    elif msg.role == 'user':\n",
        "        print(\"用户查询:\", msg.content[0].text.value)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1lqFw0kA9dl",
        "outputId": "6e21332d-1b78-4ffe-f771-0acc0e969cec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "助手回复: Sure, I can help with that. \n",
            "\n",
            "To solve the equation `3x + 11 = 14`, we can start by isolating the variable `x` on one side of the equation. We'll do this by subtracting 11 from both sides of the equation.\n",
            "\n",
            "So, the steps are:\n",
            "\n",
            "1. Start with the original equation: `3x + 11 = 14`\n",
            "2. Subtract 11 from both sides: `3x = 14 - 11` \n",
            "3. Simplify: `3x = 3`\n",
            "4. Finally, divide both sides by 3: `x = 1`\n",
            "\n",
            "Therefore, the solution to the equation `3x + 11 = 14` is `x = 1`.\n",
            "用户查询: I need to solve the equation `3x + 11 = 14`. Can you help me?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#dataset"
      ],
      "metadata": {
        "id": "ZR6xX2wm82Bf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSi3ra_DFVk6",
        "outputId": "669683fc-cadd-4f4d-9b18-3bc060749d54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (1.3.7)\n",
            "Requirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.5.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/523p\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEUIKT-uDvU1",
        "outputId": "4fe099e0-b197-48ed-a2fd-e0450e674526"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/523p\n",
            "dataset.csv  \u001b[0m\u001b[01;34mpapers\u001b[0m/  papers.zip  testing_data.jsonl  training_data.jsonl  \u001b[01;34mtxts\u001b[0m/  txts.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import tiktoken\n",
        "import os"
      ],
      "metadata": {
        "id": "FUXifIiY83sQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('dataset.csv')\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "y6IsKoHiHBdY",
        "outputId": "0f9e83f6-9527-4f24-baab-62f33b04c74b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                      Paper  Expertise\n",
              "0  4264599665522594d9ecb521dd2e1d002e85a961       5.00\n",
              "1  3db9649f2ae986cac13f3e748375f8802f9b07fc       5.00\n",
              "2  9045bf2a9c1e2b9621c69c57f991d10880e91f18       1.00\n",
              "3  5aea95e1ae78a66474051a330ded374e199b658c       5.00\n",
              "4  bb6317bbd2c4a81e94cf3d7eb1b73da246a022db       4.67"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-6c99d4f5-2122-406b-a605-5b8536dce41d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Paper</th>\n",
              "      <th>Expertise</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4264599665522594d9ecb521dd2e1d002e85a961</td>\n",
              "      <td>5.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3db9649f2ae986cac13f3e748375f8802f9b07fc</td>\n",
              "      <td>5.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9045bf2a9c1e2b9621c69c57f991d10880e91f18</td>\n",
              "      <td>1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5aea95e1ae78a66474051a330ded374e199b658c</td>\n",
              "      <td>5.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>bb6317bbd2c4a81e94cf3d7eb1b73da246a022db</td>\n",
              "      <td>4.67</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6c99d4f5-2122-406b-a605-5b8536dce41d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-6c99d4f5-2122-406b-a605-5b8536dce41d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-6c99d4f5-2122-406b-a605-5b8536dce41d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-503a369a-1e61-4a95-bae0-8444bbe2168c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-503a369a-1e61-4a95-bae0-8444bbe2168c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-503a369a-1e61-4a95-bae0-8444bbe2168c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score.\""
      ],
      "metadata": {
        "id": "gSMxSVrCAyHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import pandas as pd\n",
        "\n",
        "size = len(df)\n",
        "\n",
        "# 划分比例\n",
        "train_ratio = 0.6\n",
        "val_ratio = 0.2\n",
        "test_ratio = 0.2\n",
        "\n",
        "# 计算划分的索引\n",
        "train_end = int(size * train_ratio)\n",
        "val_end = train_end + int(size * val_ratio)\n",
        "\n",
        "# 划分数据集\n",
        "train_data = df.iloc[:train_end]\n",
        "val_data = df.iloc[train_end:val_end]\n",
        "test_data = df.iloc[val_end:]\n",
        "\n",
        "\n",
        "data = df\n",
        "# 定义读取和格式化数据的函数\n",
        "def read_and_format_paper(paper_id):\n",
        "    file_path = os.path.join(\"papers\", f\"{paper_id}.json\")\n",
        "    with open(file_path, 'r') as file:\n",
        "        data = json.load(file)\n",
        "        title = data.get(\"title\", \"\")\n",
        "        abstract = data.get(\"abstract\", \"\")\n",
        "        return title, abstract\n",
        "\n",
        "# 创建训练和测试数据\n",
        "train_data = []\n",
        "val_data = []\n",
        "test_data = []\n",
        "\n",
        "for i, row in df.iterrows():\n",
        "    title, abstract = read_and_format_paper(row['Paper'])\n",
        "    if i < train_end:  # 训练数据\n",
        "        train_data.append({\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": f'{system_prompt}'},\n",
        "                {\"role\": \"user\", \"content\": f\"Title: {title}\\nAbstract: {abstract}\"},\n",
        "                {\"role\": \"assistant\", \"content\": f\"{row['Expertise']}\"}\n",
        "            ]\n",
        "        })\n",
        "    elif train_end <= i < val_end:  # 测试数据\n",
        "        val_data.append({\n",
        "            \"messages\": [\n",
        "                {\"role\": \"system\", \"content\": f'{system_prompt}'},\n",
        "                {\"role\": \"user\", \"content\": f\"Title: {title}\\nAbstract: {abstract}\"},\n",
        "                {\"role\": \"assistant\", \"content\": f\"{row['Expertise']}\"}\n",
        "            ]\n",
        "        })\n",
        "    elif val_end <= i:\n",
        "          test_data.append({\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\",\"content\": f'{system_prompt}'},\n",
        "            {\"role\": \"user\", \"content\": f\"Title: {title}\\nAbstract: {abstract}\"},\n",
        "            {\"role\": \"assistant\", \"content\": f\"Expertise rating: {row.Expertise}\"}\n",
        "        ]\n",
        "    })\n",
        "\n",
        "\n",
        "# 输出训练集和测试集的第一个实例查看结构\n",
        "print(train_data[0] if train_data else \"No training data\")\n",
        "print(val_data[0] if val_data else \"No testing data\")\n",
        "print(test_data[0] if test_data else \"No testing data\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPXQ9938DLc3",
        "outputId": "235168fd-5033-4557-93cd-5a208f45197c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'messages': [{'role': 'system', 'content': 'You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score.'}, {'role': 'user', 'content': 'Title: Paper Matching with Local Fairness Constraints\\nAbstract: Automatically matching reviewers to papers is a crucial step of the peer review process for venues receiving thousands of submissions. Unfortunately, common paper matching algorithms often construct matchings suffering from two critical problems: (1) the group of reviewers assigned to a paper do not collectively possess sufficient expertise, and (2) reviewer workloads are highly skewed. In this paper, we propose a novel local fairness formulation of paper matching that directly addresses both of these issues. Since optimizing our formulation is not always tractable, we introduce two new algorithms, FairIR and FairFlow, for computing fair matchings that approximately optimize the new formulation. FairIR solves a relaxation of the local fairness formulation and then employs a rounding technique to construct a valid matching that provably maximizes the objective and only compromises on fairness with respect to reviewer loads and papers by a small constant. In contrast, FairFlow is not provably guaranteed to produce fair matchings, however it can be 2x as efficient as FairIR and an order of magnitude faster than matching algorithms that directly optimize for fairness. Empirically, we demonstrate that both FairIR and FairFlow improve fairness over standard matching algorithms on real conference data. Moreover, in comparison to state-of-the-art matching algorithms that optimize for fairness only, FairIR achieves higher objective scores, FairFlow achieves competitive fairness, and both are capable of more evenly allocating reviewers.'}, {'role': 'assistant', 'content': '5.0'}]}\n",
            "{'messages': [{'role': 'system', 'content': 'You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score.'}, {'role': 'user', 'content': 'Title: SLAM: A Unified Encoder for Speech and Language Modeling via Speech-Text Joint Pre-Training\\nAbstract: Unsupervised pre-training is now the predominant approach for both text and speech understanding. Self-attention models pre-trained on large amounts of unannotated data have been hugely successful when fine-tuned on downstream tasks from a variety of domains and languages. This paper takes the universality of unsupervised language pre-training one step further, by unifying speech and text pre-training within a single model. We build a single encoder with the BERT objective on unlabeled text together with the w2v-BERT objective on unlabeled speech. To further align our model representations across modalities, we leverage alignment losses, specifically Translation Language Modeling (TLM) and Speech Text Matching (STM) that make use of supervised speech-text recognition data. We demonstrate that incorporating both speech and text data during pre-training can significantly improve downstream quality on CoVoST 2 speech translation, by around 1 BLEU compared to single-modality pre-trained models, while retaining close to SotA performance on LibriSpeech and SpeechStew ASR tasks. On four GLUE tasks and text-normalization, we observe evidence of capacity limitations and interference between the two modalities, leading to degraded performance compared to an equivalent text-only model, while still being competitive with BERT. Through extensive empirical analysis we also demonstrate the importance of the choice of objective function for speech pre-training, and the beneficial effect of adding additional supervised signals on the quality of the learned representations.'}, {'role': 'assistant', 'content': '3.0'}]}\n",
            "{'messages': [{'role': 'system', 'content': 'You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score.'}, {'role': 'user', 'content': 'Title: Can We Automate Scientific Reviewing?\\nAbstract: The rapid development of science and technology has been accompanied by an exponential growth in peer-reviewed scientific publications. At the same time, the review of each paper is a laborious process that must be carried out by subject matter experts. Thus, providing high-quality reviews of this growing number of papers is a significant challenge. In this work, we ask the question “can we automate scientific reviewing?”, discussing the possibility of using state-of-the-art natural language processing (NLP) models to generate first-pass peer reviews for scientific papers. Arguably the most difficult part of this is defining what a “good” review is in the first place, so we first discuss possible evaluation measures for such reviews. We then collect a dataset of papers in the machine learning domain, annotate them with different aspects of content covered in each review, and train targeted ∗Corresponding author. summarization models that take in papers to generate reviews. Comprehensive experimental results show that system-generated reviews tend to touch upon more aspects of the paper than human-written reviews, but the generated text can suffer from lower constructiveness for all aspects except the explanation of the core ideas of the papers, which are largely factually correct. We finally summarize eight challenges in the pursuit of a good review generation system together with potential solutions, which, hopefully, will inspire more future research on this subject. We make all code, and the dataset publicly available: https://github. com/neulab/ReviewAdvisor as well as a ReviewAdvisor system: http://review.nlpedia.ai/ (See demo screenshot in A.2). The review of this paper (without TL;QR section) written by the system of this paper can be found A.1'}, {'role': 'assistant', 'content': 'Expertise rating: 4.0'}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
        "    \"\"\"返回文本字符串中的token数量。\"\"\"\n",
        "    encoding = tiktoken.get_encoding(encoding_name)\n",
        "    num_tokens = len(encoding.encode(string))\n",
        "    return num_tokens\n"
      ],
      "metadata": {
        "id": "0hbmUhe4HMIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for num,training_data_i in enumerate(training_data):\n",
        "  first_training_example = training_data_i\n",
        "\n",
        "  # 将所有消息的内容合并为一个字符串\n",
        "  training_example_text = \"\".join(message[\"content\"] for message in first_training_example[\"messages\"])\n",
        "\n",
        "  # 计算token数量\n",
        "  training_example_token_count = num_tokens_from_string(training_example_text, \"cl100k_base\")\n",
        "  print(f\"第{num}个训练实例的token数量：{training_example_token_count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phdZREWlHPyx",
        "outputId": "6ac5afc2-963d-48b3-cc66-4ba1da3e965c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "第0个训练实例的token数量：294\n",
            "第1个训练实例的token数量：298\n",
            "第2个训练实例的token数量：432\n",
            "第3个训练实例的token数量：186\n",
            "第4个训练实例的token数量：246\n",
            "第5个训练实例的token数量：200\n",
            "第6个训练实例的token数量：227\n",
            "第7个训练实例的token数量：292\n",
            "第8个训练实例的token数量：160\n",
            "第9个训练实例的token数量：324\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def save_as_jsonl(data, filename):\n",
        "    with open(filename, 'w') as file:\n",
        "        for entry in data:\n",
        "            json.dump(entry, file)\n",
        "            file.write('\\n')\n",
        "\n",
        "save_as_jsonl(train_data, 'train_data.jsonl')\n",
        "save_as_jsonl(val_data, 'val_data.jsonl')\n",
        "save_as_jsonl(test_data, 'test_data.jsonl')"
      ],
      "metadata": {
        "id": "6RmcF2lSN3-B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# fine-tune data format check"
      ],
      "metadata": {
        "id": "s_2LaPjFOuv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import tiktoken # for token counting\n",
        "import numpy as np\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "YSJQ0NBbOIKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"train_data.jsonl\"\n",
        "\n",
        "# Load the dataset\n",
        "with open(data_path, 'r', encoding='utf-8') as f:\n",
        "    dataset = [json.loads(line) for line in f]\n",
        "\n",
        "# Initial dataset stats\n",
        "print(\"Num examples:\", len(dataset))\n",
        "print(\"First example:\")\n",
        "for message in dataset[0][\"messages\"]:\n",
        "    print(message)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQf9TrM5OLW9",
        "outputId": "97fa582a-8c36-472f-dd7b-17401e2cc897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num examples: 274\n",
            "First example:\n",
            "{'role': 'system', 'content': 'You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score.'}\n",
            "{'role': 'user', 'content': 'Title: Paper Matching with Local Fairness Constraints\\nAbstract: Automatically matching reviewers to papers is a crucial step of the peer review process for venues receiving thousands of submissions. Unfortunately, common paper matching algorithms often construct matchings suffering from two critical problems: (1) the group of reviewers assigned to a paper do not collectively possess sufficient expertise, and (2) reviewer workloads are highly skewed. In this paper, we propose a novel local fairness formulation of paper matching that directly addresses both of these issues. Since optimizing our formulation is not always tractable, we introduce two new algorithms, FairIR and FairFlow, for computing fair matchings that approximately optimize the new formulation. FairIR solves a relaxation of the local fairness formulation and then employs a rounding technique to construct a valid matching that provably maximizes the objective and only compromises on fairness with respect to reviewer loads and papers by a small constant. In contrast, FairFlow is not provably guaranteed to produce fair matchings, however it can be 2x as efficient as FairIR and an order of magnitude faster than matching algorithms that directly optimize for fairness. Empirically, we demonstrate that both FairIR and FairFlow improve fairness over standard matching algorithms on real conference data. Moreover, in comparison to state-of-the-art matching algorithms that optimize for fairness only, FairIR achieves higher objective scores, FairFlow achieves competitive fairness, and both are capable of more evenly allocating reviewers.'}\n",
            "{'role': 'assistant', 'content': '5.0'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Format error checks\n",
        "format_errors = defaultdict(int)\n",
        "\n",
        "for ex in dataset:\n",
        "    if not isinstance(ex, dict):\n",
        "        format_errors[\"data_type\"] += 1\n",
        "        continue\n",
        "\n",
        "    messages = ex.get(\"messages\", None)\n",
        "    if not messages:\n",
        "        format_errors[\"missing_messages_list\"] += 1\n",
        "        continue\n",
        "\n",
        "    for message in messages:\n",
        "        if \"role\" not in message or \"content\" not in message:\n",
        "            format_errors[\"message_missing_key\"] += 1\n",
        "\n",
        "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\") for k in message):\n",
        "            format_errors[\"message_unrecognized_key\"] += 1\n",
        "\n",
        "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
        "            format_errors[\"unrecognized_role\"] += 1\n",
        "\n",
        "        content = message.get(\"content\", None)\n",
        "        function_call = message.get(\"function_call\", None)\n",
        "\n",
        "        if (not content and not function_call) or not isinstance(content, str):\n",
        "            format_errors[\"missing_content\"] += 1\n",
        "\n",
        "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
        "        format_errors[\"example_missing_assistant_message\"] += 1\n",
        "\n",
        "if format_errors:\n",
        "    print(\"Found errors:\")\n",
        "    for k, v in format_errors.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "else:\n",
        "    print(\"No errors found\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGlAKw-fOPlg",
        "outputId": "f190f4c2-fcf9-445f-ab3e-e660b797bc0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No errors found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
        "\n",
        "# not exact!\n",
        "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
        "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        num_tokens += tokens_per_message\n",
        "        for key, value in message.items():\n",
        "            num_tokens += len(encoding.encode(value))\n",
        "            if key == \"name\":\n",
        "                num_tokens += tokens_per_name\n",
        "    num_tokens += 3\n",
        "    return num_tokens\n",
        "\n",
        "def num_assistant_tokens_from_messages(messages):\n",
        "    num_tokens = 0\n",
        "    for message in messages:\n",
        "        if message[\"role\"] == \"assistant\":\n",
        "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
        "    return num_tokens\n",
        "\n",
        "def print_distribution(values, name):\n",
        "    print(f\"\\n#### Distribution of {name}:\")\n",
        "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
        "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
        "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
      ],
      "metadata": {
        "id": "z-Vp_TbEOT8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Warnings and tokens counts\n",
        "n_missing_system = 0\n",
        "n_missing_user = 0\n",
        "n_messages = []\n",
        "convo_lens = []\n",
        "assistant_message_lens = []\n",
        "\n",
        "for ex in dataset:\n",
        "    messages = ex[\"messages\"]\n",
        "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
        "        n_missing_system += 1\n",
        "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
        "        n_missing_user += 1\n",
        "    n_messages.append(len(messages))\n",
        "    convo_lens.append(num_tokens_from_messages(messages))\n",
        "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
        "\n",
        "print(\"Num examples missing system message:\", n_missing_system)\n",
        "print(\"Num examples missing user message:\", n_missing_user)\n",
        "print_distribution(n_messages, \"num_messages_per_example\")\n",
        "print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
        "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
        "n_too_long = sum(l > 4096 for l in convo_lens)\n",
        "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecpEIL-iOc48",
        "outputId": "ec5693f6-205d-422f-8bff-391ad1606fdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num examples missing system message: 0\n",
            "Num examples missing user message: 0\n",
            "\n",
            "#### Distribution of num_messages_per_example:\n",
            "min / max: 3, 3\n",
            "mean / median: 3.0, 3.0\n",
            "p5 / p95: 3.0, 3.0\n",
            "\n",
            "#### Distribution of num_total_tokens_per_example:\n",
            "min / max: 185, 1849\n",
            "mean / median: 333.76277372262774, 316.5\n",
            "p5 / p95: 245.6, 405.70000000000005\n",
            "\n",
            "#### Distribution of num_assistant_tokens_per_example:\n",
            "min / max: 3, 3\n",
            "mean / median: 3.0, 3.0\n",
            "p5 / p95: 3.0, 3.0\n",
            "\n",
            "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pricing and default n_epochs estimate\n",
        "MAX_TOKENS_PER_EXAMPLE = 4096\n",
        "\n",
        "TARGET_EPOCHS = 3\n",
        "MIN_TARGET_EXAMPLES = 100\n",
        "MAX_TARGET_EXAMPLES = 25000\n",
        "MIN_DEFAULT_EPOCHS = 1\n",
        "MAX_DEFAULT_EPOCHS = 25\n",
        "\n",
        "n_epochs = TARGET_EPOCHS\n",
        "n_train_examples = len(dataset)\n",
        "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
        "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
        "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
        "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
        "\n",
        "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
        "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
        "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
        "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYgqWJ2COgoR",
        "outputId": "3accf11f-10b4-4a70-fa5d-a423aba6dfae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset has ~91451 tokens that will be charged for during training\n",
            "By default, you'll train for 3 epochs on this dataset\n",
            "By default, you'll be charged for ~274353 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Upload files"
      ],
      "metadata": {
        "id": "8ndgIDQCPpBM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key = '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')\n",
        "\n",
        "# 上传文件\n",
        "client.files.create(\n",
        "    file=open(\"train_data.jsonl\", \"rb\"),\n",
        "    purpose=\"fine-tune\"\n",
        ")\n",
        "# 上传文件\n",
        "client.files.create(\n",
        "    file=open(\"val_data.jsonl\", \"rb\"),\n",
        "    purpose=\"fine-tune\"\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPajWFn3PoCu",
        "outputId": "3df121bd-b710-4dc8-8d8f-0889c5418351"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FileObject(id='file-HsewJ9vBL1orVcNjdsiQw0kZ', bytes=154496, created_at=1701999334, filename='val_data.jsonl', object='file', purpose='fine-tune', status='processed', status_details=None)"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# fine-tune"
      ],
      "metadata": {
        "id": "vETqP3TfVjT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# 初始化OpenAI客户端\n",
        "client = OpenAI(api_key = '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')\n",
        "train_id = 'file-lIrjntIqzpNBv3KmrgD3HqEP'\n",
        "val_id = 'file-HsewJ9vBL1orVcNjdsiQw0kZ'\n",
        "# 开始fine-tune\n",
        "fine_tune_job = client.fine_tuning.jobs.create(\n",
        "    model=\"gpt-3.5-turbo-1106\",\n",
        "    training_file=train_id,  # 训练数据文件ID\n",
        "    validation_file=val_id, # 测试（验证）数据文件ID\n",
        "    suffix='final'\n",
        ")\n"
      ],
      "metadata": {
        "id": "enAznR7_VkW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ftjob_id = 'ftjob-AVDoyWPyBjFNdtVGHlm2ipuM'\n",
        "client.fine_tuning.jobs.retrieve(ftjob_id).status"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "cbG_9kmYY5T5",
        "outputId": "e86f75bd-5951-40ff-aab4-faa3d1113fd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'succeeded'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLyGv-e_gal_",
        "outputId": "3d24213c-0379-4b8f-fc4a-d1f1d55365a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRLSZH5D0Hkq",
        "outputId": "f90ed112-a06c-4249-b5a0-f4f46ac52d3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [{'role': 'user',\n",
              "   'content': 'Title: Multimodal Conditional Image Synthesis with Product-of-Experts GANs\\nAbstract: Existing conditional image synthesis frameworks generate images based on user inputs in a single modality, such as text, segmentation, sketch, or style reference. They are often unable to leverage multimodal user inputs when available, which reduces their practicality. To address this limitation, we propose the Product-of-Experts Generative Adversarial Networks (PoE-GAN) framework, which can synthesize images conditioned on multiple input modalities or any subset of them, even the empty set. PoE-GAN consists of a productof-experts generator and a multimodal multiscale projection discriminator. Through our carefully designed training scheme, PoE-GAN learns to synthesize images with high quality and diversity. Besides advancing the state of the art in multimodal conditional image synthesis, PoE-GAN also outperforms the best existing unimodal conditional image synthesis approaches when tested in the unimodal setting. The project website is available at this link.'},\n",
              "  {'role': 'assistant', 'content': 'Expertise rating: 2.5'}]}"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Some Useful functions"
      ],
      "metadata": {
        "id": "iOi6aaKB_s-a"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8vHork6i_vlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def calculate_errors(real_ratings, predicted_ratings):\n",
        "    \"\"\"\n",
        "    计算平均误差（MAE）和均方误差（MSE）。\n",
        "\n",
        "    参数:\n",
        "    real_ratings (list of str): 实际评分列表。\n",
        "    predicted_ratings (list of str): 模型预测的评分列表。\n",
        "\n",
        "    返回:\n",
        "    tuple: 包含MAE和MSE的元组。\n",
        "    \"\"\"\n",
        "    # 将评分从字符串转换为浮点数\n",
        "    real_ratings_float = np.array([float(rating) for rating in real_ratings])\n",
        "    predicted_ratings_float = np.array([float(rating) for rating in predicted_ratings])\n",
        "\n",
        "    # 计算平均误差 (MAE)\n",
        "    mae = np.mean(np.abs(real_ratings_float - predicted_ratings_float))\n",
        "\n",
        "    # 计算均方误差 (MSE)\n",
        "    mse = np.mean((real_ratings_float - predicted_ratings_float) ** 2)\n",
        "\n",
        "    return mae, mse\n"
      ],
      "metadata": {
        "id": "uYGPu2Qb9fNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# use the fine-tuned model"
      ],
      "metadata": {
        "id": "VdU1SJhc0IvB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## demo model\n",
        "10 for train and 1 for val"
      ],
      "metadata": {
        "id": "YzgDcw6y_zgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = 'ft:gpt-3.5-turbo-1106:astalos:demo:8TITWaN1'"
      ],
      "metadata": {
        "id": "bNDVi1Nc0LLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key='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')\n",
        "model_id = 'ft:gpt-3.5-turbo-1106:astalos:demo:8TITWaN1'\n",
        "\n",
        "\n",
        "test_inputs = [{\"role\": \"user\", \"content\": item[\"messages\"][0][\"content\"]} for item in test_data]\n",
        "\n",
        "\n",
        "predicted_ratings = []\n",
        "for test_input in test_inputs:\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_id,\n",
        "        messages=[test_input]\n",
        "    )\n",
        "    predicted_rating = response.choices[0].message.content\n",
        "    predicted_ratings.append(predicted_rating)\n",
        "\n",
        "\n",
        "real_ratings = [item[\"messages\"][2][\"content\"].split(\": \")[1] for item in test_data]\n",
        "\n",
        "mae,mse = calculate_errors(real_ratings=real_ratings, predicted_ratings=predicted_ratings)\n",
        "print(mae,mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gwc68Qht2ovW",
        "outputId": "ba7d7069-b9aa-4210-9087-81ffcacf7b96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.1993 2.413417\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## base model\n",
        "none train none val"
      ],
      "metadata": {
        "id": "cGDVDNCi_5-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data[0]['messages'][:-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEiQ1kYGOU44",
        "outputId": "62fc903d-6603-4b01-b793-c750250dd38c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'role': 'system',\n",
              "  'content': 'You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score.'},\n",
              " {'role': 'user',\n",
              "  'content': 'Title: Can We Automate Scientific Reviewing?\\nAbstract: The rapid development of science and technology has been accompanied by an exponential growth in peer-reviewed scientific publications. At the same time, the review of each paper is a laborious process that must be carried out by subject matter experts. Thus, providing high-quality reviews of this growing number of papers is a significant challenge. In this work, we ask the question “can we automate scientific reviewing?”, discussing the possibility of using state-of-the-art natural language processing (NLP) models to generate first-pass peer reviews for scientific papers. Arguably the most difficult part of this is defining what a “good” review is in the first place, so we first discuss possible evaluation measures for such reviews. We then collect a dataset of papers in the machine learning domain, annotate them with different aspects of content covered in each review, and train targeted ∗Corresponding author. summarization models that take in papers to generate reviews. Comprehensive experimental results show that system-generated reviews tend to touch upon more aspects of the paper than human-written reviews, but the generated text can suffer from lower constructiveness for all aspects except the explanation of the core ideas of the papers, which are largely factually correct. We finally summarize eight challenges in the pursuit of a good review generation system together with potential solutions, which, hopefully, will inspire more future research on this subject. We make all code, and the dataset publicly available: https://github. com/neulab/ReviewAdvisor as well as a ReviewAdvisor system: http://review.nlpedia.ai/ (See demo screenshot in A.2). The review of this paper (without TL;QR section) written by the system of this paper can be found A.1'}]"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key='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')\n",
        "model_id = 'gpt-3.5-turbo-1106'\n",
        "\n",
        "\n",
        "test_inputs = [item['messages'][:-1] for item in test_data]\n",
        "\n",
        "\n",
        "predicted_ratings = []\n",
        "for test_input in test_inputs:\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_id,\n",
        "        messages=test_input\n",
        "    )\n",
        "    predicted_rating = response.choices[0].message.content\n",
        "    predicted_ratings.append(predicted_rating)\n",
        "\n",
        "\n",
        "real_ratings = [item[\"messages\"][2][\"content\"].split(\": \")[1] for item in test_data]\n",
        "\n",
        "mae,mse = calculate_errors(real_ratings=real_ratings, predicted_ratings=predicted_ratings)\n",
        "print(mae,mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoMsucGr_-or",
        "outputId": "1cfc2f48-5974-4aed-f947-a0831ebcea61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.2880434782608696 2.796195652173913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_rating"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PRGVEfzKPNcI",
        "outputId": "8743e0c9-e2b8-40f0-c171-734c67797cb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'4.50'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# final model"
      ],
      "metadata": {
        "id": "N205sXynAhIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key='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')\n",
        "model_id = 'ft:gpt-3.5-turbo-1106:astalos:final:8TKc6gvX'\n",
        "\n",
        "\n",
        "test_inputs = [item['messages'][:-1] for item in test_data]\n",
        "\n",
        "\n",
        "predicted_ratings = []\n",
        "for test_input in test_inputs:\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_id,\n",
        "        messages=test_input\n",
        "    )\n",
        "    predicted_rating = response.choices[0].message.content\n",
        "    predicted_ratings.append(predicted_rating)\n",
        "\n",
        "\n",
        "real_ratings = [item[\"messages\"][2][\"content\"].split(\": \")[1] for item in test_data]\n",
        "\n",
        "mae,mse = calculate_errors(real_ratings=real_ratings, predicted_ratings=predicted_ratings)\n",
        "print(mae,mse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GiB5VTRPXkK",
        "outputId": "cfe99e25-fffe-4caf-8a6a-2202b050acd2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.25 2.4864130434782608\n"
          ]
        }
      ]
    }
  ]
}