{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Paper Matching with Local Fairness Constraints\nAbstract: Automatically matching reviewers to papers is a crucial step of the peer review process for venues receiving thousands of submissions. Unfortunately, common paper matching algorithms often construct matchings suffering from two critical problems: (1) the group of reviewers assigned to a paper do not collectively possess sufficient expertise, and (2) reviewer workloads are highly skewed. In this paper, we propose a novel local fairness formulation of paper matching that directly addresses both of these issues. Since optimizing our formulation is not always tractable, we introduce two new algorithms, FairIR and FairFlow, for computing fair matchings that approximately optimize the new formulation. FairIR solves a relaxation of the local fairness formulation and then employs a rounding technique to construct a valid matching that provably maximizes the objective and only compromises on fairness with respect to reviewer loads and papers by a small constant. In contrast, FairFlow is not provably guaranteed to produce fair matchings, however it can be 2x as efficient as FairIR and an order of magnitude faster than matching algorithms that directly optimize for fairness. Empirically, we demonstrate that both FairIR and FairFlow improve fairness over standard matching algorithms on real conference data. Moreover, in comparison to state-of-the-art matching algorithms that optimize for fairness only, FairIR achieves higher objective scores, FairFlow achieves competitive fairness, and both are capable of more evenly allocating reviewers."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: The Low-Resource Double Bind: An Empirical Study of Pruning for Low-Resource Machine Translation\nAbstract: A \u201cbigger is better\u201d explosion in the number of parameters in deep neural networks has made it increasingly challenging to make stateof-the-art networks accessible in computerestricted environments. Compression techniques have taken on renewed importance as a way to bridge the gap. However, evaluation of the trade-offs incurred by popular compression techniques has been centered on high-resource datasets. In this work, we instead consider the impact of compression in a data-limited regime. We introduce the term low-resource double bind to refer to the co-occurrence of data limitations and compute resource constraints. This is a common setting for NLP for low-resource languages, yet the trade-offs in performance are poorly studied. Our work offers surprising insights into the relationship between capacity and generalization in data-limited regimes for the task of machine translation. Our experiments on magnitude pruning for translations from English into Yoruba, Hausa, Igbo and German show that in low-resource regimes, sparsity preserves performance on frequent sentences but has a disparate impact on infrequent ones. However, it improves robustness to out-of-distribution shifts, especially for datasets that are very distinct from the training distribution. Our findings suggest that sparsity can play a beneficial role at curbing memorization of low frequency attributes, and therefore offers a promising solution to the low-resource double bind."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Logspace Reducibility From Secret Leakage Planted Clique\nAbstract: The planted clique problem is well-studied in the context of observing, explaining, and predicting interesting computational phenomena associated with statistical problems. When equating computational efficiency with the existence of polynomial time algorithms, the computational hardness of (some variant of) the planted clique problem can be used to infer the computational hardness of a host of other statistical problems. Is this ability to transfer computational hardness from (some variant of) the planted clique problem to other statistical problems robust to changing our notion of computational efficiency to space efficiency? We answer this question affirmatively for three different statistical problems, namely Sparse PCA [BR13a], submatrix detection [MW15], and testing almost k-wise independence [AAK07]. The key challenge is that space efficient randomized reductions need to repeatedly access the randomness they use. Known reductions to these problems are all randomized and need polynomially many random bits to implement. Since we can not store polynomially many random bits in memory, it is unclear how to implement these existing reductions space efficiently. There are two ideas involved in circumventing this issue and implementing known reductions to these problems space efficiently. 1. When solving statistical problems, we can use parts of the input itself as randomness. This idea was pioneered by [GW02] in the context of derandomizing \u2018typically correct\u2019 algorithms. Our observation is that this is useful even if we do not care about derandomization, and instead care about repeatedly accessing randomness that can not be stored in working memory. 2. Secret leakage variants of the planted clique problem with appropriate secret leakage can be more useful than the standard planted clique problem when we want to use parts of the input as randomness. The idea that secret leakage is helpful in the context of polynomial time reducibility to statistical problems was introduced by [BB20]. They demonstrated the polynomial time hardness of a wide variety of statistical problems by assuming polynomial time hardness of secret leakage variants of the planted clique problem. Our observation is that leakage is useful even for the seemingly unrelated task of finding randomness in the input."}, {"role": "assistant", "content": "1.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Representation Learning on Graphs with Jumping Knowledge Networks\nAbstract: Recent deep learning approaches for representation learning on graphs follow a neighborhood aggregation procedure. We analyze some important properties of these models, and propose a strategy to overcome those. In particular, the range of \"neighboring\" nodes that a node's representation draws from strongly depends on the graph structure, analogous to the spread of a random walk. To adapt to local neighborhood properties and tasks, we explore an architecture -- jumping knowledge (JK) networks -- that flexibly leverages, for each node, different neighborhood ranges to enable better structure-aware representation. In a number of experiments on social, bioinformatics and citation networks, we demonstrate that our model achieves state-of-the-art performance. Furthermore, combining the JK framework with models like Graph Convolutional Networks, GraphSAGE and Graph Attention Networks consistently improves those models' performance."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Generalization through Memorization: Nearest Neighbor Language Models\nAbstract: We introduce $k$NN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a $k$-nearest neighbors ($k$NN) model. The nearest neighbors are computed according to distance in the pre-trained LM embedding space, and can be drawn from any text collection, including the original LM training data. Applying this augmentation to a strong Wikitext-103 LM, with neighbors drawn from the original training set, our $k$NN-LM achieves a new state-of-the-art perplexity of 15.79 - a 2.9 point improvement with no additional training. We also show that this approach has implications for efficiently scaling up to larger training sets and allows for effective domain adaptation, by simply varying the nearest neighbor datastore, again without further training. Qualitatively, the model is particularly helpful in predicting rare patterns, such as factual knowledge. Together, these results strongly suggest that learning similarity between sequences of text is easier than predicting the next word, and that nearest neighbor search is an effective approach for language modeling in the long tail."}, {"role": "assistant", "content": "4.67"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Knowledge Neurons in Pretrained Transformers\nAbstract: Large-scale pretrained language models are surprisingly good at recalling factual knowledge presented in the training corpus (Petroni et al., 2019; Jiang et al., 2020b). In this paper, we present preliminary studies on how factual knowledge is stored in pretrained Transformers by introducing the concept of knowledge neurons. Specifically, we examine the fill-in-the-blank cloze task for BERT. Given a relational fact, we propose a knowledge attribution method to identify the neurons that express the fact. We find that the activation of such knowledge neurons is positively correlated to the expression of their corresponding facts. In our case studies, we attempt to leverage knowledge neurons to edit (such as update, and erase) specific factual knowledge without fine-tuning. Our results shed light on understanding the storage of knowledge within pretrained Transformers. The code is available at https://github.com/ Hunter-DDM/knowledge-neurons."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Making Paper Reviewing Robust to Bid Manipulation Attacks\nAbstract: Most computer science conferences rely on paper bidding to assign reviewers to papers. Although paper bidding enables high-quality assignments in days of unprecedented submission numbers, it also opens the door for dishonest reviewers to adversarially influence paper reviewing assignments. Anecdotal evidence suggests that some reviewers bid on papers by \u201cfriends\u201d or colluding authors, even though these papers are outside their area of expertise, and recommend them for acceptance without considering the merit of the work. In this paper, we study the efficacy of such bid manipulation attacks and find that, indeed, they can jeopardize the integrity of the review process. We develop a novel approach for paper bidding and assignment that is much more robust against such attacks. We show empirically that our approach provides robustness even when dishonest reviewers collude, have full knowledge of the assignment system\u2019s internal workings, and have access to the system\u2019s inputs. In addition to being more robust, the quality of our paper review assignments is comparable to that of current, non-robust assignment approaches."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: REMEMBERING FOR THE RIGHT REASONS: EXPLANATIONS REDUCE\nAbstract: The goal of continual learning (CL) is to learn a sequence of tasks without suffering from the phenomenon of catastrophic forgetting. Previous work has shown that leveraging memory in the form of a replay buffer can reduce performance degradation on prior tasks. We hypothesize that forgetting can be further reduced when the model is encouraged to remember the evidence for previously made decisions. As a first step towards exploring this hypothesis, we propose a simple novel training paradigm, called Remembering for the Right Reasons (RRR), that additionally stores visual model explanations for each example in the buffer and ensures the model has \u201cthe right reasons\u201d for its predictions by encouraging its explanations to remain consistent with those used to make decisions at training time. Without this constraint, there is a drift in explanations and increase in forgetting as conventional continual learning algorithms learn new tasks. We demonstrate how RRR can be easily added to any memory or regularizationbased approach and results in reduced forgetting, and more importantly, improved model explanations. We have evaluated our approach in the standard and few-shot settings and observed a consistent improvement across various CL approaches using different architectures and techniques to generate model explanations and demonstrated our approach showing a promising connection between explainability and continual learning. Our code is available at https://github.com/ SaynaEbrahimi/Remembering-for-the-Right-Reasons."}, {"role": "assistant", "content": "4.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Theoretical Understanding of Batch-normalization: A Markov Chain Perspective\nAbstract: Batch-normalization (BN) is a key component to effectively train deep neural networks. Empirical evidence has shown that without BN, the training process is prone to unstabilities. This is however not well understood from a theoretical point of view. Leveraging tools from Markov chain theory, we show that BN has a direct effect on the rank of the pre-activation matrices of a neural network. Specifically, while deep networks without BN exhibit rank collapse and poor training performance, networks equipped with BN have a higher rank. In an extensive set of experiments on standard neural network architectures and datasets, we show that the latter quantity is a good predictor for the optimization speed of training."}, {"role": "assistant", "content": "2.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Predicting Polypharmacy Side-effects Using Knowledge Graph Embeddings.\nAbstract: Polypharmacy is the use of drug combinations and is commonly used for treating complex and terminal diseases. Despite its effectiveness in many cases, it poses high risks of adverse side effects. Polypharmacy side-effects occur due to unwanted interactions of combined drugs, and they can cause severe complications to patients which results in increasing the risks of morbidity and leading to new mortalities. The use of drug polypharmacy is currently in its early stages; thus, the knowledge of their probable side-effects is limited. This encouraged multiple works to investigate machine learning techniques to efficiently and reliably predict adverse effects of drug combinations. In this context, the Decagon model is known to provide state-of-the-art results. It models polypharmacy side-effect data as a knowledge graph and formulates finding possible adverse effects as a link prediction task over the knowledge graph. The link prediction is solved using an embedding model based on graph convolutions. Despite its effectiveness, the Decagon approach still suffers from a high rate of false positives. In this work, we propose a new knowledge graph embedding technique that uses multi-part embedding vectors to predict polypharmacy side-effects. Like in the Decagon model, we model polypharmacy side effects as a knowledge graph. However, we perform the link prediction task using an approach based on tensor decomposition. Our experimental evaluation shows that our approach outperforms the Decagon model with 12% and 16% margins in terms of the area under the ROC and precision recall curves, respectively."}, {"role": "assistant", "content": "1.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units\nAbstract: Self-supervised approaches for speech representation learning are challenged by three unique problems: (1) there are multiple sound units in each input utterance, (2) there is no lexicon of input sound units during the pre-training phase, and (3) sound units have variable lengths with no explicit segmentation. To deal with these three problems, we propose the Hidden-Unit BERT (HuBERT) approach for self-supervised speech representation learning, which utilizes an offline clustering step to provide aligned target labels for a BERT-like prediction loss. A key ingredient of our approach is applying the prediction loss over the masked regions only, which forces the model to learn a combined acoustic and language model over the continuous inputs. HuBERT relies primarily on the consistency of the unsupervised clustering step rather than the intrinsic quality of the assigned cluster labels. Starting with a simple k-means teacher of 100 clusters, and using two iterations of clustering, the HuBERT model either matches or improves upon the state-of-the-art wav2vec 2.0 performance on the Librispeech (960 h) and Libri-light (60,000 h) benchmarks with 10 min, 1 h, 10 h, 100 h, and 960 h fine-tuning subsets. Using a 1B parameter model, HuBERT shows up to 19% and 13% relative WER reduction on the more challenging dev-other and test-other evaluation subsets.12"}, {"role": "assistant", "content": "4.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: TEACh: Task-driven Embodied Agents that Chat\nAbstract: Robots operating in human spaces must be able to engage in natural language interaction, both understanding and executing instructions, and using conversation to resolve ambiguity and correct mistakes. To study this, we introduce TEACh, a dataset of over 3,000 human\u2013human, interactive dialogues to complete household tasks in simulation. A Commander with access to oracle information about a task communicates in natural language with a Follower. The Follower navigates through and interacts with the environment to complete tasks varying in complexity from MAKE COFFEE to PREPARE BREAKFAST, asking questions and getting additional information from the Commander. We propose three benchmarks using TEACh to study embodied intelligence challenges, and we evaluate initial models\u2019 abilities in dialogue understanding, language grounding, and task execution."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Adafactor: Adaptive Learning Rates with Sublinear Memory Cost\nAbstract: In several recently proposed stochastic optimization methods (e.g. RMSProp, Adam, Adadelta), parameter updates are scaled by the inverse square roots of exponential moving averages of squared past gradients. Maintaining these per-parameter second-moment estimators requires memory equal to the number of parameters. For the case of neural network weight matrices, we propose maintaining only the per-row and per-column sums of these moving averages, and estimating the per-parameter second moments based on these sums. We demonstrate empirically that this method produces similar results to the baseline. Secondly, we show that adaptive methods can produce larger-than-desired updates when the decay rate of the second moment accumulator is too slow. We propose update clipping and a gradually increasing decay rate scheme as remedies. Combining these methods and dropping momentum, we achieve comparable results to the published Adam regime in training the Transformer model on the WMT 2014 English-German machine translation task, while using very little auxiliary storage in the optimizer. Finally, we propose scaling the parameter updates based on the scale of the parameters themselves."}, {"role": "assistant", "content": "3.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Interpreting Dense Retrieval as Mixture of Topics\nAbstract: Dense Retrieval (DR) reaches state-of-the-art results in first-stage retrieval, but little is known about the mechanisms that contribute to its success. Therefore, in this work, we conduct an interpretation study of recently proposed DR models. Specifically, we first discretize the embeddings output by the document and query encoders. Based on the discrete representations, we analyze the attribution of input tokens. Both qualitative and quantitative experiments are carried out on public test collections. Results suggest that DR models pay attention to different aspects of input and extract various high-level topic representations. Therefore, we can regard the representations learned by DR models as a mixture of high-level topics."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: SARG: A Novel Semi Autoregressive Generator for Multi-turn Incomplete Utterance Restoration\nAbstract: Dialogue systems in the open domain have achieved great success due to large conversation data and the development of deep learning, but multi-turn scenarios are still a challenge because of the frequent coreference and information omission. In this paper, we investigate the incomplete utterance restoration since it has brought general improvement over multi-turn dialogue systems in recent studies. Inspired by the autoregression for generation and the sequence labeling for text editing, we propose a novel semi autoregressive generator (SARG) with the high efficiency and flexibility. Moreover, experiments on Restoration-200k show that our proposed model significantly outperforms the state-of-the-art models with faster inference speed."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: From characters to words: the turning point of BPE merges\nAbstract: The distributions of orthographic word types are very different across languages due to typological characteristics, different writing traditions and potentially other factors. The wide range of cross-linguistic diversity is still a major challenge for NLP and the study of language. We use BPE and information-theoretic measures to investigate if distributions become similar under specific levels of subword tokenization. We perform a cross-linguistic comparison, following incremental merges of BPE (we go from characters to words) for 47 diverse languages. We show that text entropy values (a feature of probability distributions) tend to converge at specific subword levels: relatively few BPE merges (around 350) lead to the most similar distributions across languages. Additionally, we analyze the interaction between subword and word-level distributions and show that our findings can be interpreted in light of the ongoing discussion regarding different types of morphological complexity."}, {"role": "assistant", "content": "3.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Dynamic Feature Selection for Dependency Parsing\nAbstract: Feature computation and exhaustive search have significantly restricted the speed of graph-based dependency parsing. We propose a faster framework of dynamic feature selection, where features are added sequentially as needed, edges are pruned early, and decisions are made online for each sentence. We model this as a sequential decision-making problem and solve it by imitation learning techniques. We test our method on 7 languages. Our dynamic parser can achieve accuracies comparable or even superior to parsers using a full set of features, while computing fewer than 30% of the feature templates."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Differentiable Reasoning over a Virtual Knowledge Base\nAbstract: We consider the task of answering complex multi-hop questions using a corpus as a virtual knowledge base (KB). In particular, we describe a neural module, DrKIT, that traverses textual data like a virtual KB, softly following paths of relations between mentions of entities in the corpus. At each step the operation uses a combination of sparse-matrix TFIDF indices and maximum inner product search (MIPS) on a special index of contextual representations. This module is differentiable, so the full system can be trained completely end-to-end using gradient based methods, starting from natural language inputs. We also describe a pretraining scheme for the index mention encoder by generating hard negative examples using existing knowledge bases. We show that DrKIT improves accuracy by 9 points on 3-hop questions in the MetaQA dataset, cutting the gap between text-based and KB-based state-of-the-art by 70%. DrKIT is also very efficient, processing upto 10x more queries per second than existing state-of-the-art QA systems."}, {"role": "assistant", "content": "3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: General-Purpose Question-Answering with Macaw\nAbstract: Despite the successes of pretrained language models, there are still few high-quality, general-purpose QA systems that are freely available. In response, we present MACAW, a versatile, generative question-answering (QA) system that we are making available to the community. MACAW is built on UnifiedQA, itself built on T5, and exhibits strong performance, zero-shot, on a wide variety of topics, including outperforming GPT-3 by over 10% (absolute) on Challenge300, a suite of 300 challenge questions, despite being an order of magnitude smaller (11 billion vs. 175 billion parameters). In addition, MACAW allows different permutations (\u201cangles\u201d) of its inputs and outputs to be used, for example MACAW can take a question and produce an answer; or take an answer and produce a question; or take an answer and question, and produce multiple-choice options. We describe the system, and illustrate a variety of question types where it produces surprisingly good answers, well outside the training setup. We also identify question classes where it still appears to struggle, offering insights into the limitations of pretrained language models. MACAW is freely available, and we hope that it proves useful to the community."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Debugging Tests for Model Explanations\nAbstract: We investigate whether post-hoc model explanations are effective for diagnosing model errors--model debugging. In response to the challenge of explaining a model's prediction, a vast array of explanation methods have been proposed. Despite increasing use, it is unclear if they are effective. To start, we categorize \\textit{bugs}, based on their source, into:~\\textit{data, model, and test-time} contamination bugs. For several explanation methods, we assess their ability to: detect spurious correlation artifacts (data contamination), diagnose mislabeled training examples (data contamination), differentiate between a (partially) re-initialized model and a trained one (model contamination), and detect out-of-distribution inputs (test-time contamination). We find that the methods tested are able to diagnose a spurious background bug, but not conclusively identify mislabeled training examples. In addition, a class of methods, that modify the back-propagation algorithm are invariant to the higher layer parameters of a deep network; hence, ineffective for diagnosing model contamination. We complement our analysis with a human subject study, and find that subjects fail to identify defective models using attributions, but instead rely, primarily, on model predictions. Taken together, our results provide guidance for practitioners and researchers turning to explanations as tools for model debugging."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\nAbstract: Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."}, {"role": "assistant", "content": "3.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Learning Fair Classifiers with Partially Annotated Group Labels\nAbstract: Recently, fairness-aware learning have become increas-ingly crucial, but most of those methods operate by assuming the availability of fully annotated demographic group labels. We emphasize that such assumption is unrealistic for real-world applications since group label annotations are expensive and can conflict with privacy issues. In this paper, we consider a more practical scenario, dubbed as Algorithmic Group Fair ness with the P artially annotated G roup labels ( Fair-PG ). We observe that the existing methods to achieve group fairness perform even worse than the vanilla training, which simply uses full data only with target labels, under Fair-PG. To address this problem, we propose a simple C onfidence-based G roup L abel assignment ( CGL ) strategy that is readily applicable to any fairness-aware learning method. CGL utilizes an auxiliary group classifier to assign pseudo group labels, where random labels are assigned to low confident samples. We first theoretically show that our method design is better than the vanilla pseudo-labeling strategy in terms of fairness criteria. Then, we empirically show on several benchmark datasets that by combining CGL and the state-of-the-art fairness-aware in-processing methods, the target accuracies and the fairness metrics can be jointly improved compared to the baselines. Furthermore, we convincingly show that CGL enables to naturally augment the given group-labeled dataset with external target label-only datasets so that both accuracy and fairness can be improved. Code is available at https: //github.com/naver-ai/cgl_fairness ."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Taking a hint: How to leverage loss predictors in contextual bandits?\nAbstract: We initiate the study of learning in contextual bandits with the help of loss predictors. The main question we address is whether one can improve over the minimax regret $\\mathcal{O}(\\sqrt{T})$ for learning over $T$ rounds, when the total error of the predictor $\\mathcal{E} \\leq T$ is relatively small. We provide a complete answer to this question, including upper and lower bounds for various settings: adversarial versus stochastic environments, known versus unknown $\\mathcal{E}$, and single versus multiple predictors. We show several surprising results, such as 1) the optimal regret is $\\mathcal{O}(\\min\\{\\sqrt{T}, \\sqrt{\\mathcal{E}}T^\\frac{1}{4}\\})$ when $\\mathcal{E}$ is known, a sharp contrast to the standard and better bound $\\mathcal{O}(\\sqrt{\\mathcal{E}})$ for non-contextual problems (such as multi-armed bandits); 2) the same bound cannot be achieved if $\\mathcal{E}$ is unknown, but as a remedy, $\\mathcal{O}(\\sqrt{\\mathcal{E}}T^\\frac{1}{3})$ is achievable; 3) with $M$ predictors, a linear dependence on $M$ is necessary, even if logarithmic dependence is possible for non-contextual problems. \nWe also develop several novel algorithmic techniques to achieve matching upper bounds, including 1) a key action remapping technique for optimal regret with known $\\mathcal{E}$, 2) implementing Catoni's robust mean estimator efficiently via an ERM oracle leading to an efficient algorithm in the stochastic setting with optimal regret, 3) constructing an underestimator for $\\mathcal{E}$ via estimating the histogram with bins of exponentially increasing size for the stochastic setting with unknown $\\mathcal{E}$, and 4) a self-referential scheme for learning with multiple predictors, all of which might be of independent interest."}, {"role": "assistant", "content": "3.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Train simultaneously, generalize better: Stability of gradient-based minimax learners\nAbstract: The success of minimax learning problems of generative adversarial networks (GANs) has been observed to depend on the minimax optimization algorithm used for their training. This dependence is commonly attributed to the convergence speed and robustness properties of the underlying optimization algorithm. In this paper, we show that the optimization algorithm also plays a key role in the generalization performance of the trained minimax model. To this end, we analyze the generalization properties of standard gradient descent ascent (GDA) and proximal point method (PPM) algorithms through the lens of algorithmic stability under both convex concave and non-convex non-concave minimax settings. While the GDA algorithm is not guaranteed to have a vanishing excess risk in convex concave problems, we show the PPM algorithm enjoys a bounded excess risk in the same setup. For non-convex non-concave problems, we compare the generalization performance of stochastic GDA and GDmax algorithms where the latter fully solves the maximization subproblem at every iteration. Our generalization analysis suggests the superiority of GDA provided that the minimization and maximization subproblems are solved simultaneously with similar learning rates. We discuss several numerical results indicating the role of optimization algorithms in the generalization of the learned minimax models."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Persistent Homology with Improved Locality Information for more Effective Delineation\nAbstract: We present a new, more effective way to use Persistent Homology (PH), a method to compare the topology of two data sets, for training deep networks to delineate road networks in aerial images and neuronal processes in microscopy scans. Its essence is in a novel filtration function, derived from a fusion of two existing techniques: thresholding-based filtration, previously used to train deep networks to segment medical images, and filtration with height functions, used before for comparison of 2D and 3D shapes. We experimentally demonstrate that deep networks trained with our Persistent-Homology-based loss yield reconstructions of road networks and neuronal processes that preserve the connectivity of the originals better than existing topological and non-topological loss functions."}, {"role": "assistant", "content": "3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: CSFCube - A Test Collection of Computer Science Research Articles for Faceted Query by Example\nAbstract: Query by Example is a well-known information retrieval task in which a document is chosen by the user as the search query and the goal is to retrieve relevant documents from a large collection. However, a document often covers multiple aspects of a topic. To address this scenario we introduce the task of faceted Query by Example in which users can also specify a finer grained aspect in addition to the input query document. We focus on the application of this task in scientific literature search. We envision models which are able to retrieve scientific papers analogous to a query scientific paper along specifically chosen rhetorical structure elements as one solution to this problem. In this work, the rhetorical structure elements, which we refer to as facets, indicate backgrounds, methods, or results of a scientific paper. We introduce and describe an expert annotated test collection to evaluate models trained to perform this task. Our test collection consists of a diverse set of 50 query documents, drawn from computational linguistics and machine learning venues. We carefully followed the annotation guideline used by TREC for depth-k pooling (k = 100 or 250) and the resulting data collection consists of graded relevance scores with high annotation agreement. The data is freely available for research purposes."}, {"role": "assistant", "content": "3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Learning from Crowds by Modeling Common Confusions\nAbstract: Crowdsourcing provides a practical way to obtain large amounts of labeled data at a low cost. However, the annotation quality of annotators varies considerably, which imposes new challenges in learning a high-quality model from the crowdsourced annotations. In this work, we provide a new perspective to decompose annotation noise into common noise and individual noise and differentiate the source of confusion based on instance difficulty and annotator expertise on a per-instance-annotator basis. We realize this new crowdsourcing model by an end-to-end learning solution with two types of noise adaptation layers: one is shared across annotators to capture their commonly shared confusions, and the other one is pertaining to each annotator to realize individual confusion. To recognize the source of noise in each annotation, we use an auxiliary network to choose from the two noise adaptation layers with respect to both instances and annotators. Extensive experiments on both synthesized and real-world benchmarks demonstrate the effectiveness of our proposed common noise adaptation solution."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Causal Effects of Linguistic Properties\nAbstract: We consider the problem of using observational data to estimate the causal effects of linguistic properties. For example, does writing a complaint politely lead to a faster response time? How much will a positive product review increase sales? This paper addresses two technical challenges related to the problem before developing a practical method. First, we formalize the causal quantity of interest as the effect of a writer\u2019s intent, and establish the assumptions necessary to identify this from observational data. Second, in practice, we only have access to noisy proxies for the linguistic properties of interest\u2014e.g., predictions from classifiers and lexicons. We propose an estimator for this setting and prove that its bias is bounded when we perform an adjustment for the text. Based on these results, we introduce TextCause, an algorithm for estimating causal effects of linguistic properties. The method leverages (1) distant supervision to improve the quality of noisy proxies, and (2) a pre-trained language model (BERT) to adjust for the text. We show that the proposed method outperforms related approaches when estimating the effect of Amazon review sentiment on semi-simulated sales figures. Finally, we present an applied case study investigating the effects of complaint politeness on bureaucratic response times."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: WILDS: A Benchmark of in-the-Wild Distribution Shifts\nAbstract: Distribution shifts can cause significant degradation in a broad range of machine learning (ML) systems deployed in the wild. However, many widely-used datasets in the ML community today were not designed for evaluating distribution shifts. These datasets typically have training and test sets drawn from the same distribution, and prior work on retrofitting them with distribution shifts has generally relied on artificial shifts that need not represent the kinds of shifts encountered in the wild. In this paper, we present WILDS, a benchmark of in-the-wild distribution shifts spanning diverse data modalities and applications, from tumor identification to wildlife monitoring to poverty mapping. WILDS builds on top of recent data collection efforts by domain experts in these applications and provides a unified collection of datasets with evaluation metrics and train/test splits that are representative of real-world distribution shifts. These datasets reflect distribution shifts arising from training and testing on different hospitals, cameras, countries, time periods, demographics, molecular scaffolds, etc., all of which cause substantial performance drops in our baseline models. Finally, we survey other applications that would be promising additions to the benchmark but for which we did not manage to find appropriate datasets; we discuss their associated challenges and detail datasets and shifts where we did not see an appreciable performance drop. By unifying datasets from a variety of application areas and making them accessible to the ML community, we hope to encourage the development of general-purpose methods that are anchored to real-world distribution shifts and that work well across different applications and problem settings. Data loaders, default models, and leaderboards are available at this https URL."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: GPL: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval\nAbstract: Dense retrieval approaches can overcome the lexical gap and lead to signi\ufb01cantly improved search results. However, they require large amounts of training data which is not available for most domains. As shown in previous work (Thakur et al., 2021b), the performance of dense retrievers severely degrades under a domain shift. This limits the usage of dense retrieval approaches to only a few domains with large training datasets. In this paper, we propose the novel unsupervised domain adaptation method Generative Pseudo Labeling (GPL), which combines a query generator with pseudo labeling from a cross-encoder. On six representative domain-specialized datasets, we \ufb01nd the proposed GPL can outperform an out-of-the-box state-of-the-art dense retrieval approach by up to 9.3 points nDCG@10. GPL requires less (unlabeled) data from the target domain and is more robust in its training than previous methods. We further investigate the role of six recent pre-training methods in the scenario of domain adaptation for retrieval tasks, where only three could yield improved results. The best approach, TSDAE (Wang et al., 2021) can be combined with GPL, yielding another average improvement of 1.4 points nDCG@10 across the six tasks. Code and models are available at https://gpl.sbert.net ."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Reinforcement Learning under Drift\nAbstract: We propose algorithms with state-of-the-art \\emph{dynamic regret} bounds for un-discounted reinforcement learning under drifting non-stationarity, where both the reward functions and state transition distributions are allowed to evolve over time. Our main contributions are: 1) A tuned Sliding Window Upper-Confidence bound for Reinforcement Learning with Confidence-Widening (\\texttt{SWUCRL2-CW}) algorithm, which attains low dynamic regret bounds against the optimal non-stationary policy in various cases. 2) The Bandit-over-Reinforcement Learning (\\texttt{BORL}) framework that further permits us to enjoy these dynamic regret bounds in a parameter-free manner."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents\nAbstract: Can world knowledge learned by large language models (LLMs) be used to act in interactive environments? In this paper, we investigate the possibility of grounding high-level tasks, expressed in natural language (e.g. \u201cmake breakfast\u201d), to a chosen set of actionable steps (e.g. \u201copen fridge\u201d). While prior work focused on learning from explicit step-by-step examples of how to act, we surprisingly find that if pre-trained LMs are large enough and prompted appropriately, they can effectively decompose high-level tasks into mid-level plans without any further training. However, the plans produced naively by LLMs often cannot map precisely to admissible actions. We propose a procedure that conditions on existing demonstrations and semantically translates the plans to admissible actions. Our evaluation in the recent VirtualHome environment shows that the resulting method substantially improves executability over the LLM baseline. The conducted human evaluation reveals a trade-off between executability and correctness but shows a promising sign towards extracting actionable knowledge from language models."}, {"role": "assistant", "content": "4.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Multi-Fact Correction in Abstractive Text Summarization\nAbstract: Pre-trained neural abstractive summarization systems have dominated extractive strategies on news summarization performance, at least in terms of ROUGE. However, system-generated abstractive summaries often face the pitfall of factual inconsistency: generating incorrect facts with respect to the source text. To address this challenge, we propose Span-Fact, a suite of two factual correction models that leverages knowledge learned from question answering models to make corrections in system-generated summaries via span selection. Our models employ single or multi-masking strategies to either iteratively or auto-regressively replace entities in order to ensure semantic consistency w.r.t. the source text, while retaining the syntactic structure of summaries generated by abstractive summarization models. Experiments show that our models significantly boost the factual consistency of system-generated summaries without sacrificing summary quality in terms of both automatic metrics and human evaluation."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: MERLOT: Multimodal Neural Script Knowledge Models\nAbstract: As humans, we understand events in the visual world contextually, performing multimodal reasoning across time to make inferences about the past, present, and future. We introduce MERLOT, a model that learns multimodal script knowledge by watching millions of YouTube videos with transcribed speech \u2013 in an entirely label-free, self-supervised manner. By pretraining with a mix of both framelevel (spatial) and video-level (temporal) objectives, our model not only learns to match images to temporally corresponding words, but also to contextualize what is happening globally over time. As a result, MERLOT exhibits strong out-of-the-box representations of temporal commonsense, and achieves state-ofthe-art performance on 12 different video QA datasets when finetuned. It also transfers well to the world of static images, allowing models to reason about the dynamic context behind visual scenes. On Visual Commonsense Reasoning, MERLOT answers questions correctly with 80.6% accuracy, outperforming stateof-the-art models of similar size by over 3%, even those that make heavy use of auxiliary supervised data (like object bounding boxes). Ablation analyses demonstrate the complementary importance of: 1) training on videos versus static images; 2) scaling the magnitude and diversity of the pretraining video corpus; and 3) using diverse objectives that encourage full-stack multimodal reasoning, from the recognition to cognition level."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Learning Transferable Visual Models From Natural Language Supervision\nAbstract: State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on."}, {"role": "assistant", "content": "3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Kernel Change-point Detection with Auxiliary Deep Generative Models\nAbstract: Detecting the emergence of abrupt property changes in time series is a challenging problem. Kernel two-sample test has been studied for this task which makes fewer assumptions on the distributions than traditional parametric approaches. However, selecting kernels is non-trivial in practice. Although kernel selection for two-sample test has been studied, the insufficient samples in change point detection problem hinder the success of those developed kernel selection algorithms. In this paper, we propose KL-CPD, a novel kernel learning framework for time series CPD that optimizes a lower bound of test power via an auxiliary generative model. With deep kernel parameterization, KL-CPD endows kernel two-sample test with the data-driven kernel to detect different types of change-points in real-world applications. The proposed approach significantly outperformed other state-of-the-art methods in our comparative evaluation of benchmark datasets and simulation studies."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Unsupervised Transcription of Historical Documents\nAbstract: We present a generative probabilistic model, inspired by historical printing processes, for transcribing images of documents from the printing press era. By jointly modeling the text of the document and the noisy (but regular) process of rendering glyphs, our unsupervised system is able to decipher font structure and more accurately transcribe images into text. Overall, our system substantially outperforms state-of-the-art solutions for this task, achieving a 31% relative reduction in word error rate over the leading commercial system for historical transcription, and a 47% relative reduction over Tesseract, Google\u2019s open source OCR system."}, {"role": "assistant", "content": "3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Chaos as an interpretable benchmark for forecasting and data-driven modelling\nAbstract: The striking fractal geometry of strange attractors underscores the generative nature of chaos: like probability distributions, chaotic systems can be repeatedly measured to produce arbitrarily-detailed information about the underlying attractor. Chaotic systems thus pose a unique challenge to modern statistical learning techniques, while retaining quanti\ufb01able mathematical properties that make them controllable and interpretable as benchmarks. Here, we present a growing database currently comprising 131 known chaotic dynamical systems spanning \ufb01elds such as astro-physics, climatology, and biochemistry. Each system is paired with precomputed multivariate and univariate time series. Our dataset has comparable scale to existing static time series databases; however, our systems can be re-integrated to produce additional datasets of arbitrary length and granularity. Our dataset is annotated with known mathematical properties of each system, and we perform feature analysis to broadly categorize the diverse dynamics present across the collection. Chaotic systems inherently challenge forecasting models, and across extensive benchmarks we correlate forecasting performance with the degree of chaos present. We also exploit the unique generative properties of our dataset in several proof-of-concept experiments: surrogate transfer learning to improve time series classi\ufb01cation, importance sampling to accelerate model training, and benchmarking symbolic regression algorithms."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Open-domain clarification question generation without question examples\nAbstract: An overarching goal of natural language processing is to enable machines to communicate seamlessly with humans. However, natural language can be ambiguous or unclear. In cases of uncertainty, humans engage in an interactive process known as repair: asking questions and seeking clarification until their uncertainty is resolved. We propose a framework for building a visually grounded question-asking model capable of producing polar (yes-no) clarification questions to resolve misunderstandings in dialogue. Our model uses an expected information gain objective to derive informative questions from an off-the-shelf image captioner without requiring any supervised question-answer data. We demonstrate our model\u2019s ability to pose questions that improve communicative success in a goal-oriented 20 questions game with synthetic and human answerers."}, {"role": "assistant", "content": "4.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Charformer: Fast Character Transformers via Gradient-based Subword Tokenization\nAbstract: State-of-the-art models in natural language processing rely on separate rigid subword tokenization algorithms, which limit their generalization ability and adaptation to new settings. In this paper, we propose a new model inductive bias that learns a subword tokenization end-to-end as part of the model. To this end, we introduce a soft gradient-based subword tokenization module (GBST) that automatically learns latent subword representations from characters in a data-driven fashion. Concretely, GBST enumerates candidate subword blocks and learns to score them in a position-wise fashion using a block scoring network. We additionally introduce CHARFORMER, a deep Transformer model that integrates GBST and operates on the byte level. Via extensive experiments on English GLUE, multilingual, and noisy text datasets, we show that CHARFORMER outperforms a series of competitive byte-level baselines while generally performing on par and sometimes outperforming subword-based models. Additionally, CHARFORMER is fast, improving the speed of both vanilla byte-level and subword-level Transformers by 28-100% while maintaining competitive quality. We believe this work paves the way for highly performant token-free models that are trained completely end-to-end."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Changing the World by Changing the Data\nAbstract: NLP community is currently investing a lot more research and resources into development of deep learning models than training data. While we have made a lot of progress, it is now clear that our models learn all kinds of spurious patterns, social biases, and annotation artifacts. Algorithmic solutions have so far had limited success. An alternative that is being actively discussed is more careful design of datasets so as to deliver specific signals. This position paper maps out the arguments for and against data curation, and argues that fundamentally the point is moot: curation already is and will be happening, and it is changing the world. The question is only how much thought we want to invest into that process."}, {"role": "assistant", "content": "3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Evidence-aware Fake News Detection with Graph Neural Networks\nAbstract: The prevalence and perniciousness of fake news has been a critical issue on the Internet, which stimulates the development of automatic fake news detection in turn. In this paper, we focus on the evidence-based fake news detection, where several evidences are utilized to probe the veracity of news (i.e., a claim). Most previous methods first employ sequential models to embed the semantic information and then capture the claim-evidence interaction based on different attention mechanisms. Despite their effectiveness, they still suffer from two main weaknesses. Firstly, due to the inherent drawbacks of sequential models, they fail to integrate the relevant information that is scattered far apart in evidences for veracity checking. Secondly, they neglect much redundant information contained in evidences that may be useless or even harmful. To solve these problems, we propose a unified Graph-based sEmantic sTructure mining framework, namely GET in short. Specifically, different from the existing work that treats claims and evidences as sequences, we model them as graph-structured data and capture the long-distance semantic dependency among dispersed relevant snippets via neighborhood propagation. After obtaining contextual semantic information, our model reduces information redundancy by performing graph structure learning. Finally, the fine-grained semantic representations are fed into the downstream claim-evidence interaction module for predictions. Comprehensive experiments have demonstrated the superiority of GET over the state-of-the-arts."}, {"role": "assistant", "content": "4.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: BLEU Might Be Guilty but References Are Not Innocent\nAbstract: The quality of automatic metrics for machine translation has been increasingly called into question, especially for high-quality systems. This paper demonstrates that, while choice of metric is important, the nature of the references is also critical. We study different methods to collect references and compare their value in automated evaluation by reporting correlation with human evaluation for a variety of systems and metrics. Motivated by the finding that typical references exhibit poor diversity, concentrating around translationese language, we develop a paraphrasing task for linguists to perform on existing reference translations, which counteracts this bias. Our method yields higher correlation with human judgment not only for the submissions of WMT 2019 English to German, but also for Back-translation and APE augmented MT output, which have been shown to have low correlation with automatic metrics using standard references. We demonstrate that our methodology improves correlation with all modern evaluation metrics we look at, including embedding-based methods. To complete this picture, we reveal that multi-reference BLEU does not improve the correlation for high quality output, and present an alternative multi-reference formulation that is more effective."}, {"role": "assistant", "content": "3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models\nAbstract: Existing neural information retrieval (IR) models have often been studied in homogeneous and narrow settings, which has considerably limited insights into their out-of-distribution (OOD) generalization capabilities. To address this, and to facilitate researchers to broadly evaluate the effectiveness of their models, we introduce Benchmarking-IR (BEIR), a robust and heterogeneous evaluation benchmark for information retrieval. We leverage a careful selection of 18 publicly available datasets from diverse text retrieval tasks and domains and evaluate 10 state-of-theart retrieval systems including lexical, sparse, dense, late-interaction and re-ranking architectures on the BEIR benchmark. Our results show BM25 is a robust baseline and re-ranking and late-interaction based models on average achieve the best zeroshot performances, however, at high computational costs. In contrast, dense and sparse-retrieval models are computationally more efficient but often underperform other approaches, highlighting the considerable room for improvement in their generalization capabilities. We hope this framework allows us to better evaluate and understand existing retrieval systems, and contributes to accelerating progress towards better robust and generalizable systems in the future. BEIR is publicly available at https://github.com/UKPLab/beir."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Pop Music Transformer: Generating Music with Rhythm and Harmony\nAbstract: The task automatic music composition entails generative modeling of music in symbolic formats such as the musical scores. By serializing a score as a sequence of MIDI-like events, recent work has demonstrated that state-of-the-art sequence models with self-attention work nicely for this task, especially for composing music with long-range coherence. In this paper, we show that sequence models can do even better when we improve the way a musical score is converted into events. The new event set, dubbed \"REMI\" (REvamped MIDI-derived events), provides sequence models a metric context for modeling the rhythmic patterns of music, while allowing for local tempo changes. Moreover, it explicitly sets up a harmonic structure and makes chord progression controllable. It also facilitates coordinating different tracks of a musical piece, such as the piano, bass and drums. With this new approach, we build a Pop Music Transformer that composes Pop piano music with a more plausible rhythmic structure than prior arts do. The code, data and pre-trained model are publicly available.\\footnote{\\url{this https URL}}"}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Extracting Training Data from Large Language Models\nAbstract: It has become common to publish large (billion parameter) language models that have been trained on private datasets. This paper demonstrates that in such settings, an adversary can perform a training data extraction attack to recover individual training examples by querying the language model. \nWe demonstrate our attack on GPT-2, a language model trained on scrapes of the public Internet, and are able to extract hundreds of verbatim text sequences from the model's training data. These extracted examples include (public) personally identifiable information (names, phone numbers, and email addresses), IRC conversations, code, and 128-bit UUIDs. Our attack is possible even though each of the above sequences are included in just one document in the training data. \nWe comprehensively evaluate our extraction attack to understand the factors that contribute to its success. For example, we find that larger models are more vulnerable than smaller models. We conclude by drawing lessons and discussing possible safeguards for training large language models."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: (Un)solving Morphological Inflection: Lemma Overlap Artificially Inflates Models' Performance\nAbstract: In the domain of Morphology, Inflection is a fundamental and important task that gained a lot of traction in recent years, mostly via SIGMORPHON\u2019s shared-tasks. With average accuracy above 0.9 over the scores of all languages, the task is considered mostly solved using relatively generic neural seq2seq models, even with little data provided. In this work, we propose to re-evaluate morphological inflection models by employing harder train-test splits that will challenge the generalization capacity of the models. In particular, as opposed to the na\u00efve split-by-form, we propose a split-by-lemma method to challenge the performance on existing benchmarks. Our experiments with the three top-ranked systems on the SIGMORPHON\u2019s 2020 shared-task show that the lemma-split presents an average drop of 30 percentage points in macro-average for the 90 languages included. The effect is most significant for low-resourced languages with a drop as high as 95 points, but even highresourced languages lose about 10 points on average. Our results clearly show that generalizing inflection to unseen lemmas is far from being solved, presenting a simple yet effective means to promote more sophisticated models."}, {"role": "assistant", "content": "4.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Extracting a Knowledge Base of Mechanisms from COVID-19 Papers\nAbstract: The COVID-19 pandemic has spawned a diverse body of scientific literature that is challenging to navigate, stimulating interest in automated tools to help find useful knowledge. We pursue the construction of a knowledge base (KB) of mechanisms\u2014a fundamental concept across the sciences, which encompasses activities, functions and causal relations, ranging from cellular processes to economic impacts. We extract this information from the natural language of scientific papers by developing a broad, unified schema that strikes a balance between relevance and breadth. We annotate a dataset of mechanisms with our schema and train a model to extract mechanism relations from papers. Our experiments demonstrate the utility of our KB in supporting interdisciplinary scientific search over COVID-19 literature, outperforming the prominent PubMed search in a study with clinical experts. Our search engine, dataset and code are publicly available."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Joint part-of-speech and dependency projection from multiple sources\nAbstract: Most previous work on annotation projection has been limited to a subset of IndoEuropean languages, using only a single source language, and projecting annotation for one task at a time. In contrast, we present an Integer Linear Programming (ILP) algorithm that simultaneously projects annotation for multiple tasks from multiple source languages, relying on parallel corpora available for hundreds of languages. When training POS taggers and dependency parsers on jointly projected POS tags and syntactic dependencies using our algorithm, we obtain better performance than a standard approach on 20/23 languages using one parallel corpus; and 18/27 languages using another."}, {"role": "assistant", "content": "3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Characterizing and addressing the issue of oversmoothing in neural autoregressive sequence modeling\nAbstract: Neural autoregressive sequence models smear the probability among many possible sequences including degenerate ones, such as empty or repetitive sequences. In this work, we tackle one specific case where the model assigns a high probability to unreasonably short sequences. We define the oversmoothing rate to quantify this issue. After confirming the high degree of oversmoothing in neural machine translation, we propose to explicitly minimize the oversmoothing rate during training. We conduct a set of experiments to study the effect of the proposed regularization on both model distribution and decoding performance. We use a neural machine translation task as the testbed and consider three different datasets of varying size. Our experiments reveal three major findings. First, we can control the oversmoothing rate of the model by tuning the strength of the regularization. Second, by enhancing the oversmoothing loss contribution, the probability and the rank of \u3008eos\u3009 token decrease heavily at positions where it is not supposed to be. Third, the proposed regularization impacts the outcome of beam search especially when a large beam is used. The degradation of translation quality (measured in BLEU) with a large beam significantly lessens with lower oversmoothing rate, but the degradation compared to smaller beam sizes remains to exist. From these observations, we conclude that the high degree of oversmoothing is the main reason behind the degenerate case of overly probable short sequences in a neural autoregressive model."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: FinQA: A Dataset of Numerical Reasoning over Financial Data\nAbstract: The sheer volume of financial statements makes it difficult for humans to access and analyze a business\u2019s financials. Robust numerical reasoning likewise faces unique challenges in this domain. In this work, we focus on answering deep questions over financial data, aiming to automate the analysis of a large corpus of financial documents. In contrast to existing tasks on general domain, the finance domain includes complex numerical reasoning and understanding of heterogeneous representations. To facilitate analytical progress, we propose a new large-scale dataset, FinQA, with Question-Answering pairs over Financial reports, written by financial experts. We also annotate the gold reasoning programs to ensure full explainability. We further introduce baselines and conduct comprehensive experiments in our dataset. The results demonstrate that popular, large, pre-trained models fall far short of expert humans in acquiring finance knowledge and in complex multi-step numerical reasoning on that knowledge. Our dataset \u2013 the first of its kind \u2013 should therefore enable significant, new community research into complex application domains. The dataset and code are publicly available at https://github.com/czyssrs/FinQA."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: What to Pre-Train on? Efficient Intermediate Task Selection\nAbstract: Intermediate task fine-tuning has been shown to culminate in large transfer gains across many NLP tasks. With an abundance of candidate datasets as well as pre-trained language models, it has become infeasible to experiment with all combinations to find the best transfer setting. In this work, we provide a comprehensive comparison of different methods for efficiently identifying beneficial tasks for intermediate transfer learning. We focus on parameter and computationally efficient adapter settings, highlight different data-availability scenarios, and provide expense estimates for each method. We experiment with a diverse set of 42 intermediate and 11 target English classification, multiple choice, question answering, and sequence tagging tasks. Our results demonstrate that efficient embedding based methods, which rely solely on the respective datasets, outperform computational expensive few-shot fine-tuning approaches. Our best methods achieve an average Regret@3 of 1% across all target tasks, demonstrating that we are able to efficiently identify the best datasets for intermediate training."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Research Challenges in Designing Differentially Private Text Generation Mechanisms\nAbstract: Accurately learning from user data while ensuring quantifiable privacy guarantees provides an opportunity to build better Machine Learning (ML) models while maintaining user trust. Recent literature has demonstrated the applicability of a generalized form of Differential Privacy to provide guarantees over text queries. Such mechanisms add privacy preserving noise to vectorial representations of text in high dimension and return a text based projection of the noisy vectors. However, these mechanisms are sub-optimal in their trade-off between privacy and utility. This is due to factors such as a fixed global sensitivity which leads to too much noise added in dense spaces while simultaneously guaranteeing protection for sensitive outliers. In this proposal paper, we describe some challenges in balancing the tradeoff between privacy and utility for these differentially private text mechanisms. At a high level, we provide two proposals: (1) a framework called LAC which defers some of the noise to a privacy amplification step and (2), an additional suite of three different techniques for calibrating the noise based on the local region around a word. Our objective in this paper is not to evaluate a single solution but to further the conversation on these challenges and chart pathways for building better mechanisms."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: I Will Have Order! Optimizing Orders for Fair Reviewer Assignment\nAbstract: Scientific advancement requires effective peer review. Papers should be reviewed by experts in the subject area, but it is equally important that reviewer quality is fairly distributed amongst papers. We model reviewer assignment as an instance of a fair allocation problem, presenting an extension of the classic round-robin mechanism, called Reviewer Round Robin (RRR). Round-robin mechanisms are a standard tool to ensure envy-free up to one item (EF1) allocations. However, fairness often comes at the cost of decreased efficiency. To overcome this challenge, we carefully select an approximately optimal round-robin order. Applying a relaxation of submodularity, \u03b3-weak submodularity, we show that greedily inserting papers into an order yields a (1 + \u03b3)-approximation to the maximum welfare attainable by our round-robin mechanism under any order. Our approach outputs highly efficient EF1 allocations for three real conference datasets, outperforming several state-of-the-art paper assignment methods in fairness, efficiency and runtime."}, {"role": "assistant", "content": "4.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Long-Term Secure Distributed Storage Using Quantum Key Distribution Network With Third-Party Verification\nAbstract: The quantum key distribution (QKD) network with Vernam's one-time pad encryption and secret sharing are powerful security tools to realize an information theoretically secure (ITS) distributed storage system. In the work of Fujiwara et al., a single-password-authenticated secret sharing (SPSS) scheme based on the QKD network and Shamirs secret sharing was experimentally demonstrated; it confirmed ITS data transmission, storage, authentication, and integrity. To achieve data integrity, an ITS message authentication code (MAC) tag is employed, and a data owner of the secret sharing performs both the MAC tag generation and verification. However, for a scenario in which the data owner and end users are different entities, the above approach may not work, since the data owner can cheat the end users. In this article, we resolve this problem by proposing an ITS integrity protection scheme employing a third-party verification with time-stamp. The ITS integrity protection is realized by two steps: integrity check by the data owner at data reconstruction and data integrity certification by the data owner, the end user, and the third-party verifier using a MAC based on universal2 hash function and random number provided from the QKD network. In addition to introducing the third-party verifier, we institute \u201ca trusted calculator,\u201d which computes shares of the data and MAC tags and sends MAC tags to the third-party verifier. The random number used in calculation is stored in the trusted calculator. We implement this scheme on the SPSS system installed in the Tokyo QKD Network."}, {"role": "assistant", "content": "2.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Adaptive Self-training for Few-shot Neural Sequence Labeling\nAbstract: Neural sequence labeling is an important technique employed for many Natural Language Processing (NLP) tasks, such as Named Entity Recognition (NER), slot tagging for dialog systems and semantic parsing. Large-scale pre-trained language models obtain very good performance on these tasks when fine-tuned on large amounts of task-specific labeled data. However, such large-scale labeled datasets are difficult to obtain for several tasks and domains due to the high cost of human annotation as well as privacy and data access constraints for sensitive user applications. This is exacerbated for sequence labeling tasks requiring such annotations at token-level. In this work, we develop techniques to address the label scarcity challenge for neural sequence labeling models. Specifically, we develop self-training and meta-learning techniques for few-shot training of neural sequence taggers, namely MetaST. While self-training serves as an effective mechanism to learn from large amounts of unlabeled data -- meta-learning helps in adaptive sample re-weighting to mitigate error propagation from noisy pseudo-labels. Extensive experiments on six benchmark datasets including two massive multilingual NER datasets and four slot tagging datasets for task-oriented dialog systems demonstrate the effectiveness of our method with around 10% improvement over state-of-the-art systems for the 10-shot setting."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: AmbigQA: Answering Ambiguous Open-domain Questions\nAbstract: Ambiguity is inherent to open-domain question answering; especially when exploring new topics, it can be difficult to ask questions that have a single, unambiguous answer. In this paper, we introduce AmbigQA, a new open-domain question answering task which involves predicting a set of question-answer pairs, where every plausible answer is paired with a disambiguated rewrite of the original question. To study this task, we construct AmbigNQ, a dataset covering 14,042 questions from NQ-open, an existing open-domain QA benchmark. We find that over half of the questions in NQ-open are ambiguous, exhibiting diverse types of ambiguity. We also present strong baseline models for AmbigQA which we show benefit from weakly supervised learning that incorporates NQ-open, strongly suggesting our new task and data will support significant future research effort. Our data is available at this https URL."}, {"role": "assistant", "content": "3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Auctions and Prediction Markets for Scientific Peer Review\nAbstract: Peer reviewed publications are considered the gold standard in certifying and disseminating ideas that a research community considers valuable. However, we identify two major drawbacks of the current system: (1) the overwhelming demand for reviewers due to a large volume of submissions, and (2) the lack of incentives for reviewers to participate and expend the necessary effort to provide high-quality reviews. In this work, we adopt a mechanism-design approach to propose improvements to the peer review process. We present a two-stage mechanism which ties together the paper submission and review process, simultaneously incentivizing high-quality reviews and high-quality submissions. In the first stage, authors participate in a VCG auction for review slots by submitting their papers along with a bid that represents their expected value for having their paper reviewed. For the second stage, we propose a novel prediction market-style mechanism (H-DIPP) building on recent work in the information elicitation literature, which incentivizes participating reviewers to provide honest and effortful reviews. The revenue raised by the Stage I auction is used in Stage II to pay reviewers based on the quality of their reviews."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: NL2Bash: A Corpus and Semantic Parser for Natural Language Interface to the Linux Operating System\nAbstract: We present new data and semantic parsing methods for the problem of mapping English sentences to Bash commands (NL2Bash). Our long-term goal is to enable any user to perform operations such as file manipulation, search, and application-specific scripting by simply stating their goals in English. We take a first step in this domain, by providing a new dataset of challenging but commonly used Bash commands and expert-written English descriptions, along with baseline methods to establish performance levels on this task."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Optimizing AI for Teamwork\nAbstract: In many high-stakes domains such as criminal justice, finance, and healthcare, AI systems may recommend actions to a human expert responsible for final decisions, a context known as AI-advised decision making. When AI practitioners deploy the most accurate system in these domains, they implicitly assume that the system will function alone in the world. We argue that the most accurate AI team-mate is not necessarily the em best teammate; for example, predictable performance is worth a slight sacrifice in AI accuracy. So, we propose training AI systems in a human-centered manner and directly optimizing for team performance. We study this proposal for a specific type of human-AI team, where the human overseer chooses to accept the AI recommendation or solve the task themselves. To optimize the team performance we maximize the team's expected utility, expressed in terms of quality of the final decision, cost of verifying, and individual accuracies. Our experiments with linear and non-linear models on real-world, high-stakes datasets show that the improvements in utility while being small and varying across datasets and parameters (such as cost of mistake), are real and consistent with our definition of team utility. We discuss the shortcoming of current optimization approaches beyond well-studied loss functions such as log-loss, and encourage future work on human-centered optimization problems motivated by human-AI collaborations."}, {"role": "assistant", "content": "4.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Breaking the Softmax Bottleneck: A High-Rank RNN Language Model\nAbstract: We formulate language modeling as a matrix factorization problem, and show that the expressiveness of Softmax-based models (including the majority of neural language models) is limited by a Softmax bottleneck. Given that natural language is highly context-dependent, this further implies that in practice Softmax with distributed word embeddings does not have enough capacity to model natural language. We propose a simple and effective method to address this issue, and improve the state-of-the-art perplexities on Penn Treebank and WikiText-2 to 47.69 and 40.68 respectively."}, {"role": "assistant", "content": "3.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Evaluating Large Language Models Trained on Code\nAbstract: We introduce Codex, a GPT language model finetuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics. Equal contribution OpenAI, San Francisco, California, USA. Anthropic AI, San Francisco, California, USA. Work performed while at OpenAI. Zipline, South San Francisco, California, USA. Work performed while at OpenAI. Correspondence to: Mark Chen <mark@openai.com>, Jerry Tworek <jt@openai.com>, Heewoo Jun <heewoo@openai.com>, Qiming Yuan <qiming@openai.com>."}, {"role": "assistant", "content": "4.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Modifying Memories in Transformer Models\nAbstract: Large Transformer models have achieved impressive performance in many natural language tasks. In particular, Transformer based language models have been shown to have great capabilities in encoding factual knowledge in their vast amount of parameters. While the tasks of improving the memorization and generalization of Transformers have been widely studied, it is not well known how to make transformers forget specific old facts and memorize new ones. In this paper, we propose a new task of \\emph{explicitly modifying specific factual knowledge in Transformer models while ensuring the model performance does not degrade on the unmodified facts}. This task is useful in many scenarios, such as updating stale knowledge, protecting privacy, and eliminating unintended biases stored in the models. We benchmarked several approaches that provide natural baseline performances on this task. This leads to the discovery of key components of a Transformer model that are especially effective for knowledge modifications. The work also provides insights into the role that different training phases (such as pretraining and fine-tuning) play towards memorization and knowledge modification."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: A Market-Inspired Bidding Scheme for Peer Review Paper Assignment\nAbstract: We propose a market-inspired bidding scheme for the assignment of paper reviews in large academic conferences. We provide an analysis of the incentives of reviewers during the bidding phase, when reviewers have both private costs and some information about the demand for each paper; and their goal is to obtain the best possible k papers for a predetermined k. We show that by assigning \u2018budgets\u2019 to reviewers and a \u2018price\u2019 for every paper that is (roughly) proportional to its demand, the best response of a reviewer is to bid sincerely, i.e., on her most favorite papers, and match the budget even when it is not enforced. This game-theoretic analysis is based on a simple, prototypical assignment algorithm. We show via extensive simulations on bidding data from real conferences, that our bidding scheme would substantially improve both the bid distribution and the resulting assignment."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations\nAbstract: The key idea behind the unsupervised learning of disentangled representations is that real-world data is generated by a few explanatory factors of variation which can be recovered by unsupervised learning algorithms. In this paper, we provide a sober look at recent progress in the field and challenge some common assumptions. We first theoretically show that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases on both the models and the data. Then, we train more than 12000 models covering most prominent methods and evaluation metrics in a reproducible large-scale experimental study on seven different data sets. We observe that while the different methods successfully enforce properties ``encouraged'' by the corresponding losses, well-disentangled models seemingly cannot be identified without supervision. Furthermore, increased disentanglement does not seem to lead to a decreased sample complexity of learning for downstream tasks. Our results suggest that future work on disentanglement learning should be explicit about the role of inductive biases and (implicit) supervision, investigate concrete benefits of enforcing disentanglement of the learned representations, and consider a reproducible experimental setup covering several data sets."}, {"role": "assistant", "content": "3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Rates of superlinear convergence for classical quasi-Newton methods\nAbstract: We present a new theoretical analysis of local superlinear convergence of the classical quasi-Newton methods from the convex Broyden class. Our analysis is based on the potential function involving the logarithm of determinant of Hessian approximation and the trace of inverse Hessian approximation. For the well-known DFP and BFGS methods, we obtain the rates of the form $\\left[\\frac L \\mu \\left(exp \\left\\{\\frac n k \\ln \\frac L \\mu\\right\\} - 1\\right)\\right]^{k/2}$ and $\\left[exp\\left\\{\\frac n k \\ln \\frac L \\mu\\right\\} - 1\\right]^{k/2}$ respectively, where k is the iteration counter, n is the dimension of the problem, \u03bc is the strong convexity parameter, and L is the Lipschitz constant of the gradient. Currently, these are the best known superlinear convergence rates for these methods. In particular, our results show that the starting moment of superlinear convergence of BFGS method depends on the logarithm of the condition number $\\frac L \u03bc$ in the worst case."}, {"role": "assistant", "content": "3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Graph Attention Networks with Positional Embeddings\nAbstract: Graph Neural Networks (GNNs) are deep learning methods which provide the current state of the art performance in node classification tasks. GNNs often assume homophily \u2013 neighboring nodes having similar features and labels\u2013, and therefore may not be at their full potential when dealing with non-homophilic graphs. In this work, we focus on addressing this limitation and enable Graph Attention Networks (GAT), a commonly used variant of GNNs, to explore the structural information within each graph locality. Inspired by the positional encoding in the Transformers, we propose a framework, termed Graph Attentional Networks with Positional Embeddings (GAT-POS), to enhance GATs with positional embeddings which capture structural and positional information of the nodes in the graph. In this framework, the positional embeddings are learned by a model predictive of the graph context, plugged into an enhanced GAT architecture, which is able to leverage both the positional and content information of each node. The model is trained jointly to optimize for the task of node classification as well as the task of predicting graph context. Experimental results show that GAT-POS reaches remarkable improvement compared to strong GNN baselines and recent structural embedding enhanced GNNs on non-homophilic graphs."}, {"role": "assistant", "content": "2.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations\nAbstract: We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data."}, {"role": "assistant", "content": "3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Hierarchical Task Learning from Language Instructions with Unified Transformers and Self-Monitoring\nAbstract: Despite recent progress, learning new tasks through language instructions remains an extremely challenging problem. On the ALFRED benchmark for task learning, the published state-of-the-art system only achieves a task success rate of less than 10% in an unseen environment, compared to the human performance of over 90%. To address this issue, this paper takes a closer look at task learning. In a departure from a widely applied end-toend architecture, we decomposed task learning into three sub-problems: sub-goal planning, scene navigation, and object manipulation; and developed a model HiTUT1 (stands for Hierarchical Tasks via Unified Transformers) that addresses each sub-problem in a unified manner to learn a hierarchical task structure. On the ALFRED benchmark, HiTUT has achieved the best performance with a remarkably higher generalization ability. In the unseen environment, HiTUT achieves over 160% performance gain in success rate compared to the previous state of the art. The explicit representation of task structures also enables an in-depth understanding of the nature of the problem and the ability of the agent, which provides insight for future benchmark development and evaluation."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: NLP From Scratch Without Large-Scale Pretraining: A Simple and Efficient Framework\nAbstract: Pretrained language models have become the standard approach for many NLP tasks due to strong performance, but they are very expensive to train. We propose a simple and efficient learning framework TLM that does not rely on large-scale pretraining1. Given some labeled task data and a large general corpus, TLM uses task data as queries to retrieve a tiny subset of the general corpus and jointly optimizes the task objective and the language modeling objective from scratch. On eight classification datasets in four domains, TLM achieves results better than or similar to pretrained language models (e.g., RoBERTa-Large) while reducing the training FLOPs by two orders of magnitude. With high accuracy and efficiency, we hope TLM will contribute to democratizing NLP and expediting its development2."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: What Do Models Learn from Question Answering Datasets?\nAbstract: While models have reached superhuman performance on popular question answering (QA) datasets such as SQuAD, they have yet to outperform humans on the task of question answering itself. In this paper, we investigate what models are really learning from QA datasets by evaluating BERT-based models across five popular QA datasets. We evaluate models on their generalizability to out-of-domain examples, responses to missing or incorrect information in datasets, and ability to handle variations in questions. We find that no single dataset is robust to all of our experiments and identify shortcomings in both datasets and evaluation methods. Following our analysis, we make recommendations for building future QA datasets that better evaluate the task of question answering."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: On The Ingredients of an Effective Zero-shot Semantic Parser\nAbstract: Semantic parsers map natural language utterances into meaning representations (e.g. programs). Such models are typically bottlenecked by the paucity of training data due to the required laborious annotation efforts. Recent studies have performed zero-shot learning by synthesizing training examples of canonical utterances and programs from a grammar, and further paraphrasing these utterances to improve linguistic diversity. However, such synthetic examples cannot fully capture patterns in real data. In this paper we analyze zero-shot parsers through the lenses of the language and logical gaps (Herzig and Berant, 2019), which quantify the discrepancy of language and programmatic patterns between the synthetic canonical examples and real-world user-issued ones. We propose bridging these gaps using improved grammars, stronger paraphrasers, and efficient learning methods using canonical examples that most likely reflect real user intents. Our model achieves strong performance on two semantic parsing benchmarks (SCHOLAR, GEO) with zero labeled data."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Is Supervised Syntactic Parsing Beneficial for Language Understanding Tasks? An Empirical Investigation\nAbstract: Traditional NLP has long held (supervised) syntactic parsing necessary for successful higher-level semantic language understanding (LU). The recent advent of end-to-end neural models, self-supervised via language modeling (LM), and their success on a wide range of LU tasks, however, questions this belief. In this work, we empirically investigate the usefulness of supervised parsing for semantic LU in the context of LM-pretrained transformer networks. Relying on the established fine-tuning paradigm, we first couple a pretrained transformer with a biaffine parsing head, aiming to infuse explicit syntactic knowledge from Universal Dependencies treebanks into the transformer. We then fine-tune the model for LU tasks and measure the effect of the intermediate parsing training (IPT) on downstream LU task performance. Results from both monolingual English and zero-shot language transfer experiments (with intermediate target-language parsing) show that explicit formalized syntax, injected into transformers through IPT, has very limited and inconsistent effect on downstream LU performance. Our results, coupled with our analysis of transformers\u2019 representation spaces before and after intermediate parsing, make a significant step towards providing answers to an essential question: how (un)availing is supervised parsing for high-level semantic natural language understanding in the era of large neural models?"}, {"role": "assistant", "content": "3.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Get Your Vitamin C! Robust Fact Verification with Contrastive Evidence\nAbstract: Typical fact verification models use retrieved written evidence to verify claims. Evidence sources, however, often change over time as more information is gathered and revised. In order to adapt, models must be sensitive to subtle differences in supporting evidence. We present VitaminC, a benchmark infused with challenging cases that require fact verification models to discern and adjust to slight factual changes. We collect over 100,000 Wikipedia revisions that modify an underlying fact, and leverage these revisions, together with additional synthetically constructed ones, to create a total of over 400,000 claim-evidence pairs. Unlike previous resources, the examples in VitaminC are contrastive, i.e., they contain evidence pairs that are nearly identical in language and content, with the exception that one supports a given claim while the other does not. We show that training using this design increases robustness\u2014improving accuracy by 10% on adversarial fact verification and 6% on adversarial natural language inference (NLI). Moreover, the structure of VitaminC leads us to define additional tasks for fact-checking resources: tagging relevant words in the evidence for verifying the claim, identifying factual revisions, and providing automatic edits via factually consistent text generation."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: A Frustratingly Easy Approach for Joint Entity and Relation Extraction\nAbstract: End-to-end relation extraction aims to identify named entities and extract relations between them simultaneously. Most recent work models these two subtasks jointly, either by unifying them in one structured prediction framework, or multi-task learning through shared representations. In this work, we describe a very simple approach for joint entity and relation extraction, and establish the new state-of-the-art on standard benchmarks (ACE04, ACE05, and SciERC). Our approach essentially builds on two independent pre-trained encoders and merely uses the entity model to provide input features for the relation model. Through a series of careful examinations, we validate the importance of learning distinct contextual representations for entities and relations, fusing entity information at the input layer of the relation model, and incorporating global context. Finally, we also present an efficient approximation to our approach which requires only one pass of both encoders at inference time, obtaining a 8-16$\\times$ speedup with a small accuracy drop."}, {"role": "assistant", "content": "4.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Hindsight: Posterior-guided training of retrievers for improved open-ended generation\nAbstract: Many text generation systems benefit from using a retriever to retrieve passages from a textual knowledge corpus (e.g., Wikipedia) and providing these passages as additional context to the generator. For open-ended generation tasks (like generating informative utterances in conversations) many varied passages may be equally relevant and we find that existing methods that jointly train the retriever and generator underperform: the retriever may not find relevant passages even amongst the top-10 and the generator may hence not learn a preference to ground its generated output in them. We propose using an additional guide retriever that is allowed to use the target output and \u201cin hindsight\u201d retrieve relevant passages during training. We model the guide retriever after the posterior distribution Q of passages given the input and the target output and train it jointly with the standard retriever and the generator by maximizing the evidence lower bound (ELBo) in expectation over Q. For informative conversations from the Wizard of Wikipedia dataset, with posterior-guided training, the retriever finds passages with higher relevance in the top-10 (23% relative improvement), the generator\u2019s responses are more grounded in the retrieved passage (19% relative improvement) and the end-to-end system produces better overall output (6.4% relative improvement)."}, {"role": "assistant", "content": "3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Enforcing Interpretability and its Statistical Impacts: Trade-offs between Accuracy and Interpretability\nAbstract: To date, there has been no formal study of the statistical cost of interpretability in machine learning. As such, the discourse around potential trade-offs is often informal and misconceptions abound. In this work, we aim to initiate a formal study of these trade-offs. A seemingly insurmountable roadblock is the lack of any agreed upon definition of interpretability. Instead, we propose a shift in perspective. Rather than attempt to define interpretability, we propose to model the \\emph{act} of \\emph{enforcing} interpretability. As a starting point, we focus on the setting of empirical risk minimization for binary classification, and view interpretability as a constraint placed on learning. That is, we assume we are given a subset of hypothesis that are deemed to be interpretable, possibly depending on the data distribution and other aspects of the context. We then model the act of enforcing interpretability as that of performing empirical risk minimization over the set of interpretable hypotheses. This model allows us to reason about the statistical implications of enforcing interpretability, using known results in statistical learning theory. Focusing on accuracy, we perform a case analysis, explaining why one may or may not observe a trade-off between accuracy and interpretability when the restriction to interpretable classifiers does or does not come at the cost of some excess statistical risk. We close with some worked examples and some open problems, which we hope will spur further theoretical development around the tradeoffs involved in interpretability."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Is Multi-Hop Reasoning Really Explainable? Towards Benchmarking Reasoning Interpretability\nAbstract: Multi-hop reasoning has been widely studied in recent years to obtain more interpretable link prediction. However, we find in experiments that many paths given by these models are actually unreasonable, while little work has been done on interpretability evaluation for them. In this paper, we propose a unified framework to quantitatively evaluate the interpretability of multi-hop reasoning models so as to advance their development. In specific, we define three metrics, including path recall, local interpretability, and global interpretability for evaluation, and design an approximate strategy to calculate these metrics using the interpretability scores of rules. We manually annotate all possible rules and establish a benchmark. In experiments, we verify the effectiveness of our benchmark. Besides, we run nine representative baselines on our benchmark, and the experimental results show that the interpretability of current multi-hop reasoning models is less satisfactory and is 51.7% lower than the upper bound given by our benchmark. Moreover, the rule-based models outperform the multi-hop reasoning models in terms of performance and interpretability, which points to a direction for future research, i.e., how to better incorporate rule information into the multi-hop reasoning model. We will publish our codes and datasets upon acceptance."}, {"role": "assistant", "content": "1.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Gradient flows on graphons: existence, convergence, continuity equations\nAbstract: Wasserstein gradient flows on probability measures have found a host of applications in various optimization problems. They typically arise as the continuum limit of exchangeable particle systems evolving by some mean-field interaction involving a gradienttype potential. However, in many problems, such as in multi-layer neural networks, the so-called particles are edge weights on large graphs whose nodes are exchangeable. Such large graphs are known to converge to continuum limits called graphons as their size grow to infinity. We show that the Euclidean gradient flow of a suitable function of the edgeweights converges to a novel continuum limit given by a curve on the space of graphons that can be appropriately described as a gradient flow or, more technically, a curve of maximal slope. Several natural functions on graphons, such as homomorphism functions and the scalar entropy, are covered by our set-up, and the examples have been worked out in detail."}, {"role": "assistant", "content": "3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Adaptive Learning in Continuous Games: Optimal Regret Bounds and Convergence to Nash Equilibrium\nAbstract: In game-theoretic learning, several agents are simultaneously following their individual interests, so the environment is non-stationary from each player\u2019s perspective. In this context, the performance of a learning algorithm is often measured by its regret. However, no-regret algorithms are not created equal in terms of game-theoretic guarantees: depending on how they are tuned, some of them may drive the system to an equilibrium, while others could produce cyclic, chaotic, or otherwise divergent trajectories. To account for this, we propose a range of no-regret policies based on optimistic mirror descent, with the following desirable properties: i) they do not require any prior tuning or knowledge of the game; ii) they all achieve O( \u221a T ) regret against arbitrary, adversarial opponents; and iii) they converge to the best response against convergent opponents. Also, if employed by all players, then iv) they guarantee O(1) social regret; while v) the induced sequence of play converges to Nash equilibrium with O(1) individual regret in all variationally stable games (a class of games that includes all monotone and convex-concave zero-sum games)."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Fairness Without Demographics in Repeated Loss Minimization\nAbstract: Machine learning models (e.g., speech recognizers) are usually trained to minimize average loss, which results in representation disparity---minority groups (e.g., non-native speakers) contribute less to the training objective and thus tend to suffer higher loss. Worse, as model accuracy affects user retention, a minority group can shrink over time. In this paper, we first show that the status quo of empirical risk minimization (ERM) amplifies representation disparity over time, which can even make initially fair models unfair. To mitigate this, we develop an approach based on distributionally robust optimization (DRO), which minimizes the worst case risk over all distributions close to the empirical distribution. We prove that this approach controls the risk of the minority group at each time step, in the spirit of Rawlsian distributive justice, while remaining oblivious to the identity of the groups. We demonstrate that DRO prevents disparity amplification on examples where ERM fails, and show improvements in minority group user satisfaction in a real-world text autocomplete task."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Adjusting the Ground Truth Annotations for Connectivity-Based Learning to Delineate\nAbstract: Deep learning-based approaches to delineating 3D structure depend on accurate annotations to train the networks. Yet, in practice, people, no matter how conscientious, have trouble precisely delineating in 3D and on a large scale, in part because the data is often hard to interpret visually and in part because the 3D interfaces are awkward to use. In this paper, we introduce a method that explicitly accounts for annotation inaccuracies. To this end, we treat the annotations as active contour models that can deform themselves while preserving their topology. This enables us to jointly train the network and correct potential errors in the original annotations. The result is an approach that boosts performance of deep networks trained with potentially inaccurate annotations."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Good Classification Measures and How to Find Them\nAbstract: Several performance measures can be used for evaluating classification results: accuracy, F-measure, and many others. Can we say that some of them are better than others, or, ideally, choose one measure that is best in all situations? To answer this question, we conduct a systematic analysis of classification performance measures: we formally define a list of desirable properties and theoretically analyze which measures satisfy which properties. We also prove an impossibility theorem: some desirable properties cannot be simultaneously satisfied. Finally, we propose a new family of measures satisfying all desirable properties except one. This family includes the Matthews correlation coefficient and a so-called symmetric balanced accuracy that was not previously used in classification literature. We believe that our systematic approach gives an important tool to practitioners for adequately evaluating classification results."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: VoroCNN: deep convolutional neural network built on 3D Voronoi tessellation of protein structures\nAbstract: MOTIVATION\nEffective use of evolutionary information has recently led to tremendous progress in computational prediction of three-dimensional (3D) structures of proteins and their complexes. Despite the progress, the accuracy of predicted structures tends to vary considerably from case to case. Since the utility of computational models depends on their accuracy, reliable estimates of deviation between predicted and native structures are of utmost importance.\n\n\nRESULTS\nFor the first time, we present a deep convolutional neural network (CNN) constructed on a Voronoi tessellation of 3D molecular structures. Despite the irregular data domain, our data representation allows us to efficiently introduce both convolution and pooling operations and train the network in an end-to-end fashion without precomputed descriptors. The resultant model, VoroCNN, predicts local qualities of 3D protein folds. The prediction results are competitive to state of the art and superior to the previous 3D CNN architectures built for the same task. We also discuss practical applications of VoroCNN, for example, in recognition of protein binding interfaces.\n\n\nAVAILABILITY\nThe model, data, and evaluation tests are available at https://team.inria.fr/nano-d/software/vorocnn/.\n\n\nSUPPLEMENTARY INFORMATION\nSupplementary data are available at Bioinformatics online."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Using Text Embeddings for Causal Inference\nAbstract: We address causal inference with text documents. For example, does adding a theorem to a paper affect its chance of acceptance? Does reporting the gender of a forum post author affect the popularity of the post? We estimate these effects from observational data, where they may be confounded by features of the text such as the subject or writing quality. Although the text suffices for causal adjustment, it is prohibitively high-dimensional. The challenge is to find a low-dimensional text representation that can be used in causal inference. A key insight is that causal adjustment requires only the aspects of text that are predictive of both the treatment and outcome. Our proposed method adapts deep language models to learn low-dimensional embeddings from text that predict these values well; these embeddings suffice for causal adjustment. We establish theoretical properties of this method. We study it empirically on semi-simulated and real data on paper acceptance and forum post popularity. Code is available at this https URL."}, {"role": "assistant", "content": "3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Underspecification Presents Challenges for Credibility in Modern Machine Learning\nAbstract: ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Large Dual Encoders Are Generalizable Retrievers\nAbstract: It has been shown that dual encoders trained on one domain often fail to generalize to other domains for retrieval tasks. One widespread belief is that the bottleneck layer of a dual encoder, where the final score is simply a dotproduct between a query vector and a passage vector, is too limited to make dual encoders an effective retrieval model for out-ofdomain generalization. In this paper, we challenge this belief by scaling up the size of the dual encoder model while keeping the bottleneck embedding size fixed. With multi-stage training, surprisingly, scaling up the model size brings significant improvement on a variety of retrieval tasks, especially for out-ofdomain generalization. Experimental results show that our dual encoders, Generalizable T5-based dense Retrievers (GTR), outperform existing sparse and dense retrievers on the BEIR dataset (Thakur et al., 2021) significantly. Most surprisingly, our ablation study finds that GTR is very data efficient, as it only needs 10% of MS Marco supervised data to achieve the best out-of-domain performance.1"}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Improved Algorithms for Linear Stochastic Bandits\nAbstract: We improve the theoretical analysis and empirical performance of algorithms for the stochastic multi-armed bandit problem and the linear stochastic multi-armed bandit problem. In particular, we show that a simple modification of Auer's UCB algorithm (Auer, 2002) achieves with high probability constant regret. More importantly, we modify and, consequently, improve the analysis of the algorithm for the for linear stochastic bandit problem studied by Auer (2002), Dani et al. (2008), Rusmevichientong and Tsitsiklis (2010), Li et al. (2010). Our modification improves the regret bound by a logarithmic factor, though experiments show a vast improvement. In both cases, the improvement stems from the construction of smaller confidence sets. For their construction we use a novel tail inequality for vector-valued martingales."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Depth Separation for Neural Networks\nAbstract: Let $f:\\mathbb{S}^{d-1}\\times \\mathbb{S}^{d-1}\\to\\mathbb{S}$ be a function of the form $f(\\mathbf{x},\\mathbf{x}') = g(\\langle\\mathbf{x},\\mathbf{x}'\\rangle)$ for $g:[-1,1]\\to \\mathbb{R}$. We give a simple proof that shows that poly-size depth two neural networks with (exponentially) bounded weights cannot approximate $f$ whenever $g$ cannot be approximated by a low degree polynomial. Moreover, for many $g$'s, such as $g(x)=\\sin(\\pi d^3x)$, the number of neurons must be $2^{\\Omega\\left(d\\log(d)\\right)}$. Furthermore, the result holds w.r.t.\\ the uniform distribution on $\\mathbb{S}^{d-1}\\times \\mathbb{S}^{d-1}$. As many functions of the above form can be well approximated by poly-size depth three networks with poly-bounded weights, this establishes a separation between depth two and depth three networks w.r.t.\\ the uniform distribution on $\\mathbb{S}^{d-1}\\times \\mathbb{S}^{d-1}$."}, {"role": "assistant", "content": "1.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Explaining Answers with Entailment Trees\nAbstract: Our goal, in the context of open-domain textual question-answering (QA), is to explain answers by showing the line of reasoning from what is known to the answer, rather than simply showing a fragment of textual evidence (a \u201crationale\u201d). If this could be done, new opportunities for understanding and debugging the system\u2019s reasoning become possible. Our approach is to generate explanations in the form of entailment trees, namely a tree of multipremise entailment steps from facts that are known, through intermediate conclusions, to the hypothesis of interest (namely the question + answer). To train a model with this skill, we created ENTAILMENTBANK, the first dataset to contain multistep entailment trees. Given a hypothesis (question + answer), we define three increasingly difficult explanation tasks: generate a valid entailment tree given (a) all relevant sentences (b) all relevant and some irrelevant sentences, or (c) a corpus. We show that a strong language model can partially solve these tasks, in particular when the relevant sentences are included in the input (e.g., 35% of trees for (a) are perfect), and with indications of generalization to other domains. This work is significant as it provides a new type of dataset (multistep entailments) and baselines, offering a new avenue for the community to generate richer, more systematic explanations."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: LEMON: Language-Based Environment Manipulation via Execution-Guided Pre-training\nAbstract: Language-based environment manipulation requires agents to manipulate the environment following natural language instructions, which is challenging due to the huge space of the environments. To address this challenge, various approaches have been proposed in recent work. Although these approaches work well for their intended environments, they are difficult to generalize across environments. In this work, we propose LEMON, a general framework for language-based environment manipulation tasks. Specifically, we first propose a unified approach to deal with various environments using the same generative language model. Then we propose an execution-guided pre-training strategy to inject prior knowledge of environments to the language model with a pure synthetic pre-training corpus. Experimental results on tasks including ALCHEMY, SCENE, TANGRAMS and PROPARA demonstrate the effectiveness of LEMON: it achieves new state-of-the-art results on ALCHEMY, SCENE and PROPARA, and the execution-guided pretraining strategy brings remarkable improvements on all experimental tasks."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Does My Multimodal Model Learn Cross-modal Interactions? It\u2019s Harder to Tell than You Might Think!\nAbstract: Modeling expressive cross-modal interactions seems crucial in multimodal tasks, such as visual question answering. However, sometimes high-performing black-box algorithms turn out to be mostly exploiting unimodal signals in the data. We propose a new diagnostic tool, empirical multimodally-additive function projection (EMAP), for isolating whether or not cross-modal interactions improve performance for a given model on a given task. This function projection modifies model predictions so that cross-modal interactions are eliminated, isolating the additive, unimodal structure. For seven image+text classification tasks (on each of which we set new state-of-the-art benchmarks), we find that, in many cases, removing cross-modal interactions results in little to no performance degradation. Surprisingly, this holds even when expressive models, with capacity to consider interactions, otherwise outperform less expressive models; thus, performance improvements, even when present, often cannot be attributed to consideration of cross-modal feature interactions. We hence recommend that researchers in multimodal machine learning report the performance not only of unimodal baselines, but also the EMAP of their best-performing model."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Soft Labels for Ordinal Regression\nAbstract: Ordinal regression attempts to solve classification problems in which categories are not independent, but rather follow a natural order. It is crucial to classify each class correctly while learning adequate interclass ordinal relationships. We present a simple and effective method that constrains these relationships among categories by seamlessly incorporating metric penalties into ground truth label representations. This encoding allows deep neural networks to automatically learn intraclass and interclass relationships without any explicit modification of the network architecture. Our method converts data labels into soft probability distributions that pair well with common categorical loss functions such as cross-entropy. We show that this approach is effective by using off-the-shelf classification and segmentation networks in four wildly different scenarios: image quality ranking, age estimation, horizon line regression, and monocular depth estimation. We demonstrate that our general-purpose method is very competitive with respect to specialized approaches, and adapts well to a variety of different network architectures and metrics."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Meta-Learning for Low-Resource Neural Machine Translation\nAbstract: In this paper, we propose to extend the recently introduced model-agnostic meta-learning algorithm (MAML, Finn, et al., 2017) for low-resource neural machine translation (NMT). We frame low-resource translation as a meta-learning problem where we learn to adapt to low-resource languages based on multilingual high-resource language tasks. We use the universal lexical representation (Gu et al., 2018b) to overcome the input-output mismatch across different languages. We evaluate the proposed meta-learning strategy using eighteen European languages (Bg, Cs, Da, De, El, Es, Et, Fr, Hu, It, Lt, Nl, Pl, Pt, Sk, Sl, Sv and Ru) as source tasks and five diverse languages (Ro,Lv, Fi, Tr and Ko) as target tasks. We show that the proposed approach significantly outperforms the multilingual, transfer learning based approach (Zoph et al., 2016) and enables us to train a competitive NMT system with only a fraction of training examples. For instance, the proposed approach can achieve as high as 22.04 BLEU on Romanian-English WMT\u201916 by seeing only 16,000 translated words (~600 parallel sentences)"}, {"role": "assistant", "content": "1.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Latent Equilibrium: A unified learning theory for arbitrarily fast computation with arbitrarily slow neurons\nAbstract: The response time of physical computational elements is finite, and neurons are no exception. In hierarchical models of cortical networks each layer thus introduces a response lag. This inherent property of physical dynamical systems results in delayed processing of stimuli and causes a timing mismatch between network output and instructive signals, thus afflicting not only inference, but also learning. We introduce Latent Equilibrium, a new framework for inference and learning in networks of slow components which avoids these issues by harnessing the ability of biological neurons to phase-advance their output with respect to their membrane potential. This principle enables quasi-instantaneous inference independent of network depth and avoids the need for phased plasticity or computationally expensive network relaxation phases. We jointly derive disentangled neuron and synapse dynamics from a prospective energy function that depends on a network\u2019s generalized position and momentum. The resulting model can be interpreted as a biologically plausible approximation of error backpropagation in deep cortical networks with continuous-time, leaky neuronal dynamics and continuously active, local plasticity. We demonstrate successful learning of standard benchmark datasets, achieving competitive performance using both fully-connected and convolutional architectures, and show how our principle can be applied to detailed models of cortical microcircuitry. Furthermore, we study the robustness of our model to spatio-temporal substrate imperfections to demonstrate its feasibility for physical realization, be it in vivo or in silico."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Program Synthesis with Pragmatic Communication\nAbstract: Program synthesis techniques construct or infer programs from user-provided specifications, such as input-output examples. Yet most specifications, especially those given by end-users, leave the synthesis problem radically ill-posed, because many programs may simultaneously satisfy the specification. Prior work resolves this ambiguity by using various inductive biases, such as a preference for simpler programs. This work introduces a new inductive bias derived by modeling the program synthesis task as rational communication, drawing insights from recursive reasoning models of pragmatics. Given a specification, we score a candidate program both on its consistency with the specification, and also whether a rational speaker would chose this particular specification to communicate that program. We develop efficient algorithms for such an approach when learning from input-output examples, and build a pragmatic program synthesizer over a simple grid-like layout domain. A user study finds that end-user participants communicate more effectively with the pragmatic program synthesizer over a non-pragmatic one."}, {"role": "assistant", "content": "3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Zero-Resource Cross-Lingual Named Entity Recognition\nAbstract: Recently, neural methods have achieved state-of-the-art (SOTA) results in Named Entity Recognition (NER) tasks for many languages without the need for manually crafted features. However, these models still require manually annotated training data, which is not available for many languages. In this paper, we propose an unsupervised cross-lingual NER model that can transfer NER knowledge from one language to another in a completely unsupervised way without relying on any bilingual dictionary or parallel data. Our model achieves this through word-level adversarial learning and augmented fine-tuning with parameter sharing and feature augmentation. Experiments on five different languages demonstrate the effectiveness of our approach, outperforming existing models by a good margin and setting a new SOTA for each language pair."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: RefSum: Refactoring Neural Summarization\nAbstract: Although some recent works show potential complementarity among different state-of-the-art systems, few works try to investigate this problem in text summarization. Researchers in other areas commonly refer to the techniques of reranking or stacking to approach this problem. In this work, we highlight several limitations of previous methods, which motivates us to present a new framework Refactor that provides a unified view of text summarization and summaries combination. Experimentally, we perform a comprehensive evaluation that involves twenty-two base systems, four datasets, and three different application scenarios. Besides new state-of-the-art results on CNN/DailyMail dataset (46.18 ROUGE-1), we also elaborate on how our proposed method addresses the limitations of the traditional methods and the effectiveness of the Refactor model sheds light on insight for performance improvement. Our system can be directly used by other researchers as an off-the-shelf tool to achieve further performance improvements. We open-source all the code and provide a convenient interface to use it: https://github.com/yixinL7/Refactoring-Summarization."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Assisting the Human Fact-Checkers: Detecting All Previously Fact-Checked Claims in a Document\nAbstract: Given the recent proliferation of false claims online, there has been a lot of manual fact-checking effort. As this is very timeconsuming, human fact-checkers can benefit from tools that can support them and make them more efficient. Here, we focus on building a system that could provide such support. Given an input document, it aims to detect all sentences that contain a claim that can be verified by some previously factchecked claims (from a given database). The output is a reranked list of the document sentences, so that those that can be verified are ranked as high as possible, together with corresponding evidence. Unlike previous work, which has looked into claim retrieval, here we take a document-level perspective. We create a new manually annotated dataset for the task, and we propose suitable evaluation measures. We further experiment with a learning-to-rank approach, achieving sizable performance gains over several strong baselines. Our analysis demonstrates the importance of modeling text similarity and stance, while also taking into account the veracity of the retrieved previously fact-checked claims. We believe that this research would be of interest to fact-checkers, journalists, media, and regulatory authorities."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: MAD-X: An Adapter-based Framework for Multi-task Cross-lingual Transfer\nAbstract: The main goal behind state-of-the-art pretrained multilingual models such as multilingual BERT and XLM-R is enabling and bootstrapping NLP applications in low-resource languages through zero-shot or few-shot cross-lingual transfer. However, due to limited model capacity, their transfer performance is the weakest exactly on such low-resource languages and languages unseen during pretraining. We propose MAD-X, an adapter-based framework that enables high portability and parameter-efficient transfer to arbitrary tasks and languages by learning modular language and task representations. In addition, we introduce a novel invertible adapter architecture and a strong baseline method for adapting a pretrained multilingual model to a new language. MAD-X outperforms the state of the art in cross-lingual transfer across a representative set of typologically diverse languages on named entity recognition and achieves competitive results on question answering."}, {"role": "assistant", "content": "3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: MIDI-DDSP: Detailed Control of Musical Performance via Hierarchical Modeling\nAbstract: Musical expression requires control of both what notes are played, and how they are performed. Conventional audio synthesizers provide detailed expressive controls, but at the cost of realism. Black-box neural audio synthesis and concatenative samplers can produce realistic audio, but have few mechanisms for control. In this work, we introduce MIDI-DDSP, a hierarchical model of musical instruments that enables both realistic neural audio synthesis and detailed user control. Starting from interpretable Differentiable Digital Signal Processing (DDSP) synthesis parameters, we infer musical notes and high-level properties of their expressive performance (such as timbre, vibrato, dynamics, and articulation). This creates a 3-level hierarchy (notes, performance, synthesis) that affords individuals the option to intervene at each level, or utilize trained priors (performance given notes, synthesis given performance) for creative assistance. Through quantitative experiments and listening tests, we demonstrate that this hierarchy can reconstruct high-fidelity audio, accurately predict performance attributes for a note sequence, independently manipulate the attributes of a given performance, and as a complete system, generate realistic audio from a novel note sequence. By utilizing an interpretable hierarchy, with multiple levels of granularity, MIDI-DDSP opens the door to assistive tools to empower individuals across a diverse range of musical experience."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Does BERT Pretrained on Clinical Notes Reveal Sensitive Data?\nAbstract: Large Transformers pretrained over clinical notes from Electronic Health Records (EHR) have afforded substantial gains in performance on predictive clinical tasks. The cost of training such models (and the necessity of data access to do so) coupled with their utility motivates parameter sharing, i.e., the release of pretrained models such as ClinicalBERT. While most efforts have used deidentified EHR, many researchers have access to large sets of sensitive, non-deidentified EHR with which they might train a BERT model (or similar). Would it be safe to release the weights of such a model if they did? In this work, we design a battery of approaches intended to recover Personal Health Information (PHI) from a trained BERT. Specifically, we attempt to recover patient names and conditions with which they are associated. We find that simple probing methods are not able to meaningfully extract sensitive information from BERT trained over the MIMIC-III corpus of EHR. However, more sophisticated \u201cattacks\u201d may succeed in doing so: To facilitate such research, we make our experimental setup and baseline probing models available at https://github.com/elehman16/exposing_patient_data_release."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Why don't people use character-level machine translation?\nAbstract: We present a literature and empirical survey that critically assesses the state of the art in character-level modeling for machine translation (MT). Despite evidence in the literature that character-level systems are comparable with subword systems, they are virtually never used in competitive setups in WMT competitions. We empirically show that even with recent modeling innovations in character-level natural language processing, character-level MT systems still struggle to match their subword-based counterparts. Character-level MT systems show neither better domain robustness, nor better morphological generalization, despite being often so motivated. However, we are able to show robustness towards source side noise and that translation quality does not degrade with increasing beam size at decoding time."}, {"role": "assistant", "content": "3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Characterizing English Variation across Social Media Communities with BERT\nAbstract: Abstract Much previous work characterizing language variation across Internet social groups has focused on the types of words used by these groups. We extend this type of study by employing BERT to characterize variation in the senses of words as well, analyzing two months of English comments in 474 Reddit communities. The specificity of different sense clusters to a community, combined with the specificity of a community\u2019s unique word types, is used to identify cases where a social group\u2019s language deviates from the norm. We validate our metrics using user-created glossaries and draw on sociolinguistic theories to connect language variation with trends in community behavior. We find that communities with highly distinctive language are medium-sized, and their loyal and highly engaged users interact in dense networks."}, {"role": "assistant", "content": "3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Learning to Link Grammar and Encyclopedic Information of Assist ESL Learners\nAbstract: We introduce a system aimed at improving and expanding second language learners\u2019 English vocabulary. In addition to word definitions, we provide rich lexical information such as collocations and grammar patterns for target words. We present Linggle Booster that takes an article, identifies target vocabulary, provides lexical information, and generates a quiz on target words. Linggle Booster also links named-entity to corresponding Wikipedia pages. Evaluation on a set of target words shows that the method have reasonably good performance in terms of generating useful and information for learning vocabulary."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Conditional Poisson Stochastic Beams\nAbstract: Beam search is the default decoding strategy for many sequence generation tasks in NLP. The set of approximate K-best items returned by the algorithm is a useful summary of the distribution for many applications; however, the candidates typically exhibit high overlap and may give a highly biased estimate for expectations under our model. These problems can be addressed by instead using stochastic decoding strategies. In this work, we propose a new method for turning beam search into a stochastic process: Conditional Poisson stochastic beam search. Rather than taking the maximizing set at each iteration, we sample K candidates without replacement according to the conditional Poisson sampling design. We view this as a more natural alternative to Kool et al. (2019)\u2019s stochastic beam search (SBS). Furthermore, we show how samples generated under the CPSBS design can be used to build consistent estimators and sample diverse sets from sequence models. In our experiments, we observe CPSBS produces lower variance and more efficient estimators than SBS, even showing improvements in high entropy settings."}, {"role": "assistant", "content": "3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: NOAHQA: Numerical Reasoning with Interpretable Graph Question Answering Dataset\nAbstract: While diverse question answering (QA) datasets have been proposed and contributed significantly to the development of deep learning models for QA tasks, the existing datasets fall short in two aspects. First, we lack QA datasets covering complex questions that involve answers as well as the reasoning processes to get the answers. As a result, the state-of-the-art QA research on numerical reasoning still focuses on simple calculations and does not provide the mathematical expressions or evidences justifying the answers. Second, the QA community has contributed much effort to improving the interpretability of QA models. However, these models fail to explicitly show the reasoning process, such as the evidence order for reasoning and the interactions between different pieces of evidence. To address the above shortcomings, we introduce NOAHQA, a conversational and bilingual QA dataset with questions requiring numerical reasoning with compound mathematical expressions. With NOAHQA, we develop an interpretable reasoning graph as well as the appropriate evaluation metric to measure the answer quality. We evaluate the state-of-the-art QA models trained using existing QA datasets on NOAHQA and show that the best among them can only achieve 55.5 exact match scores, while the human performance is 89.7. We also present a new QA model for generating a reasoning graph where the reasoning graph metric still has a large gap compared with that of humans, e.g., 28 scores. The dataset and code are publicly available 1."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers\nAbstract: Recently, many datasets have been proposed to test the systematic generalization ability of neural networks. The companion baseline Transformers, typically trained with default hyper-parameters from standard tasks, are shown to fail dramatically. Here we demonstrate that by revisiting model configurations as basic as scaling of embeddings, early stopping, relative positional embedding, and Universal Transformer variants, we can drastically improve the performance of Transformers on systematic generalization. We report improvements on five popular datasets: SCAN, CFQ, PCFG, COGS, and Mathematics dataset. Our models improve accuracy from 50% to 85% on the PCFG productivity split, and from 35% to 81% on COGS. On SCAN, relative positional embedding largely mitigates the EOS decision problem (Newman et al., 2020), yielding 100% accuracy on the length split with a cutoff at 26. Importantly, performance differences between these models are typically invisible on the IID data split. This calls for proper generalization validation sets for developing neural networks that generalize systematically. We publicly release the code to reproduce our results."}, {"role": "assistant", "content": "3.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Privacy Regularization: Joint Privacy-Utility Optimization in LanguageModels\nAbstract: Neural language models are known to have a high capacity for memorization of training samples. This may have serious privacy im- plications when training models on user content such as email correspondence. Differential privacy (DP), a popular choice to train models with privacy guarantees, comes with significant costs in terms of utility degradation and disparate impact on subgroups of users. In this work, we introduce two privacy-preserving regularization methods for training language models that enable joint optimization of utility and privacy through (1) the use of a discriminator and (2) the inclusion of a novel triplet-loss term. We compare our methods with DP through extensive evaluation. We show the advantages of our regularizers with favorable utility-privacy trade-off, faster training with the ability to tap into existing optimization approaches, and ensuring uniform treatment of under-represented subgroups."}, {"role": "assistant", "content": "3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Effective sampling and learning for mallows models with pairwise-preference data\nAbstract: Learning preference distributions is a critical problem in many areas (e.g., recommender systems, IR, social choice). However, many existing learning and inference methods impose restrictive assumptions on the form of user preferences that can be admitted as evidence. We relax these restrictions by considering as data arbitrary pairwise comparisons of alternatives, which represent the fundamental building blocks of ordinal rankings. We develop the first algorithms for learning Mallows models (and mixtures thereof) from pairwise comparison data. At the heart of our technique is a new algorithm, the generalized repeated insertion model (GRIM), which allows sampling from arbitrary ranking distributions, and conditional Mallows models in particular. While we show that sampling from a Mallows model with pairwise evidence is computationally difficult in general, we develop approximate samplers that are exact for many important special cases|and have provable bounds with pairwise evidence--and derive algorithms for evaluating log-likelihood, learning Mallows mixtures, and non-parametric estimation. Experiments on real-world data sets demonstrate the effectiveness of our approach."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: iFlow: Numerically Invertible Flows for Efficient Lossless Compression via a Uniform Coder\nAbstract: It was estimated that the world produced 59ZB (5.9\u00d7 10GB) of data in 2020, resulting in the enormous costs of both data storage and transmission. Fortunately, recent advances in deep generative models have spearheaded a new class of socalled \"neural compression\" algorithms, which significantly outperform traditional codecs in terms of compression ratio. Unfortunately, the application of neural compression garners little commercial interest due to its limited bandwidth; therefore, developing highly efficient frameworks is of critical practical importance. In this paper, we discuss lossless compression using normalizing flows which have demonstrated a great capacity for achieving high compression ratios. As such, we introduce iFlow, a new method for achieving efficient lossless compression. We first propose Modular Scale Transform (MST) and a novel family of numerically invertible flow transformations based on MST. Then we introduce the Uniform Base Conversion System (UBCS), a fast uniform-distribution codec incorporated into iFlow, enabling efficient compression. iFlow achieves state-of-the-art compression ratios and is 5\u00d7 quicker than other high-performance schemes. Furthermore, the techniques presented in this paper can be used to accelerate coding time for a broad class of flow-based algorithms."}, {"role": "assistant", "content": "3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Creating Training Sets via Weak Indirect Supervision\nAbstract: Creating labeled training sets has become one of the major roadblocks in machine learning. To address this, recent Weak Supervision (WS) frameworks synthesize training labels from multiple potentially noisy supervision sources. However, existing frameworks are restricted to supervision sources that share the same output space as the target task. To extend the scope of usable sources, we formulate Weak Indirect Supervision (WIS), a new research problem for automatically synthesizing training labels based on indirect supervision sources that have different output label spaces. To overcome the challenge of mismatched output spaces, we develop a probabilistic modeling approach, PLRM, which uses user-provided label relations to model and leverage indirect supervision sources. Moreover, we provide a theoretically-principled test of the distinguishability of PLRM for unseen labels, along with a generalization bound. On both image and text classification tasks as well as an industrial advertising application, we demonstrate the advantages of PLRM by outperforming baselines by a margin of 2%-9%."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Joint Passage Ranking for Diverse Multi-Answer Retrieval\nAbstract: We study multi-answer retrieval, an under-explored problem that requires retrieving passages to cover multiple distinct answers for a given question. This task requires joint modeling of retrieved passages, as models should not repeatedly retrieve passages containing the same answer at the cost of missing a different valid answer. Prior work focusing on single-answer retrieval is limited as it cannot reason about the set of passages jointly. In this paper, we introduce JPR, a joint passage retrieval model focusing on reranking. To model the joint probability of the retrieved passages, JPR makes use of an autoregressive reranker that selects a sequence of passages, equipped with novel training and decoding algorithms. Compared to prior approaches, JPR achieves significantly better answer coverage on three multi-answer datasets. When combined with downstream question answering, the improved retrieval enables larger answer generation models since they need to consider fewer passages, establishing a new state-of-the-art."}, {"role": "assistant", "content": "1.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Aspect-based Sentiment Analysis of Scientific Reviews\nAbstract: Scientific papers are complex and understanding the usefulness of these papers requires prior knowledge. Peer reviews are comments on a paper provided by designated experts on that field and hold a substantial amount of information, not only for the editors and chairs to make the final decision, but also to judge the potential impact of the paper. In this paper, we propose to use aspect-based sentiment analysis of scientific reviews to be able to extract useful information, which correlates well with the accept/reject decision. While working on a dataset of close to 8k reviews from ICLR, one of the top conferences in the field of machine learning, we use an active learning framework to build a training dataset for aspect prediction, which is further used to obtain the aspects and sentiments for the entire dataset. We show that the distribution of aspect-based sentiments obtained from a review is significantly different for accepted and rejected papers. We use the aspect sentiments from these reviews to make an intriguing observation, certain aspects present in a paper and discussed in the review strongly determine the final recommendation. As a second objective, we quantify the extent of disagreement among the reviewers refereeing a paper. We also investigate the extent of disagreement between the reviewers and the chair and find that the inter-reviewer disagreement may have a link to the disagreement with the chair. One of the most interesting observations from this study is that reviews, where the reviewer score and the aspect sentiments extracted from the review text written by the reviewer are consistent, are also more likely to be concurrent with the chair's decision."}, {"role": "assistant", "content": "2.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Fully Test-time Adaptation by Entropy Minimization\nAbstract: Faced with new and different data during testing, a model must adapt itself. We consider the setting of fully test-time adaptation, in which a supervised model confronts unlabeled test data from a different distribution, without the help of its labeled training data. We propose an entropy minimization approach for adaptation: we take the model's confidence as our objective as measured by the entropy of its predictions. During testing, we adapt the model by modulating its representation with affine transformations to minimize entropy. Our experiments show improved robustness to corruptions for image classification on CIFAR-10/100 and ILSVRC and demonstrate the feasibility of target-only domain adaptation for digit classification on MNIST and SVHN."}, {"role": "assistant", "content": "2.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Does the Whole Exceed its Parts? The Effect of AI Explanations on Complementary Team Performance\nAbstract: Many researchers motivate explainable AI with studies showing that human-AI team performance on decision-making tasks improves when the AI explains its recommendations. However, prior studies observed improvements from explanations only when the AI, alone, outperformed both the human and the best team. Can explanations help lead to complementary performance, where team accuracy is higher than either the human or the AI working solo? We conduct mixed-method user studies on three datasets, where an AI with accuracy comparable to humans helps participants solve a task (explaining itself in some conditions). While we observed complementary improvements from AI augmentation, they were not increased by explanations. Rather, explanations increased the chance that humans will accept the AI\u2019s recommendation, regardless of its correctness. Our result poses new challenges for human-centered AI: Can we develop explanatory approaches that encourage appropriate trust in AI, and therefore help generate (or improve) complementary performance?"}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Program Synthesis with Large Language Models\nAbstract: This paper explores the limits of the current generation of large language models for program synthesis in general purpose programming languages. We evaluate a collection of such models (with between 244M and 137B parameters) on two new benchmarks, MBPP and MathQA-Python, in both the few-shot and fine-tuning regimes. Our benchmarks are designed to measure the ability of these models to synthesize short Python programs from natural language descriptions. The Mostly Basic Programming Problems (MBPP) dataset contains 974 programming tasks, designed to be solvable by entry-level programmers. The MathQA-Python dataset, a Python version of the MathQA benchmark, contains 23914 problems that evaluate the ability of the models to synthesize code from more complex text. On both datasets, we find that synthesis performance scales log-linearly with model size. Our largest models, even without finetuning on a code dataset, can synthesize solutions to 59.6% of the problems from MBPP using few-shot learning with a well-designed prompt. Fine-tuning on a held-out portion of the dataset improves performance by about 10 percentage points across most model sizes. On the MathQA-Python dataset, the largest fine-tuned model achieves 83.8% accuracy. Going further, we study the model\u2019s ability to engage in dialog about code, incorporating human feedback to improve its solutions. We find that natural language feedback from a human halves the error rate compared to the model\u2019s initial prediction. Additionally, we conduct an error analysis to shed light on where these models fall short and what types of programs are most difficult to generate. Finally, we explore the semantic grounding of these models by fine-tuning them to predict the results of program execution. We find that even our best models are generally unable to predict the output of a program given a specific input."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Efficient Nearest Neighbor Language Models\nAbstract: Non-parametric neural language models (NLMs) learn predictive distributions of text utilizing an external datastore, which allows them to learn through explicitly memorizing the training datapoints. While effective, these models often require retrieval from a large datastore at test time, significantly increasing the inference overhead and thus limiting the deployment of non-parametric NLMs in practical applications. In this paper, we take the recently proposed k-nearest neighbors language model as an example, exploring methods to improve its efficiency along various dimensions. Experiments on the standard WikiText-103 benchmark and domain-adaptation datasets show that our methods are able to achieve up to a 6x speed-up in inference speed while retaining comparable performance. The empirical analysis we present may provide guidelines for future research seeking to develop or deploy more efficient non-parametric NLMs."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Compositional Attention: Disentangling Search and Retrieval\nAbstract: Multi-head, key-value attention is the backbone of the widely successful Transformer model and its variants. This attention mechanism uses multiple parallel key-value attention blocks (called heads), each performing two fundamental computations: (1) search \u2013 selection of a relevant entity from a set via query-key interactions, and (2) retrieval \u2013 extraction of relevant features from the selected entity via a value matrix. Importantly, standard attention heads learn a rigid mapping between search and retrieval. In this work, we first highlight how this static nature of the pairing can potentially: (a) lead to learning of redundant parameters in certain tasks, and (b) hinder generalization. To alleviate this problem, we propose a novel attention mechanism, called Compositional Attention, that replaces the standard head structure. The proposed mechanism disentangles search and retrieval and composes them in a dynamic, flexible and context-dependent manner through an additional soft competition stage between the query-key combination and value pairing. Through a series of numerical experiments, we show that it outperforms standard multi-head attention on a variety of tasks, including some out-of-distribution settings. Through our qualitative analysis, we demonstrate that Compositional Attention leads to dynamic specialization based on the type of retrieval needed. Our proposed mechanism generalizes multi-head attention, allows independent scaling of search and retrieval, and can easily be implemented in lieu of standard attention heads in any network architecture.1"}, {"role": "assistant", "content": "3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Human-Level Performance in No-Press Diplomacy via Equilibrium Search\nAbstract: Prior AI breakthroughs in complex games have focused on either the purely adversarial or purely cooperative settings. In contrast, Diplomacy is a game of shifting alliances that involves both cooperation and competition. For this reason, Diplomacy has proven to be a formidable research challenge. In this paper we describe an agent for the no-press variant of Diplomacy that combines supervised learning on human data with one-step lookahead search via external regret minimization. External regret minimization techniques have been behind previous AI successes in adversarial games, most notably poker, but have not previously been shown to be successful in large-scale games involving cooperation. We show that our agent greatly exceeds the performance of past no-press Diplomacy bots, is unexploitable by expert humans, and achieves a rank of 23 out of 1,128 human players when playing anonymous games on a popular Diplomacy website."}, {"role": "assistant", "content": "2.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Interpretable and Interactive Summaries of Actionable Recourses\nAbstract: As predictive models are increasingly being deployed in high-stakes decision-making, there has been a lot of interest in developing algorithms which can provide recourses to affected individuals. While developing such tools is important, it is even more critical to analyse and interpret a predictive model, and vet it thoroughly to ensure that the recourses it offers are meaningful and non-discriminatory before it is deployed in the real world. To this end, we propose a novel model agnostic framework called Actionable Recourse Summaries (AReS) to construct global counterfactual explanations which provide an interpretable and accurate summary of recourses for the entire population. We formulate a novel objective which simultaneously optimizes for correctness of the recourses and interpretability of the explanations, while minimizing overall recourse costs across the entire population. More specifically, our objective enables us to learn, with optimality guarantees on recourse correctness, a small number of compact rule sets each of which capture recourses for well defined subpopulations within the data. Our framework is also interactive i.e., it allows users to input specific features of interest which will in turn be used to characterize subpopulations when generating recourse summaries. We also demonstrate theoretically that several of the prior approaches proposed to generate recourses for individuals are special cases of our framework. Experimental evaluation with real world datasets and user studies demonstrate that our framework can provide decision makers with a comprehensive overview of recourses corresponding to any black box model, and consequently help detect undesirable model biases and discrimination."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Explore Aggressively, Update Conservatively: Stochastic Extragradient Methods with Variable Stepsize Scaling\nAbstract: Owing to their stability and convergence speed, extragradient methods have become a staple for solving large-scale saddle-point problems in machine learning. The basic premise of these algorithms is the use of an extrapolation step before performing an update; thanks to this exploration step, extra-gradient methods overcome many of the non-convergence issues that plague gradient descent/ascent schemes. On the other hand, as we show in this paper, running vanilla extragradient with stochastic gradients may jeopardize its convergence, even in simple bilinear models. To overcome this failure, we investigate a double stepsize extragradient algorithm where the exploration step evolves at a more aggressive time-scale compared to the update step. We show that this modification allows the method to converge even with stochastic gradients, and we derive sharp convergence rates under an error bound condition."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Optimal auctions through deep learning\nAbstract: Designing an incentive compatible auction that maximizes expected revenue is an intricate task. The single-item case was resolved in a seminal piece of work by Myerson in 1981. Even after 30--40 years of intense research, the problem remains unsolved for settings with two or more items. We overview recent research results that show how tools from deep learning are shaping up to become a powerful tool for the automated design of near-optimal auctions auctions. In this approach, an auction is modeled as a multilayer neural network, with optimal auction design framed as a constrained learning problem that can be addressed with standard machine learning pipelines. Through this approach, it is possible to recover to a high degree of accuracy essentially all known analytically derived solutions for multi-item settings and obtain novel mechanisms for settings in which the optimal mechanism is unknown."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Echo State Speech Recognition\nAbstract: We propose automatic speech recognition (ASR) models inspired by echo state network (ESN) [1], in which a subset of recurrent neural networks (RNN) layers in the models are randomly initialized and untrained. Our study focuses on RNN-T and Conformer models, and we show that model quality does not drop even when the decoder is fully randomized. Furthermore, such models can be trained more efficiently as the decoders do not require to be updated. By contrast, randomizing encoders hurts model quality, indicating that optimizing encoders and learn proper representations for acoustic inputs are more vital for speech recognition. Overall, we challenge the common practice of training ASR models for all components, and demonstrate that ESN-based models can perform equally well but enable more efficient training and storage than fully-trainable counterparts."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: CLIPort: What and Where Pathways for Robotic Manipulation\nAbstract: How can we imbue robots with the ability to manipulate objects pre1 cisely but also to reason about them in terms of abstract concepts? Recent works 2 in manipulation have shown that end-to-end networks can learn dexterous skills 3 that require precise spatial reasoning, but these methods often fail to generalize to 4 new goals or quickly learn transferable concepts. In parallel, there has been great 5 progress in learning generalizable semantic representations for vision and lan6 guage by training on large-scale internet data, however these representations lack 7 the spatial understanding necessary for fine-grained manipulation. To this end, we 8 propose a framework that combines the best of both worlds: a two-stream archi9 tecture with semantic (what) and spatial (where) pathways for vision-based ma10 nipulation. Specifically, we present CLIPORT, a language-conditioned imitation11 learning agent that combines the broad semantic understanding of CLIP [1] with 12 the spatial precision of Transporter [2]. Our end-to-end framework is capable of 13 solving a variety of language-specified tabletop tasks from packing unseen objects 14 to folding cloths, all without any explicit representations of object poses, instance 15 segmentations, history, symbolic states, or syntactic structures. Experiments in 16 simulation and hardware show that our approach is data efficient in few-shot set17 tings and generalizes effectively to seen and unseen semantic concepts. We even 18 learn one multi-task policy for 10 simulated and 9 real-world tasks that shows 19 better or comparable performance to single-task models. 20"}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: End-to-End Entity Resolution and Question Answering Using Differentiable Knowledge Graphs\nAbstract: Recently, end-to-end (E2E) trained models for question answering over knowledge graphs (KGQA) have delivered promising results using only a weakly supervised dataset. However, these models are trained and evaluated in a setting where hand-annotated question entities are supplied to the model, leaving the important and non-trivial task of entity resolution (ER) outside the scope of E2E learning. In this work, we extend the boundaries of E2E learning for KGQA to include the training of an ER component. Our model only needs the question text and the answer entities to train, and delivers a stand-alone QA model that does not require an additional ER component to be supplied during runtime. Our approach is fully differentiable, thanks to its reliance on a recent method for building differentiable KGs (Cohen et al., 2020). We evaluate our E2E trained model on two public datasets and show that it comes close to baseline models that use hand-annotated entities."}, {"role": "assistant", "content": "2.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: NeuSpell: A Neural Spelling Correction Toolkit\nAbstract: We introduce NeuSpell, an open-source toolkit for spelling correction in English. Our toolkit comprises ten different models, and benchmarks them on naturally occurring misspellings from multiple sources. We find that many systems do not adequately leverage the context around the misspelt token. To remedy this, (i) we train neural models using spelling errors in context, synthetically constructed by reverse engineering isolated misspellings; and (ii) use richer representations of the context. By training on our synthetic examples, correction rates improve by 9% (absolute) compared to the case when models are trained on randomly sampled character perturbations. Using richer contextual representations boosts the correction rate by another 3%. Our toolkit enables practitioners to use our proposed and existing spelling correction systems, both via a simple unified command line, as well as a web interface. Among many potential applications, we demonstrate the utility of our spell-checkers in combating adversarial misspellings. The toolkit can be accessed at neuspell.github.io."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Controllable Semantic Parsing via Retrieval Augmentation\nAbstract: In practical applications of semantic parsing, we often want to rapidly change the behavior of the parser, such as enabling it to handle queries in a new domain, or changing its predictions on certain targeted queries. While we can introduce new training examples exhibiting the target behavior, a mechanism for enacting such behavior changes without expensive model re-training would be preferable. To this end, we propose ControllAble Semantic Parser via Exemplar Retrieval (CASPER). Given an input query, the parser retrieves related exemplars from a retrieval index, augments them to the query, and then applies a generative seq2seq model to produce an output parse. The exemplars act as a control mechanism over the generic generative model: by manipulating the retrieval index or how the augmented query is constructed, we can manipulate the behavior of the parser. On the MTOP dataset, in addition to achieving state-of-the-art on the standard setup, we show that CASPER can parse queries in a new domain, adapt the prediction toward the specified patterns, or adapt to new semantic schemas without having to further re-train the model."}, {"role": "assistant", "content": "4.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Identifying Elements Essential for BERT\u2019s Multilinguality\nAbstract: It has been shown that multilingual BERT (mBERT) yields high quality multilingual representations and enables effective zero-shot transfer. This is surprising given that mBERT does not use any crosslingual signal during training. While recent literature has studied this phenomenon, the reasons for the multilinguality are still somewhat obscure. We aim to identify architectural properties of BERT and linguistic properties of languages that are necessary for BERT to become multilingual. To allow for fast experimentation we propose an efficient setup with small BERT models trained on a mix of synthetic and natural data. Overall, we identify four architectural and two linguistic elements that influence multilinguality. Based on our insights, we experiment with a multilingual pretraining setup that modifies the masking strategy using VecMap, i.e., unsupervised embedding alignment. Experiments on XNLI with three languages indicate that our findings transfer from our small setup to larger scale settings."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Towards Debiasing NLU Models from Unknown Biases\nAbstract: NLU models often exploit biases to achieve high dataset-specific performance without properly learning the intended task. Recently proposed debiasing methods are shown to be effective in mitigating this tendency. However, these methods rely on a major assumption that the types of bias should be known a-priori, which limits their application to many NLU tasks and datasets. In this work, we present the first step to bridge this gap by introducing a self-debiasing framework that prevents models from mainly utilizing biases without knowing them in advance. The proposed framework is general and complementary to the existing debiasing methods. We show that it allows these existing methods to retain the improvement on the challenge datasets (i.e., sets of examples designed to expose models' reliance on biases) without specifically targeting certain biases. Furthermore, the evaluation suggests that applying the framework results in improved overall robustness."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: The DDI corpus: An annotated corpus with pharmacological substances and drug-drug interactions\nAbstract: The management of drug\u2013drug interactions (DDIs) is a critical issue resulting from the overwhelming amount of information available on them. Natural Language Processing (NLP) techniques can provide an interesting way to reduce the time spent by healthcare professionals on reviewing biomedical litera-ture. However, NLP techniques rely mostly on the availability of the annotated corpora. While there are several annotated corpora with biological entities and their relationships, there is a lack of corpora anno-tated with pharmacological substances and DDIs. Moreover, other works in this field have focused in pharmacokinetic (PK) DDIs only, but not in pharmacodynamic (PD) DDIs. To address this problem, we have created a manually annotated corpus consisting of 792 texts selected from the DrugBank database and other 233 Medline abstracts. This fined-grained corpus has been annotated with a total of 18,502 pharmacological substances and 5028 DDIs, including both PK as well as PD interactions. The quality and consistency of the annotation process has been ensured through the creation of annotation guide-lines and has been evaluated by the measurement of the inter-annotator agreement between two anno-tators. The agreement was almost perfect (Kappa up to 0.96 and generally over 0.80), except for the DDIs in the MedLine database (0.55\u20130.72). The DDI corpus has been used in the SemEval 2013 DDIExtraction challenge as a gold standard for the evaluation of information extraction techniques applied to the rec-ognition of pharmacological substances and the detection of DDIs from biomedical texts. DDIExtraction 2013 has attracted wide attention with a total of 14 teams from 7 different countries. For the task of rec-ognition and classification of pharmacological names, the best system achieved an F1 of 71.5%, while, for the detection and classification of DDIs, the best result was F1 of 65.1%. These results show that the cor-pus has enough quality to be used for training and testing NLP techniques applied to the field of Pharma-covigilance. The DDI corpus and the annotation guidelines are free for use for academic research and are available at http://labda.inf.uc3m.es/ddicorpus."}, {"role": "assistant", "content": "3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Relation-Guided Pre-Training for Open-Domain Question Answering\nAbstract: Answering complex open-domain questions requires understanding the latent relations between involving entities. However, we found that the existing QA datasets are extremely imbalanced in some types of relations, which hurts the generalization performance over questions with long-tail relations. To remedy this problem, in this paper, we propose a Relation-Guided Pre-Training (RGPT-QA) framework1. We first generate a relational QA dataset covering a wide range of relations from both the Wikidata triplets and Wikipedia hyperlinks. We then pre-train a QA model to infer the latent relations from the question, and then conduct extractive QA to get the target answer entity. We demonstrate that by pretraining with propoed RGPT-QA techique, the popular open-domain QA model, Dense Passage Retriever (DPR), achieves 2.2%, 2.4%, and 6.3% absolute improvement in Exact Match accuracy on Natural Questions, TriviaQA, and WebQuestions. Particularly, we show that RGPT-QA improves significantly on questions with long-tail relations."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Uncertain Decisions Facilitate Better Preference Learning\nAbstract: Existing observational approaches for learning human preferences, such as inverse reinforcement learning, usually make strong assumptions about the observability of the human\u2019s environment. However, in reality, people make many important decisions under uncertainty. To better understand preference learning in these cases, we study the setting of inverse decision theory (IDT), a previously proposed framework where a human is observed making non-sequential binary decisions under uncertainty. In IDT, the human\u2019s preferences are conveyed through their loss function, which expresses a tradeoff between different types of mistakes. We give the first statistical analysis of IDT, providing conditions necessary to identify these preferences and characterizing the sample complexity\u2014the number of decisions that must be observed to learn the tradeoff the human is making to a desired precision. Interestingly, we show that it is actually easier to identify preferences when the decision problem is more uncertain. Furthermore, uncertain decision problems allow us to relax the unrealistic assumption that the human is an optimal decision maker but still identify their exact preferences; we give sample complexities in this suboptimal case as well. Our analysis contradicts the intuition that partial observability should make preference learning more difficult. It also provides a first step towards understanding and improving preference learning methods for uncertain and suboptimal humans."}, {"role": "assistant", "content": "3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Schema-Guided Paradigm for Zero-Shot Dialog\nAbstract: Developing mechanisms that flexibly adapt dialog systems to unseen tasks and domains is a major challenge in dialog research. Neural models implicitly memorize task-specific dialog policies from the training data. We posit that this implicit memorization has precluded zero-shot transfer learning. To this end, we leverage the schema-guided paradigm, wherein the task-specific dialog policy is explicitly provided to the model. We introduce the Schema Attention Model (SAM) and improved schema representations for the STAR corpus. SAM obtains significant improvement in zero-shot settings, with a +22 F1 score improvement over prior work. These results validate the feasibility of zero-shot generalizability in dialog. Ablation experiments are also presented to demonstrate the efficacy of SAM."}, {"role": "assistant", "content": "4.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Ranking and Tuning Pre-trained Models: A New Paradigm of Exploiting Model Hubs\nAbstract: Model hubs with many pre-trained models (PTMs) have been a cornerstone in deep learning. Although built at a high cost, they remain under-exploited : practitioners usually pick one PTM from the provided model hub by popularity and then fine-tune the PTM to solve the target task. This n\u00e4\u0131ve but common practice poses two obstacles to sufficient exploitation of pre-trained model hubs: (1) the PTM selection by popularity has no optimality guarantee; (2) only one PTM is used while the rest PTMs are ignored. Ideally, to exploit pre-trained model hubs maximally, trying all combinations of PTMs and extensively fine-tuning each PTM combination are required, which incurs exponential combinations and an unaffordable computational budget. In this paper, we propose a new paradigm of exploiting model hubs by ranking and tuning pre-trained models: (1) Our conference paper (You et al., 2021) proposed LogME to estimate the maximum value of label evidence given features extracted by pre-trained models, which can rank all the PTMs in a model hub for various types of PTMs and tasks before fine-tuning. (2) The best ranked PTM can be fine-tuned and deployed if we have no preference for the model\u2019s architecture, or the target PTM can be tuned by top-K ranked PTMs via the proposed B-Tuning algorithm. The ranking part is based on the conference paper, and we complete its theoretical analyses in this paper, including the convergence proof of the heuristic evidence maximization procedure and the influence of feature dimension. The tuning part introduces a novel Bayesian Tuning (B-Tuning) method for tuning multiple PTMs, which surpasses specialized methods designed for tuning homogeneous PTMs and sets up a new state of the art for tuning heterogeneous PTMs. The new paradigm of exploiting PTM hubs can be interesting to a large audience across the machine learning community."}, {"role": "assistant", "content": "4.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: More Adaptive Algorithms for Adversarial Bandits\nAbstract: We develop a novel and generic algorithm for the adversarial multi-armed bandit problem (or more generally the combinatorial semi-bandit problem). When instantiated differently, our algorithm achieves various new data-dependent regret bounds improving previous works. Examples include: 1) a regret bound depending on the variance of only the best arm; 2) a regret bound depending on the first-order path-length of only the best arm; 3) a regret bound depending on the sum of first-order path-lengths of all arms as well as an important negative term, which together lead to faster convergence rates for some normal form games with partial feedback; 4) a regret bound that simultaneously implies small regret when the best arm has small loss and logarithmic regret when there exists an arm whose expected loss is always smaller than those of others by a fixed gap (e.g. the classic i.i.d. setting). In some cases, such as the last two results, our algorithm is completely parameter-free. \nThe main idea of our algorithm is to apply the optimism and adaptivity techniques to the well-known Online Mirror Descent framework with a special log-barrier regularizer. The challenges are to come up with appropriate optimistic predictions and correction terms in this framework. Some of our results also crucially rely on using a sophisticated increasing learning rate schedule."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: From Recommendation Systems to Facility Location Games\nAbstract: Recommendation systems are extremely popular tools for matching users and contents. However, when content providers are strategic, the basic principle of matching users to the closest content, where both users and contents are modeled as points in some semantic space, may yield low social welfare. This is due to the fact that content providers are strategic and optimize their offered content to be recommended to as many users as possible. Motivated by modern applications, we propose the widely studied framework of facility location games to study recommendation systems with strategic content providers. Our conceptual contribution is the introduction of a mediator to facility location models, in the pursuit of better social welfare. We aim at designing mediators that a) induce a game with high social welfare in equilibrium, and b) intervene as little as possible. In service of the latter, we introduce the notion of intervention cost, which quantifies how much damage a mediator may cause to the social welfare when an off-equilibrium profile is adopted. As a case study in high-welfare low-intervention mediator design, we consider the one-dimensional segment as the user domain. We propose a mediator that implements the socially optimal strategy profile as the unique equilibrium profile, and show a tight bound on its intervention cost. Ultimately, we consider some extensions, and highlight open questions for the general agenda."}, {"role": "assistant", "content": "4.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Promoting Connectivity of Network-Like Structures by Enforcing Region Separation\nAbstract: We propose a novel, connectivity-oriented loss function for training deep convolutional networks to reconstruct network-like structures, like roads and irrigation canals, from aerial images. The main idea behind our loss is to express the connectivity of roads, or canals, in terms of disconnections that they create between background regions of the image. In simple terms, a gap in the predicted road causes two background regions, that lie on the opposite sides of a ground truth road, to touch in prediction. Our loss function is designed to prevent such unwanted connections between background regions, and therefore close the gaps in predicted roads. It also prevents predicting false positive roads and canals by penalizing unwarranted disconnections of background regions. In order to capture even short, dead-ending road segments, we evaluate the loss in small image crops. We show, in experiments on two standard road benchmarks and a new data set of irrigation canals, that convnets trained with our loss function recover road connectivity so well that it suffices to skeletonize their output to produce state of the art maps. A distinct advantage of our approach is that the loss can be plugged in to any existing training setup without further modifications."}, {"role": "assistant", "content": "3.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Systematic Analysis of Cluster Similarity Indices: How to Validate Validation Measures\nAbstract: There are many cluster similarity indices used to evaluate clustering algorithms, and choosing the best one for a particular task is usually an open problem. In this paper, we perform a thorough analysis of this problem: we develop a list of desirable properties (requirements) and theoretically verify which indices satisfy them. In particular, we investigate dozens of pair-counting indices and prove that none of them meet all the requirements. Based on our analysis, we propose using the arccosine of the correlation coefficient as a similarity measure and show that it satisfies almost all the requirements (except for one, which is still satisfied asymptotically). We illustrate the practical importance of our analysis via an online experiment within a major news aggregator system."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs\nAbstract: Enabling effective and ef\ufb01cient machine learning (ML) over large-scale graph data ( e.g. , graphs with billions of edges) can have a great impact on both industrial and scienti\ufb01c applications. However, existing efforts to advance large-scale graph ML have been largely limited by the lack of a suitable public benchmark. Here we present OGB Large-Scale Challenge (OGB-LSC), a collection of three real-world datasets for facilitating the advancements in large-scale graph ML. The OGB-LSC datasets are orders of magnitude larger than existing ones, covering three core graph learning tasks\u2014link prediction, graph regression, and node classi\ufb01cation. Furthermore, we provide dedicated baseline experiments, scaling up expressive graph ML models to the massive datasets. We show that expressive models signi\ufb01cantly outperform simple scalable baselines, indicating an opportunity for dedicated efforts to further improve graph ML at scale. Moreover, OGB-LSC datasets were deployed at ACM KDD Cup 2021 and attracted more than 500 team registrations globally, during which signi\ufb01cant performance im-provements were made by a variety of innovative techniques. We summarize the common techniques used by the winning solutions and highlight the current best practices in large-scale graph ML. Finally, we describe how we have updated the datasets after the KDD Cup to further facilitate research advances. The OGB-LSC datasets, baseline code, and all the information about the KDD Cup are available at https://ogb.stanford.edu/docs/lsc/ ."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Adapting Neural Networks for the Estimation of Treatment Effects\nAbstract: This paper addresses the use of neural networks for the estimation of treatment effects from observational data. Generally, estimation proceeds in two stages. First, we fit models for the expected outcome and the probability of treatment (propensity score) for each unit. Second, we plug these fitted models into a downstream estimator of the effect. Neural networks are a natural choice for the models in the first step. The question we address is: how can we adapt the design and training of the neural networks used in the first step in order to improve the quality of the final estimate of the treatment effect? We propose two adaptations based on insights from the statistical literature on the estimation of treatment effects. The first is a new architecture, the Dragonnet, that exploits the sufficiency of the propensity score for estimation adjustment. The second is a regularization procedure, targeted regularization, that induces a bias towards models that have non-parametrically optimal asymptotic properties `out-of-the-box`. Studies on benchmark datasets for causal inference show these adaptations outperform existing methods. Code is available at this http URL."}, {"role": "assistant", "content": "3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Influence Functions in Deep Learning Are Fragile\nAbstract: Influence functions approximate the effect of training samples in test-time predictions and have a wide variety of applications in machine learning interpretability and uncertainty estimation. A commonly-used (first-order) influence function can be implemented efficiently as a post-hoc method requiring access only to the gradients and Hessian of the model. For linear models, influence functions are well-defined due to the convexity of the underlying loss function and are generally accurate even across difficult settings where model changes are fairly large such as estimating group influences. Influence functions, however, are not well-understood in the context of deep learning with non-convex loss functions. In this paper, we provide a comprehensive and large-scale empirical study of successes and failures of influence functions in neural network models trained on datasets such as Iris, MNIST, CIFAR-10 and ImageNet. Through our extensive experiments, we show that the network architecture, its depth and width, as well as the extent of model parameterization and regularization techniques have strong effects in the accuracy of influence functions. In particular, we find that (i) influence estimates are fairly accurate for shallow networks, while for deeper networks the estimates are often erroneous; (ii) for certain network architectures and datasets, training with weight-decay regularization is important to get high-quality influence estimates; and (iii) the accuracy of influence estimates can vary significantly depending on the examined test points. These results suggest that in general influence functions in deep learning are fragile and call for developing improved influence estimation methods to mitigate these issues in non-convex setups."}, {"role": "assistant", "content": "4.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Improving language models by retrieving from trillions of tokens\nAbstract: Language modelling (LM) is an unsupervised task that consists of modelling the probability of text, usually by factorising it into conditional next-token predictions p(x1, . . . , xn) = \u220f i p(xi |x<i). Neural networks have proven to be powerful language models, first in the form of recurrent architectures (Graves, 2013; Jozefowicz et al., 2016; Mikolov et al., 2010) and more recently in the form of Transformers (Vaswani et al., 2017), that use attention to contextualise the past. Large performance improvements have come from increasing the amount of data, training compute, or model parameters. Transformers have been scaled from 100 million parameter models in seminal work to over hundred billion parameters (Brown et al., 2020; Radford et al., 2019) in the last two years which has led to models that do very well on a wide array of tasks in a zero or few-shot formulation. Increasing model size predictably improves performance on a wide range of downstream tasks (Kaplan et al., 2020). The benefits of increasing the number of parameters come from two factors: additional computations at training and inference time, and increased memorization of the training data."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Quickest Change Detection Under Transient Dynamics: Theory and Asymptotic Analysis\nAbstract: The problem of quickest change detection under transient dynamics is studied, where the change from the initial distribution to the final persistent distribution does not happen instantaneously, but after a series of transient phases. The observations within the different phases are generated by different distributions. The objective is to detect the change as quickly as possible, while controlling the average run length (ARL) to false alarm, when the durations of the transient phases are completely unknown. Two algorithms are considered: the dynamic Cumulative Sum (CuSum) algorithm, proposed in earlier work, and a newly constructed weighted dynamic CuSum algorithm. Both algorithms admit recursions that facilitate their practical implementation, and they are adaptive to the unknown transient durations. Specifically, their asymptotic optimality is established with respect to both Lorden\u2019s and Pollak\u2019s criteria as the ARL to false alarm and the durations of the transient phases go to infinity at any relative rate. Numerical results are provided to demonstrate the adaptivity of the proposed algorithms and to validate the theoretical results."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Provable Limitations of Acquiring Meaning from Ungrounded Form: What Will Future Language Models Understand?\nAbstract: Abstract Language models trained on billions of tokens have recently led to unprecedented results on many NLP tasks. This success raises the question of whether, in principle, a system can ever \u201cunderstand\u201d raw text without access to some form of grounding. We formally investigate the abilities of ungrounded systems to acquire meaning. Our analysis focuses on the role of \u201cassertions\u201d: textual contexts that provide indirect clues about the underlying semantics. We study whether assertions enable a system to emulate representations preserving semantic relations like equivalence. We find that assertions enable semantic emulation of languages that satisfy a strong notion of semantic transparency. However, for classes of languages where the same expression can take different values in different contexts, we show that emulation can become uncomputable. Finally, we discuss differences between our formal model and natural language, exploring how our results generalize to a modal setting and other semantic relations. Together, our results suggest that assertions in code or language do not provide sufficient signal to fully emulate semantic representations. We formalize ways in which ungrounded language models appear to be fundamentally limited in their ability to \u201cunderstand\u201d."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Zero-shot Neural Transfer for Cross-lingual Entity Linking\nAbstract: Cross-lingual entity linking maps an entity mention in a source language to its corresponding entry in a structured knowledge base that is in a different (target) language. While previous work relies heavily on bilingual lexical resources to bridge the gap between the source and the target languages, these resources are scarce or unavailable for many low-resource languages. To address this problem, we investigate zero-shot cross-lingual entity linking, in which we assume no bilingual lexical resources are available in the source low-resource language. Specifically, we propose pivot-basedentity linking, which leverages information from a highresource \u201cpivot\u201d language to train character-level neural entity linking models that are transferred to the source lowresource language in a zero-shot manner. With experiments on 9 low-resource languages and transfer through a total of54 languages, we show that our proposed pivot-based framework improves entity linking accuracy 17% (absolute) on average over the baseline systems, for the zero-shot scenario.1 Further, we also investigate the use of language-universal phonological representations which improves average accuracy (absolute) by 36% when transferring between languages that use different scripts."}, {"role": "assistant", "content": "1.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Retrieval Augmented Code Generation and Summarization\nAbstract: Software developers write a lot of source code and documentation during software development. Intrinsically, developers often recall parts of source code or code summaries that they had written in the past while implementing software or documenting them. To mimic developers\u2019 code or summary generation behavior, we propose a retrieval augmented framework, REDCODER, that retrieves relevant code or summaries from a retrieval database and provides them as a supplement to code generation or summarization models. REDCODER has a couple of uniqueness. First, it extends the state-of-the-art dense retrieval technique to search for relevant code or summaries. Second, it can work with retrieval databases that include unimodal (only code or natural language description) or bimodal instances (code-description pairs). We conduct experiments and extensive analysis on two benchmark datasets of code generation and summarization in Java and Python, and the promising results endorse the effectiveness of our proposed retrieval augmented framework."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Audio-Visual Instance Discrimination with Cross-Modal Agreement\nAbstract: We present a self-supervised learning approach to learn audio-visual representations from video and audio. Our method uses contrastive learning for cross-modal discrimination of video from audio and vice-versa. We show that optimizing for cross-modal discrimination, rather than within-modal discrimination, is important to learn good representations from video and audio. With this simple but powerful insight, our method achieves highly competitive performance when finetuned on action recognition tasks. Furthermore, while recent work in contrastive learning defines positive and negative samples as individual instances, we generalize this definition by exploring cross-modal agreement. We group together multiple instances as positives by measuring their similarity in both the video and audio feature spaces. Cross-modal agreement creates better positive and negative sets, which allows us to calibrate visual similarities by seeking within-modal discrimination of positive instances, and achieve significant gains on downstream tasks."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Uncovering Coordinated Networks on Social Media\nAbstract: Coordinated campaigns are used to influence and manipulate social media platforms and their users, a critical challenge to the free exchange of information online. Here we introduce a general network-based framework to uncover groups of accounts that are likely coordinated. The proposed method construct coordination networks based on arbitrary behavioral traces shared among accounts. We present five case studies of influence campaigns in the diverse contexts of U.S. elections, Hong Kong protests, the Syrian civil war, and cryptocurrencies. In each of these cases, we detect networks of coordinated Twitter accounts by examining their identities, images, hashtag sequences, retweets, and temporal patterns. The proposed framework proves to be broadly applicable to uncover different kinds of coordination across information warfare scenarios."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Two-sample Test with Kernel Projected Wasserstein Distance\nAbstract: We develop a kernel projected Wasserstein distance for the two-sample test, an essential building block in statistics and machine learning: given two sets of samples, to determine whether they are from the same distribution. This method operates by finding the nonlinear mapping in the data space which maximizes the distance between projected distributions. In contrast to existing works about projected Wasserstein distance, the proposed method circumvents the curse of dimensionality more efficiently. We present practical algorithms for computing this distance function together with the non-asymptotic uncertainty quantification of empirical estimates. Numerical examples validate our theoretical results and demonstrate good performance of the proposed method."}, {"role": "assistant", "content": "4.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: The State and Fate of Linguistic Diversity and Inclusion in the NLP World\nAbstract: Language technologies contribute to promoting multilingualism and linguistic diversity around the world. However, only a very small number of the over 7000 languages of the world are represented in the rapidly evolving language technologies and applications. In this paper we look at the relation between the types of languages, resources, and their representation in NLP conferences to understand the trajectory that different languages have followed over time. Our quantitative investigation underlines the disparity between languages, especially in terms of their resources, and calls into question the \u201clanguage agnostic\u201d status of current models and systems. Through this paper, we attempt to convince the ACL community to prioritise the resolution of the predicaments highlighted here, so that no language is left behind."}, {"role": "assistant", "content": "3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Online Learning Of Neural Computations From Sparse Temporal Feedback\nAbstract: Neuronal computations depend on synaptic connectivity and intrinsic electrophysiological properties. Synaptic connectivity determines which inputs from presynaptic neurons are integrated, while cellular properties determine how inputs are filtered over time. Unlike their biological counterparts, most computational approaches to learning in simulated neural networks are limited to changes in synaptic connectivity. However, if intrinsic parameters change, neural computations are altered drastically. Here, we include the parameters that determine the intrinsic properties, e.g., time constants and reset potential, into the learning paradigm. Using sparse feedback signals that indicate target spike times, and gradient-based parameter updates, we show that the intrinsic parameters can be learned along with the synaptic weights to produce specific input-output functions. Specifically, we use a teacher-student paradigm in which a randomly initialised leaky integrate-and-fire or resonate-and-fire neuron must recover the parameters of a teacher neuron. We show that complex temporal functions can be learned online and without backpropagation through time, relying on event-based updates only. Our results are a step towards online learning of neural computations from ungraded and unsigned sparse feedback signals with a biologically inspired learning mechanism."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: CM3: A Causal Masked Multimodal Model of the Internet\nAbstract: We introduce CM3, a family of causally masked generative models trained over a large corpus of structured multi-modal documents that can contain both text and image tokens. Our new causally masked approach generates tokens left to right while also masking out a small number of long token spans that are generated at the end of the string, instead of their original positions. The casual masking object provides a type of hybrid of the more common causal and masked language models, by enabling full generative modeling while also providing bidirectional context when generating the masked spans. We train causally masked languageimage models on large-scale web and Wikipedia articles, where each document contains all of the text, hypertext markup, hyperlinks, and image tokens (from a VQVAE-GAN), provided in the order they appear in the original HTML source (before masking). The resulting CM3 models can generate rich structured, multimodal outputs while conditioning on arbitrary masked document contexts, and thereby implicitly learn a wide range of text, image, and cross modal tasks. They can be prompted to recover, in a zero-shot fashion, the functionality of models such as DALL-E, GENRE, and HTLM (Ramesh et al., 2021; De Cao et al., 2020; Aghajanyan et al., 2021). We set the new state-of-the-art in zero-shot summarization, entity linking, and entity disambiguation while maintaining competitive performance in the fine-tuning setting. We can generate images unconditionally, conditioned on text (like DALL-E) and do captioning all in a zero-shot setting with a single model."}, {"role": "assistant", "content": "3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Multilingual NER Transfer for Low-resource Languages\nAbstract: In massively multilingual transfer NLP models over many source languages are applied to a low-resource target language. In contrast to most prior work, which use a single model or a small handful, we consider many such models, which raises the critical problem of poor transfer, particularly from distant languages. We propose two techniques for modulating the transfer: one based on unsupervised truth inference, and another using limited supervision in the target language. Evaluating on named entity recognition over 41 languages, we show that our techniques are much more effective than strong baselines, including standard ensembling, and our unsupervised method rivals oracle selection of the single best individual model.2"}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: SimCLS: A Simple Framework for Contrastive Learning of Abstractive Summarization\nAbstract: In this paper, we present a conceptually simple while empirically powerful framework for abstractive summarization, SimCLS, which can bridge the gap between the learning objective and evaluation metrics resulting from the currently dominated sequence-to-sequence learning framework by formulating text generation as a reference-free evaluation problem (i.e., quality estimation) assisted by contrastive learning. Experimental results show that, with minor modification over existing top-scoring systems, SimCLS can improve the performance of existing top-performing models by a large margin. Particularly, 2.51 absolute improvement against BART and 2.50 over PEGASUS w.r.t ROUGE-1 on the CNN/DailyMail dataset, driving the state-of-the-art performance to a new level. We have open-sourced our codes and results: https://github.com/yixinL7/SimCLS. Results of our proposed models have been deployed into ExplainaBoard platform, which allows researchers to understand our systems in a more fine-grained way."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Rumor Detection on Twitter with Claim-Guided Hierarchical Graph Attention Networks\nAbstract: Rumors are rampant in the era of social media. Conversation structures provide valuable clues to differentiate between real and fake claims. However, existing rumor detection methods are either limited to the strict relation of user responses or oversimplify the conversation structure. In this study, to substantially reinforces the interaction of user opinions while alleviating the negative impact imposed by irrelevant posts, we first represent the conversation thread as an undirected interaction graph. We then present a Claim-guided Hierarchical Graph Attention Network for rumor classification, which enhances the representation learning for responsive posts considering the entire social contexts and attends over the posts that can semantically infer the target claim. Extensive experiments on three Twitter datasets demonstrate that our rumor detection method achieves much better performance than state-of-the-art methods and exhibits a superior capacity for detecting rumors at early stages."}, {"role": "assistant", "content": "4.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: TextSETTR: Few-Shot Text Style Extraction and Tunable Targeted Restyling\nAbstract: We present a novel approach to the problem of text style transfer. Unlike previous approaches requiring style-labeled training data, our method makes use of readily-available unlabeled text by relying on the implicit connection in style between adjacent sentences, and uses labeled data only at inference time. We adapt T5 (Raffel et al., 2020), a strong pretrained text-to-text model, to extract a style vector from text and use it to condition the decoder to perform style transfer. As our label-free training results in a style vector space encoding many facets of style, we recast transfers as \u201ctargeted restyling\u201d vector operations that adjust specific attributes of the input while preserving others. We demonstrate that training on unlabeled Amazon reviews data results in a model that is competitive on sentiment transfer, even compared to models trained fully on labeled data. Furthermore, applying our novel method to a diverse corpus of unlabeled web text results in a single model capable of transferring along multiple dimensions of style (dialect, emotiveness, formality, politeness, sentiment) despite no additional training and using only a handful of exemplars at inference time."}, {"role": "assistant", "content": "4.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Content-based Models of Quotation\nAbstract: We explore the task of quotability identification, in which, given a document, we aim to identify which of its passages are the most quotable, i.e. the most likely to be directly quoted by later derived documents. We approach quotability identification as a passage ranking problem and evaluate how well both feature-based and BERT-based (Devlin et al., 2019) models rank the passages in a given document by their predicted quotability. We explore this problem through evaluations on five datasets that span multiple languages (English, Latin) and genres of literature (e.g. poetry, plays, novels) and whose corresponding derived documents are of multiple types (news, journal articles). Our experiments confirm the relatively strong performance of BERT-based models on this task, with the best model, a RoBERTA sequential sentence tagger, achieving an average rho of 0.35 and NDCG@1, 5, 50 of 0.26, 0.31 and 0.40, respectively, across all five datasets."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Counterpoint by Convolution\nAbstract: Machine learning models of music typically break up the task of composition into a chronological process, composing a piece of music in a single pass from beginning to end. On the contrary, human composers write music in a nonlinear fashion, scribbling motifs here and there, often revisiting choices previously made. In order to better approximate this process, we train a convolutional neural network to complete partial musical scores, and explore the use of blocked Gibbs sampling as an analogue to rewriting. Neither the model nor the generative procedure are tied to a particular causal direction of composition. Our model is an instance of orderless NADE (Uria et al., 2014), which allows more direct ancestral sampling. However, we find that Gibbs sampling greatly improves sample quality, which we demonstrate to be due to some conditional distributions being poorly modeled. Moreover, we show that even the cheap approximate blocked Gibbs procedure from Yao et al. (2014) yields better samples than ancestral sampling, based on both log-likelihood and human evaluation."}, {"role": "assistant", "content": "3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus\nAbstract: Large language models have led to remarkable progress on many NLP tasks, and researchers are turning to ever-larger text corpora to train them. Some of the largest corpora available are made by scraping significant portions of the internet, and are frequently introduced with only minimal documentation. In this work we provide some of the first documentation for the Colossal Clean Crawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set of filters to a single snapshot of Common Crawl. We begin by investigating where the data came from, and find a significant amount of text from unexpected sources like patents and US military websites. Then we explore the content of the text itself, and find machine-generated text (e.g., from machine translation systems) and evaluation examples from other benchmark NLP datasets. To understand the impact of the filters applied to create this dataset, we evaluate the text that was removed, and show that blocklist filtering disproportionately removes text from and about minority individuals. Finally, we conclude with some recommendations for how to created and document web-scale datasets from a scrape of the internet."}, {"role": "assistant", "content": "3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: NYTWIT: A Dataset of Novel Words in the New York Times\nAbstract: We present the New York Times Word Innovation Types dataset, or NYTWIT, a collection of over 2,500 novel English words published in the New York Times between November 2017 and March 2019, manually annotated for their class of novelty (such as lexical derivation, dialectal variation, blending, or compounding). We present baseline results for both uncontextual and contextual prediction of novelty class, showing that there is room for improvement even for state-of-the-art NLP systems. We hope this resource will prove useful for linguists and NLP practitioners by providing a real-world environment of novel word appearance."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: PMI-Masking: Principled masking of correlated spans\nAbstract: Masking tokens uniformly at random constitutes a common flaw in the pretraining of Masked Language Models (MLMs) such as BERT. We show that such uniform masking allows an MLM to minimize its training objective by latching onto shallow local signals, leading to pretraining inefficiency and suboptimal downstream performance. To address this flaw, we propose PMI-Masking, a principled masking strategy based on the concept of Pointwise Mutual Information (PMI), which jointly masks a token n-gram if it exhibits high collocation over the corpus. PMI-Masking motivates, unifies, and improves upon prior more heuristic approaches that attempt to address the drawback of random uniform token masking, such as whole-word masking, entity/phrase masking, and random-span masking. Specifically, we show experimentally that PMI-Masking reaches the performance of prior masking approaches in half the training time, and consistently improves performance at the end of training."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Samanantar: The Largest Publicly Available Parallel Corpora Collection for 11 Indic Languages\nAbstract: Abstract We present Samanantar, the largest publicly available parallel corpora collection for Indic languages. The collection contains a total of 49.7 million sentence pairs between English and 11 Indic languages (from two language families). Specifically, we compile 12.4 million sentence pairs from existing, publicly available parallel corpora, and additionally mine 37.4 million sentence pairs from the Web, resulting in a 4\u00d7 increase. We mine the parallel sentences from the Web by combining many corpora, tools, and methods: (a) Web-crawled monolingual corpora, (b) document OCR for extracting sentences from scanned documents, (c) multilingual representation models for aligning sentences, and (d) approximate nearest neighbor search for searching in a large collection of sentences. Human evaluation of samples from the newly mined corpora validate the high quality of the parallel sentences across 11 languages. Further, we extract 83.4 million sentence pairs between all 55 Indic language pairs from the English-centric parallel corpus using English as the pivot language. We trained multilingual NMT models spanning all these languages on Samanantar which outperform existing models and baselines on publicly available benchmarks, such as FLORES, establishing the utility of Samanantar. Our data and models are available publicly at Samanantar and we hope they will help advance research in NMT and multilingual NLP for Indic languages."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: FUDGE: Controlled Text Generation With Future Discriminators\nAbstract: We propose Future Discriminators for Generation (FUDGE), a flexible and modular method for controlled text generation. Given a pre-existing model G for generating text from a distribution of interest, FUDGE enables conditioning on a desired attribute a (for example, formality) while requiring access only to G\u2019s output logits. FUDGE learns an attribute predictor operating on a partial sequence, and uses this predictor\u2019s outputs to adjust G\u2019s original probabilities. We show that FUDGE models terms corresponding to a Bayesian decomposition of the conditional distribution of G given attribute a. Moreover, FUDGE can easily compose predictors for multiple desired attributes. We evaluate FUDGE on three tasks \u2014 couplet completion in poetry, topic control in language generation, and formality change in machine translation \u2014 and observe gains in all three tasks."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Diagnosing the First-Order Logical Reasoning Ability Through LogicNLI\nAbstract: Recently, language models (LMs) have achieved significant performance on many NLU tasks, which has spurred widespread interest for their possible applications in the scientific and social area. However, LMs have faced much criticism of whether they are truly capable of reasoning in NLU. In this work, we propose a diagnostic method for first-order logic (FOL) reasoning with a new proposed benchmark, LogicNLI. LogicNLI is an NLI-style dataset that effectively disentangles the target FOL reasoning from commonsense inference and can be used to diagnose LMs from four perspectives: accuracy, robustness, generalization, and interpretability. Experiments on BERT, RoBERTa, and XLNet, have uncovered the weaknesses of these LMs on FOL reasoning, which motivates future exploration to enhance the reasoning ability."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Conditional Poisson Stochastic Beam Search\nAbstract: Beam search is the default decoding strategy for many sequence generation tasks in NLP. The set of approximate K-best items returned by the algorithm is a useful summary of the distribution for many applications; however, the candidates typically exhibit high overlap and may give a highly biased estimate for expectations under our model. These problems can be addressed by instead using stochastic decoding strategies. In this work, we propose a new method for turning beam search into a stochastic process: Conditional Poisson stochastic beam search. Rather than taking the maximizing set at each iteration, we sampleK candidates without replacement according to the conditional Poisson sampling design. We view this as a more natural alternative to Kool et al. (2019)\u2019s stochastic beam search (SBS). Furthermore, we show how samples generated under the CPSBS design can be used to build consistent estimators and sample diverse sets from sequence models. In our experiments, we observe CPSBS produces lower variance and more efficient estimators than SBS, even showing improvements in high entropy settings.1"}, {"role": "assistant", "content": "3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Our Data, Ourselves: Privacy Via Distributed Noise Generation\nAbstract: In this work we provide efficient distributed protocols for generating shares of random noise, secure against malicious participants. The purpose of the noise generation is to create a distributed implementation of the privacy-preserving statistical databases described in recent papers [14,4,13]. In these databases, privacy is obtained by perturbing the true answer to a database query by the addition of a small amount of Gaussian or exponentially distributed random noise. The computational power of even a simple form of these databases, when the query is just of the form \u2211if(di), that is, the sum over all rows i in the database of a function f applied to the data in row i, has been demonstrated in [4]. A distributed implementation eliminates the need for a trusted database administrator. \n \nThe results for noise generation are of independent interest. The generation of Gaussian noise introduces a technique for distributing shares of many unbiased coins with fewer executions of verifiable secret sharing than would be needed using previous approaches (reduced by a factor of n). The generation of exponentially distributed noise uses two shallow circuits: one for generating many arbitrarily but identically biased coins at an amortized cost of two unbiased random bits apiece, independent of the bias, and the other to combine bits of appropriate biases to obtain an exponential distribution."}, {"role": "assistant", "content": "1.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Fairness-Aware Tensor-Based Recommendation\nAbstract: Tensor-based methods have shown promise in improving upon traditional matrix factorization methods for recommender systems. But tensors may achieve improved recommendation quality while worsening the fairness of the recommendations. Hence, we propose a novel fairness-aware tensor recommendation framework that is designed to maintain quality while dramatically improving fairness. Four key aspects of the proposed framework are: (i) a new sensitive latent factor matrix for isolating sensitive features; (ii) a sensitive information regularizer that extracts sensitive information which can taint other latent factors; (iii) an effective algorithm to solve the proposed optimization model; and (iv) extension to multi-feature and multi-category cases which previous efforts have not addressed. Extensive experiments on real-world and synthetic datasets show that the framework enhances recommendation fairness while preserving recommendation quality in comparison with state-of-the-art alternatives."}, {"role": "assistant", "content": "2.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Functional Broadcast Repair of Multiple Partial Failures in Wireless Distributed Storage Systems\nAbstract: We consider a distributed storage system with <inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula> nodes, where a user can recover the stored file from any <inline-formula> <tex-math notation=\"LaTeX\">$k$ </tex-math></inline-formula> nodes, and study the problem of repairing <inline-formula> <tex-math notation=\"LaTeX\">$r$ </tex-math></inline-formula> partially failed nodes. We consider <italic>broadcast repair</italic>, that is, <inline-formula> <tex-math notation=\"LaTeX\">$d$ </tex-math></inline-formula> surviving nodes transmit broadcast messages on an error-free wireless channel to the <inline-formula> <tex-math notation=\"LaTeX\">$r$ </tex-math></inline-formula> nodes being repaired, which are then used, together with the surviving data in the local memories of the failed nodes, to recover the lost content. First, we derive the trade-off between the storage capacity and the repair bandwidth for partial repair of multiple failed nodes, based on the cut-set bound for information flow graphs. It is shown that utilizing the broadcast nature of the wireless medium and the surviving contents at the partially failed nodes reduces the repair bandwidth per node significantly. Then, we list a set of invariant conditions that are sufficient for a functional repair code to be feasible. We further propose a scheme for functional repair of multiple failed nodes that satisfies the invariant conditions with high probability, and its extension to the repair of partial failures. The performance of the proposed scheme meets the cut-set bound on all the points on the trade-off curve for all admissible parameters when <inline-formula> <tex-math notation=\"LaTeX\">$k$ </tex-math></inline-formula> is divisible by <inline-formula> <tex-math notation=\"LaTeX\">$r$ </tex-math></inline-formula>, while employing linear subpacketization, which is an important practical consideration in the design of distributed storage codes. Unlike random linear codes, which are conventionally used for functional repair of failed nodes, the proposed repair scheme has lower overhead, lower input-output cost, and lower computational complexity during repair."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Text Classification Using Label Names Only: A Language Model Self-Training Approach\nAbstract: Current text classification methods typically require a good number of human-labeled documents as training data, which can be costly and difficult to obtain in real applications. Humans can perform classification without seeing any labeled examples but only based on a small set of words describing the categories to be classified. In this paper, we explore the potential of only using the label name of each class to train classification models on unlabeled data, without using any labeled documents. We use pre-trained neural language models both as general linguistic knowledge sources for category understanding and as representation learning models for document classification. Our method (1) associates semantically related words with the label names, (2) finds category-indicative words and trains the model to predict their implied categories, and (3) generalizes the model via self-training. We show that our model achieves around 90% accuracy on four benchmark datasets including topic and sentiment classification without using any labeled documents but learning from unlabeled data supervised by at most 3 words (1 in most cases) per class as the label name."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Asking and Answering Questions to Evaluate the Factual Consistency of Summaries\nAbstract: Practical applications of abstractive summarization models are limited by frequent factual inconsistencies with respect to their input. Existing automatic evaluation metrics for summarization are largely insensitive to such errors. We propose QAGS (pronounced \u201ckags\u201d), an automatic evaluation protocol that is designed to identify factual inconsistencies in a generated summary. QAGS is based on the intuition that if we ask questions about a summary and its source, we will receive similar answers if the summary is factually consistent with the source. To evaluate QAGS, we collect human judgments of factual consistency on model-generated summaries for the CNN/DailyMail (Hermann et al., 2015) and XSUM (Narayan et al., 2018) summarization datasets. QAGS has substantially higher correlations with these judgments than other automatic evaluation metrics. Also, QAGS offers a natural form of interpretability: The answers and questions generated while computing QAGS indicate which tokens of a summary are inconsistent and why. We believe QAGS is a promising tool in automatically generating usable and factually consistent text. Code for QAGS will be available at https://github.com/W4ngatang/qags."}, {"role": "assistant", "content": "2.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Competitive analysis of the top-K ranking problem\nAbstract: Motivated by applications in recommender systems, web search, social choice and crowdsourcing, we consider the problem of identifying the set of top K items from noisy pairwise comparisons. In our setting, we are non-actively given r pairwise comparisons between each pair of n items, where each comparison has noise constrained by a very general noise model called the strong stochastic transitivity (SST) model. We analyze the competitive ratio of algorithms for the top-K problem. In particular, we present a linear time algorithm for the top-K problem which has a competitive ratio of O( \u221a n); i.e. to solve any instance of top-K, our algorithm needs at most O( \u221a n) times as many samples needed as the best possible algorithm for that instance (in contrast, all previous known algorithms for the top-K problem have competitive ratios of \u03a9(n) or worse). We further show that this is tight: any algorithm for the top-K problem has competitive ratio at least \u03a9( \u221a n). Stern School of Business, New York University, email: xchen3@stern.nyu.edu Department of Computer Science, Princeton University, email: sgopi@cs.princeton.edu Department of Computer Science, Princeton University, email: jiemingm@cs.princeton.edu Department of Computer Science, Princeton University, email: js44@cs.princeton.edu 1"}, {"role": "assistant", "content": "4.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Explaining Neural Scaling Laws\nAbstract: The test loss of well-trained neural networks often follows precise power-law scaling relations with either the size of the training dataset or the number of parameters in the network. We propose a theory that explains and connects these scaling laws. We identify variance-limited and resolution-limited scaling behavior for both dataset and model size, for a total of four scaling regimes. The variance-limited scaling follows simply from the existence of a well-behaved infinite data or infinite width limit, while the resolution-limited regime can be explained by positing that models are effectively resolving a smooth data manifold. In the large width limit, this can be equivalently obtained from the spectrum of certain kernels, and we present evidence that large width and large dataset resolution-limited scaling exponents are related by a duality. We exhibit all four scaling regimes in the controlled setting of large random feature and pretrained models and test the predictions empirically on a range of standard architectures and datasets. We also observe several empirical relationships between datasets and scaling exponents: super-classing image tasks does not change exponents, while changing input distribution (via changing datasets or adding noise) has a strong effect. We further explore the effect of architecture aspect ratio on scaling exponents. 1 Scaling Laws for Neural Networks For a large variety of models and datasets, neural network performance has been empirically observed to scale as a power-law with model size and dataset size [1\u20134]. We would like to understand why these power laws emerge, and what features of the data and models determine the values of the power-law exponents. Since these exponents determine how quickly performance improves with more data and larger models, they are of great importance when considering whether to scale up existing models. In this work, we present a theoretical framework for explaining scaling laws in trained neural networks. We identify four related scaling regimes with respect to the number of model parameters P and the dataset size D. With respect to each of D, P , there is both a resolution-limited regime and a variance-limited regime. Variance-Limited Regime In the limit of infinite data or an arbitrarily wide model, some aspects of neural network training simplify. Specifically, if we fix one of D,P and study scaling with respect to the other parameter as it becomes arbitrarily large, then the loss scales as 1/x, i.e. as a power-law with exponent 1, with x = D or \u221a P \u221d width in deep networks and x = D or P in linear models. In essence, this variance-limited regime is amenable to analysis because model predictions can be series expanded in either inverse width or inverse dataset size. To demonstrate these variance-limited scalings, it is sufficient to argue that the infinite data or width limit exists and is smooth; this guarantees that an expansion in simple integer powers exists. \u2217Authors listed alphabetically \u2020A portion of work completed during an internship at Google. 1 ar X iv :2 10 2. 06 70 1v 1 [ cs .L G ] 1 2 Fe b 20 21 101 102 103 104 Dataset size (D) 10 8 10 6 10 4 10 2 100 Lo ss Lo ss ( ) Variance-limited : Theory D = 1 D: 1.02 D: 1.01 D: 1.00 (MSE) D: 0.98 (CNN) D: 1.01 D: 1.02 D: 1.10 D: 1.01 103 104 Dataset size (D) 10 1 100 Lo ss Resolution-limited D: 0.26 D: 0.37 D: 0.40 D: 0.58 101 102 Width 10 1 100 Lo ss Resolution-limited W: 0.46 W: 0.34 W: 0.62 W: 0.40 102 103 104 Width 10 5 10 4 10 3 10 2 10 1 100 Lo ss Lo ss ( ) Variance-limited : Theory W = 1 W: 0.98 (MSE, ERF) W: 1.03 W: 1.02 (ERF) W: 1.01 W: 1.00 W: 1.03 (ERF) Teacher-Student CIFAR-10 CIFAR-100 SVHN FashionMNIST MNIST Figure 1: Four scaling regimes Here we exhibit the four regimes we focus on in this work. (top-left, bottomright) Variance-limited scaling of under-parameterized models with dataset size and over-parameterized models with number of parameters (width) exhibit universal scaling (\u03b1D = \u03b1W = 1) independent of the architecture or underlying dataset. (top-right, bottom-left) Resolution-limited over-parameterized models with dataset or under-parameterized models with model size exhibit scaling with exponents that depend on the details of the data distribution. These four regimes are also found in random feature (Figure 3) and pretrained models (see supplement). Resolution-Limited Regime In this regime, one of D or P is effectively infinite, and we study scaling as the other parameter increases. In this case, a variety of works have empirically observed power-law scalings 1/x, typically with 0 < \u03b1 < 1 for both x = P or D. We can provide a very general argument for power-law scalings if we assume that trained models map the data into a d-dimensional data manifold. The key idea is then that additional data (in the infinite model-size limit) or added model parameters (in the infinite data limit) are used by the model to carve up the data manifold into smaller components. The model then makes independent predictions in each component of the data manifold in order to optimize the training loss. If the underlying data varies continuously on the manifold, then the size of the sub-regions into which we can divide the manifold (rather than the number of regions) determines the model\u2019s loss. To shrink the size of the sub-regions by a factor of 2 requires increasing the parameter count or dataset size by a factor of 2, and so the inverse of the scaling exponent will be proportional to the intrinsic dimension d of the data manifold, so that \u03b1 \u221d 1/d. A visualization of this successively better approximation with dataset size is shown in Figure 2 for models trained to predict data generated by a random fully-connected network. Explicit Realization These regimes can be realized in linear models, and this includes linearized versions of neural networks via the large width limit. In these limits, we can solve for the test error directly in terms of the feature covariance (kernel). The scaling of the test loss then follows from the asymptotic decay of the spectrum of the covariance matrix. Furthermore, well-known theorems provide bounds on the spectra associated with continuous kernels on a d-dimensional manifold. Since otherwise generic kernels saturate these bounds, we find a tight connection between the dimension of the data manifold, kernel spectra, and"}, {"role": "assistant", "content": "1.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Human Uncertainty Makes Classification More Robust\nAbstract: The classification performance of deep neural networks has begun to asymptote at near-perfect levels. However, their ability to generalize outside the training set and their robustness to adversarial attacks have not. In this paper, we make progress on this problem by training with full label distributions that reflect human perceptual uncertainty. We first present a new benchmark dataset which we call CIFAR10H, containing a full distribution of human labels for each image of the CIFAR10 test set. We then show that, while contemporary classifiers fail to exhibit human-like uncertainty on their own, explicit training on our dataset closes this gap, supports improved generalization to increasingly out-of-training-distribution test datasets, and confers robustness to adversarial attacks."}, {"role": "assistant", "content": "3.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Handling Syntactic Divergence in Low-resource Machine Translation\nAbstract: Despite impressive empirical successes of neural machine translation (NMT) on standard benchmarks, limited parallel data impedes the application of NMT models to many language pairs. Data augmentation methods such as back-translation make it possible to use monolingual data to help alleviate these issues, but back-translation itself fails in extreme low-resource scenarios, especially for syntactically divergent languages. In this paper, we propose a simple yet effective solution, whereby target-language sentences are re-ordered to match the order of the source and used as an additional source of training-time supervision. Experiments with simulated low-resource Japanese-to-English, and real low-resource Uyghur-to-English scenarios find significant improvements over other semi-supervised alternatives."}, {"role": "assistant", "content": "3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: TAPEX: Table Pre-training via Learning a Neural SQL Executor\nAbstract: Recent years pre-trained language models hit a success on modeling natural language sentences and (semi-)structured tables. However, existing table pre-training techniques always suffer from low data quality and low pre-training efficiency. In this paper, we show that table pre-training can be realized by learning a neural SQL executor over a synthetic corpus, which is obtained by automatically synthesizing executable SQL queries. By pre-training on the synthetic corpus, our approach TAPEX dramatically improves the performance on downstream tasks, boosting existing language models by at most 19.5%. Meanwhile, TAPEX has remarkably high pretraining efficiency and yields strong results when using a small pre-trained corpus. Experimental results demonstrate that TAPEX outperforms previous table pre-training approaches by a large margin, and our model achieves new state-of-the-art results on four well-known datasets, including improving the WIKISQL denotation accuracy to 89.6% (+4.9%), the WIKITABLEQUESTIONS denotation accuracy to 57.5% (+4.8%), the SQA denotation accuracy to 74.5% (+3.5%), and the TABFACT accuracy to 84.6% (+3.6%). Our work opens the way to reason over structured data by pre-training on synthetic executable programs. The project homepage is at https: //table-pretraining.github.io/."}, {"role": "assistant", "content": "3.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Symbolic Knowledge Distillation: from General Language Models to Commonsense Models\nAbstract: The common practice for training commonsense models has gone from\u2013human\u2013to\u2013 corpus\u2013to\u2013machine: humans author commonsense knowledge graphs in order to train commonsense models. In this work, we investigate an alternative, from\u2013machine\u2013to\u2013corpus\u2013 to\u2013machine: general language models author these commonsense knowledge graphs to train commonsense models. Our study leads to a new framework, Symbolic Knowledge Distillation. As with prior art in Knowledge Distillation (Hinton et al., 2015), our approach uses larger models to teach smaller models. A key difference is that we distill knowledge symbolically\u2013as text\u2013in addition to the neural model. We also distill only one aspect\u2013the commonsense of a general language model teacher, allowing the student to be a different type, a commonsense model. Altogether, we show that careful prompt engineering and a separately trained critic model allow us to selectively distill high-quality causal commonsense from GPT-3, a general language model. Empirical results demonstrate that, for the first time, a human-authored commonsense knowledge graph is surpassed by our automatically distilled variant in all three criteria: quantity, quality, and diversity. In addition, it results in a neural commonsense model that surpasses the teacher model\u2019s commonsense capabilities despite its 100x smaller size. We apply this to the ATOMIC resource, and share our new symbolic knowledge graph and commonsense models1."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Classification with Strategically Withheld Data\nAbstract: Machine learning techniques can be useful in applications such as credit approval and college admission. However, to be classified more favorably in such contexts, an agent may decide to strategically withhold some of her features, such as bad test scores. This is a missing data problem with a twist: which data is missing depends on the chosen classifier, because the specific classifier is what may create the incentive to withhold certain feature values. We address the problem of training classifiers that are robust to this behavior. We design three classification methods: MINCUT, HILLCLIMBING (HC) and Incentive-Compatible Logistic Regression (IC-LR). We show that MINCUT is optimal when the true distribution of data is fully known. However, it can produce complex decision boundaries, and hence be prone to overfitting in some cases. Based on a characterization of truthful classifiers (i.e., those that give no incentive to strategically hide features), we devise a simpler alternative called HC which consists of a hierarchical ensemble of out-of-thebox classifiers, trained using a specialized hill-climbing procedure which we show to be convergent. For several reasons, MINCUT and HC are not effective in utilizing a large number of complementarily informative features. To this end, we present IC-LR, a modification of Logistic Regression that removes the incentive to strategically drop features. We also show that our algorithms perform well in experiments on realworld data sets, and present insights into their relative performance in different settings."}, {"role": "assistant", "content": "3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Synthetic Benchmarks for Scientific Research in Explainable Machine Learning\nAbstract: As machine learning models grow more complex and their applications become more high-stakes, tools for explaining model predictions have become increasingly important. This has spurred a flurry of research in model explainability and has given rise to feature attribution methods such as LIME and SHAP. Despite their widespread use, evaluating and comparing different feature attribution methods remains challenging: evaluations ideally require human studies, and empirical evaluation metrics are often data-intensive or computationally prohibitive on realworld datasets. In this work, we address this issue by releasing XAI-BENCH: a suite of synthetic datasets along with a library for benchmarking feature attribution algorithms. Unlike real-world datasets, synthetic datasets allow the efficient computation of conditional expected values that are needed to evaluate groundtruth Shapley values and other metrics. The synthetic datasets we release offer a wide variety of parameters that can be configured to simulate real-world data. We demonstrate the power of our library by benchmarking popular explainability techniques across several evaluation metrics and across a variety of settings. The versatility and efficiency of our library will help researchers bring their explainability methods from development to deployment. Our code is available at https://github.com/abacusai/xai-bench."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: A Unified Theory of Decentralized SGD with Changing Topology and Local Updates\nAbstract: Decentralized stochastic optimization methods have gained a lot of attention recently, mainly because of their cheap per iteration cost, data locality, and their communication-efficiency. In this paper we introduce a unified convergence analysis that covers a large variety of decentralized SGD methods which so far have required different intuitions, have different applications, and which have been developed separately in various communities. \nOur algorithmic framework covers local SGD updates and synchronous and pairwise gossip updates on adaptive network topology. We derive universal convergence rates for smooth (convex and non-convex) problems and the rates interpolate between the heterogeneous (non-identically distributed data) and iid-data settings, recovering linear convergence rates in many special cases, for instance for over-parametrized models. Our proofs rely on weak assumptions (typically improving over prior work in several aspects) and recover (and improve) the best known complexity results for a host of important scenarios, such as for instance coorperative SGD and federated averaging (local SGD)."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: QMIX: Monotonic Value Function Factorisation for Deep Multi-Agent Reinforcement Learning\nAbstract: In many real-world settings, a team of agents must coordinate their behaviour while acting in a decentralised way. At the same time, it is often possible to train the agents in a centralised fashion in a simulated or laboratory setting, where global state information is available and communication constraints are lifted. Learning joint action-values conditioned on extra state information is an attractive way to exploit centralised learning, but the best strategy for then extracting decentralised policies is unclear. Our solution is QMIX, a novel value-based method that can train decentralised policies in a centralised end-to-end fashion. QMIX employs a network that estimates joint action-values as a complex non-linear combination of per-agent values that condition only on local observations. We structurally enforce that the joint-action value is monotonic in the per-agent values, which allows tractable maximisation of the joint action-value in off-policy learning, and guarantees consistency between the centralised and decentralised policies. We evaluate QMIX on a challenging set of StarCraft II micromanagement tasks, and show that QMIX significantly outperforms existing value-based multi-agent reinforcement learning methods."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: FastEmit: Low-Latency Streaming ASR with Sequence-Level Emission Regularization\nAbstract: Streaming automatic speech recognition (ASR) aims to emit each hypothesized word as quickly and accurately as possible. However, emitting fast without degrading quality, as measured by word error rate (WER), is highly challenging. Existing approaches including Early and Late Penalties [1] and Constrained Alignments [2], [3] penalize emission delay by manipulating per-token or per-frame probability prediction in sequence transducer models [4]. While being successful in reducing delay, these approaches suffer from significant accuracy regression and also require additional word alignment information from an existing model. In this work, we propose a sequence-level emission regularization method, named FastEmit, that applies latency regularization directly on per-sequence probability in training transducer models, and does not require any alignment. We demonstrate that FastEmit is more suitable to the sequence-level optimization of transducer models [4] for streaming ASR by applying it on various end-to-end streaming ASR networks including RNN-Transducer [5], Transformer-Transducer [6], [7], ConvNet-Transducer [8] and Conformer-Transducer [9]. We achieve 150 ~ 300ms latency reduction with significantly better accuracy over previous techniques on a Voice Search test set. FastEmit also improves streaming ASR accuracy from 4.4%/8.9% to 3.1%/7.5% WER, meanwhile reduces 90th percentile latency from 210ms to only 30ms on LibriSpeech."}, {"role": "assistant", "content": "3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Few-shot Object Grounding and Mapping for Natural Language Robot Instruction Following\nAbstract: We study the problem of learning a robot policy to follow natural language instructions that can be easily extended to reason about new objects. We introduce a few-shot language-conditioned object grounding method trained from augmented reality data that uses exemplars to identify objects and align them to their mentions in instructions. We present a learned map representation that encodes object locations and their instructed use, and construct it from our few-shot grounding output. We integrate this mapping approach into an instruction-following policy, thereby allowing it to reason about previously unseen objects at test-time by simply adding exemplars. We evaluate on the task of learning to map raw observations and instructions to continuous control of a physical quadcopter. Our approach significantly outperforms the prior state of the art in the presence of new objects, even when the prior approach observes all objects during training."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Pre-training via Paraphrasing\nAbstract: We introduce MARGE, a pre-trained sequence-to-sequence model learned with an unsupervised multi-lingual multi-document paraphrasing objective. MARGE provides an alternative to the dominant masked language modeling paradigm, where we self-supervise the reconstruction of target text by retrieving a set of related texts (in many languages) and conditioning on them to maximize the likelihood of generating the original. We show it is possible to jointly learn to do retrieval and reconstruction, given only a random initialization. The objective noisily captures aspects of paraphrase, translation, multi-document summarization, and information retrieval, allowing for strong zero-shot performance on several tasks. For example, with no additional task-specific training we achieve BLEU scores of up to 35.8 for document translation. We further show that fine-tuning gives strong performance on a range of discriminative and generative tasks in many languages, making MARGE the most generally applicable pre-training method to date."}, {"role": "assistant", "content": "3.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: QA4IE: A Question Answering based Framework for Information Extraction\nAbstract: Information Extraction (IE) refers to automatically extracting structured relation tuples from unstructured texts. Common IE solutions, including Relation Extraction (RE) and open IE systems, can hardly handle cross-sentence tuples, and are severely restricted by limited relation types as well as informal relation specifications (e.g., free-text based relation tuples). In order to overcome these weaknesses, we propose a novel IE framework named QA4IE, which leverages the flexible question answering (QA) approaches to produce high quality relation triples across sentences. Based on the framework, we develop a large IE benchmark with high quality human evaluation. This benchmark contains 293K documents, 2M golden relation triples, and 636 relation types. We compare our system with some IE baselines on our benchmark and the results show that our system achieves great improvements."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Personalized Dialogue Generation with Diversified Traits\nAbstract: Endowing a dialogue system with particular personality traits is essential to deliver more human-like conversations. However, due to the challenge of embodying personality via language expression and the lack of large-scale persona-labeled dialogue data, this research problem is still far from well-studied. In this paper, we investigate the problem of incorporating explicit personality traits in dialogue generation to deliver personalized dialogues. \nTo this end, firstly, we construct PersonalDialog, a large-scale multi-turn dialogue dataset containing various traits from a large number of speakers. The dataset consists of 20.83M sessions and 56.25M utterances from 8.47M speakers. Each utterance is associated with a speaker who is marked with traits like Age, Gender, Location, Interest Tags, etc. Several anonymization schemes are designed to protect the privacy of each speaker. This large-scale dataset will facilitate not only the study of personalized dialogue generation, but also other researches on sociolinguistics or social science. \nSecondly, to study how personality traits can be captured and addressed in dialogue generation, we propose persona-aware dialogue generation models within the sequence to sequence learning framework. Explicit personality traits (structured by key-value pairs) are embedded using a trait fusion module. During the decoding process, two techniques, namely persona-aware attention and persona-aware bias, are devised to capture and address trait-related information. Experiments demonstrate that our model is able to address proper traits in different contexts. Case studies also show interesting results for this challenging research problem."}, {"role": "assistant", "content": "3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Superbizarre Is Not Superb: Improving BERT's Interpretations of Complex Words with Derivational Morphology\nAbstract: How does the input segmentation of pretrained language models (PLMs) affect their generalization capabilities? We present the first study investigating this question, taking BERT as the example PLM and focusing on the semantic representations of derivationally complex words. We show that PLMs can be interpreted as serial dual-route models, i.e., the meanings of complex words are either stored or else need to be computed from the subwords, which implies that maximally meaningful input tokens should allow for the best generalization on new words. This hypothesis is confirmed by a series of semantic probing tasks on which derivational segmentation consistently outperforms BERT\u2019s WordPiece segmentation by a large margin. Our results suggest that the generalization capabilities of PLMs could be further improved if a morphologically-informed vocabulary of input tokens were used."}, {"role": "assistant", "content": "4.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Estimating predictive uncertainty for rumour verification models\nAbstract: The inability to correctly resolve rumours circulating online can have harmful real-world consequences. We present a method for incorporating model and data uncertainty estimates into natural language processing models for automatic rumour verification. We show that these estimates can be used to filter out model predictions likely to be erroneous so that these difficult instances can be prioritised by a human fact-checker. We propose two methods for uncertainty-based instance rejection, supervised and unsupervised. We also show how uncertainty estimates can be used to interpret model performance as a rumour unfolds."}, {"role": "assistant", "content": "4.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Condenser: a Pre-training Architecture for Dense Retrieval\nAbstract: Pre-trained Transformer language models (LM) have become go-to text representation encoders. Prior research fine-tunes deep LMs to encode text sequences such as sentences and passages into single dense vector representations for efficient text comparison and retrieval. However, dense encoders require a lot of data and sophisticated techniques to effectively train and suffer in low data situations. This paper finds a key reason is that standard LMs\u2019 internal attention structure is not ready-to-use for dense encoders, which needs to aggregate text information into the dense representation. We propose to pre-train towards dense encoder with a novel Transformer architecture, Condenser, where LM prediction CONditions on DENSE Representation. Our experiments show Condenser improves over standard LM by large margins on various text retrieval and similarity tasks."}, {"role": "assistant", "content": "2.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Interpreting Interpretability: Understanding Data Scientists' Use of Interpretability Tools for Machine Learning\nAbstract: Machine learning (ML) models are now routinely deployed in domains ranging from criminal justice to healthcare. With this newfound ubiquity, ML has moved beyond academia and grown into an engineering discipline. To that end, interpretability tools have been designed to help data scientists and machine learning practitioners better understand how ML models work. However, there has been little evaluation of the extent to which these tools achieve this goal. We study data scientists' use of two existing interpretability tools, the InterpretML implementation of GAMs and the SHAP Python package. We conduct a contextual inquiry (N=11) and a survey (N=197) of data scientists to observe how they use interpretability tools to uncover common issues that arise when building and evaluating ML models. Our results indicate that data scientists over-trust and misuse interpretability tools. Furthermore, few of our participants were able to accurately describe the visualizations output by these tools. We highlight qualitative themes for data scientists' mental models of interpretability tools. We conclude with implications for researchers and tool designers, and contextualize our findings in the social science literature."}, {"role": "assistant", "content": "4.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: End-to-End Learning of Flowchart Grounded Task-Oriented Dialogs\nAbstract: We propose a novel problem within end-to-end learning of task oriented dialogs (TOD), in which the dialog system mimics a troubleshooting agent who helps a user by diagnosing their problem (e.g., car not starting). Such dialogs are grounded in domain-specific flowcharts, which the agent is supposed to follow during the conversation. Our task exposes novel technical challenges for neural TOD, such as grounding an utterance to the flowchart without explicit annotation, referring to additional manual pages when user asks a clarification question, and ability to follow unseen flowcharts at test time. We release a dataset (FLODIAL) consisting of 2,738 dialogs grounded on 12 different troubleshooting flowcharts. We also design a neural model, FLONET, which uses a retrieval-augmented generation architecture to train the dialog agent. Our experiments find that FLONET can do zero-shot transfer to unseen flowcharts, and sets a strong baseline for future research."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Mitigating Backdoor Attacks in Federated Learning\nAbstract: Malicious clients can attack federated learning systems by using malicious data, including backdoor samples, during the training phase. The compromised global model will perform well on the validation dataset designed for the task. However, a small subset of data with backdoor patterns may trigger the model to make a wrong prediction. Previously, there was an arms race. Attackers tried to conceal attacks and defenders tried to detect attacks during the aggregation stage of training on the server-side in a federated learning system. In this work, we propose a new method to mitigate backdoor attacks after the training phase. Specifically, we designed a federated pruning method to remove redundant neurons in the network and then adjust the model's extreme weight values. Experiments conducted on distributed Fashion-MNIST have shown that our method can reduce the average attack success rate from 99.7% to 1.9% with a 5.5% loss of test accuracy on the validation dataset. To minimize the pruning influence on test accuracy, we can fine-tune after pruning, and the attack success rate drops to 6.4%, with only a 1.7% loss of test accuracy."}, {"role": "assistant", "content": "4.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Contextual Games: Multi-Agent Learning with Side Information\nAbstract: We formulate the novel class of contextual games, a type of repeated games driven by contextual information at each round. By means of kernel-based regularity assumptions, we model the correlation between different contexts and game outcomes and propose a novel online (meta) algorithm that exploits such correlations to minimize the contextual regret of individual players. We define game-theoretic notions of contextual Coarse Correlated Equilibria (c-CCE) and optimal contextual welfare for this new class of games and show that c-CCEs and optimal welfare can be approached whenever players\u2019 contextual regrets vanish. Finally, we empirically validate our results in a traffic routing experiment, where our algorithm leads to better performance and higher welfare compared to baselines that do not exploit the available contextual information or the correlations present in the game."}, {"role": "assistant", "content": "4.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: A simple adaptive procedure leading to correlated equilibrium\nAbstract: We propose a new and simple adaptive procedure for playing a game: \u2018\u2018regret-matching.\u2019\u2019 In this procedure, players may depart from their current play with probabilities that are proportional to measures of regret for not having used other strategies in the past. It is shown that our adaptive procedure guarantees that, with probability one, the empirical distributions of play converge to the set of correlated equilibria of the game."}, {"role": "assistant", "content": "4.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Beyond the Pixel-Wise Loss for Topology-Aware Delineation\nAbstract: Delineation of curvilinear structures is an important problem in Computer Vision with multiple practical applications. With the advent of Deep Learning, many current approaches on automatic delineation have focused on finding more powerful deep architectures, but have continued using the habitual pixel-wise losses such as binary cross-entropy. In this paper we claim that pixel-wise losses alone are unsuitable for this problem because of their inability to reflect the topological impact of mistakes in the final prediction. We propose a new loss term that is aware of the higher-order topological features of linear structures. We also exploit a refinement pipeline that iteratively applies the same model over the previous delineation to refine the predictions at each step, while keeping the number of parameters and the complexity of the model constant. When combined with the standard pixel-wise loss, both our new loss term and an iterative refinement boost the quality of the predicted delineations, in some cases almost doubling the accuracy as compared to the same classifier trained with the binary cross-entropy alone. We show that our approach outperforms state-of-the-art methods on a wide range of data, from microscopy to aerial images."}, {"role": "assistant", "content": "3.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: \u201cEveryone wants to do the model work, not the data work\u201d: Data Cascades in High-Stakes AI\nAbstract: AI models are increasingly applied in high-stakes domains like health and conservation. Data quality carries an elevated significance in high-stakes AI due to its heightened downstream impact, impacting predictions like cancer detection, wildlife poaching, and loan allocations. Paradoxically, data is the most under-valued and de-glamorised aspect of AI. In this paper, we report on data practices in high-stakes AI, from interviews with 53 AI practitioners in India, East and West African countries, and USA. We define, identify, and present empirical evidence on Data Cascades\u2014compounding events causing negative, downstream effects from data issues\u2014triggered by conventional AI/ML practices that undervalue data quality. Data cascades are pervasive (92% prevalence), invisible, delayed, but often avoidable. We discuss HCI opportunities in designing and incentivizing data excellence as a first-class citizen of AI, resulting in safer and more robust systems for all."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Parameter Prediction for Unseen Deep Architectures\nAbstract: Deep learning has been successful in automating the design of features in machine learning pipelines. However, the algorithms optimizing neural network parameters remain largely hand-designed and computationally inefficient. We study if we can use deep learning to directly predict these parameters by exploiting the past knowledge of training other networks. We introduce a large-scale dataset of diverse computational graphs of neural architectures \u2013 DEEPNETS-1M\u2013 and use it to explore parameter prediction on CIFAR-10 and ImageNet. By leveraging advances in graph neural networks, we propose a hypernetwork that can predict performant parameters in a single forward pass taking a fraction of a second, even on a CPU. The proposed model achieves surprisingly good performance on unseen and diverse networks. For example, it is able to predict all 24 million parameters of a ResNet-50 achieving a 60% accuracy on CIFAR-10. On ImageNet, top-5 accuracy of some of our networks approaches 50%. Our task along with the model and results can potentially lead to a new, more computationally efficient paradigm of training networks. Our model also learns a strong representation of neural architectures enabling their analysis."}, {"role": "assistant", "content": "3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Multinomial Inverse Regression for Text Analysis\nAbstract: Text data, including speeches, stories, and other document forms, are often connected to sentiment variables that are of interest for research in marketing, economics, and elsewhere. It is also very high dimensional and difficult to incorporate into statistical analyses. This article introduces a straightforward framework of sentiment-sufficient dimension reduction for text data. Multinomial inverse regression is introduced as a general tool for simplifying predictor sets that can be represented as draws from a multinomial distribution, and we show that logistic regression of phrase counts onto document annotations can be used to obtain low-dimensional document representations that are rich in sentiment information. To facilitate this modeling, a novel estimation technique is developed for multinomial logistic regression with very high-dimensional response. In particular, independent Laplace priors with unknown variance are assigned to each regression coefficient, and we detail an efficient routine for maximization of the joint posterior over coefficients and their prior scale. This \u201cgamma-lasso\u201d scheme yields stable and effective estimation for general high-dimensional logistic regression, and we argue that it will be superior to current methods in many settings. Guidelines for prior specification are provided, algorithm convergence is detailed, and estimator properties are outlined from the perspective of the literature on nonconcave likelihood penalization. Related work on sentiment analysis from statistics, econometrics, and machine learning is surveyed and connected. Finally, the methods are applied in two detailed examples and we provide out-of-sample prediction studies to illustrate their effectiveness."}, {"role": "assistant", "content": "2.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond\nAbstract: A fundamental goal of scientific research is to learn about causal relationships. However, despite its critical role in the life and social sciences, causality has not had the same importance in Natural Language Processing (NLP), which has traditionally placed more emphasis on predictive tasks. This distinction is beginning to fade, with an emerging area of interdisciplinary research at the convergence of causal inference and language processing. Still, research on causality in NLP remains scattered across domains without unified definitions, benchmark datasets and clear articulations of the remaining challenges. In this survey, we consolidate research across academic areas and situate it in the broader NLP landscape. We introduce the statistical challenge of estimating causal effects, encompassing settings where text is used as an outcome, treatment, or as a means to address confounding. In addition, we explore potential uses of causal inference to improve the performance, robustness, fairness, and interpretability of NLP models. We thus provide a unified overview of causal inference for the computational linguistics community. 1 \u2217All authors equally contributed to this paper. Author names are organized alphabetically in two clusters: First students and post-docs and then faculty members. The email address of the corresponding (first) author is: feder@campus.technion.ac.il. An online repository containing existing research on causal inference and language processing is available here: https://github.com/causaltext/"}, {"role": "assistant", "content": "4.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Mention Memory: incorporating textual knowledge into Transformers through entity mention attention\nAbstract: Natural language understanding tasks such as open-domain question answering often require retrieving and assimilating factual information from multiple sources. We propose to address this problem by integrating a semi-parametric representation of a large text corpus into a Transformer model as a source of factual knowledge. Speci\ufb01cally, our method represents knowledge with \u201cmention memory\u201d, a table of dense vector representations of every entity mention in a corpus. The proposed model - TOME - is a Transformer that accesses the information through internal memory layers in which each entity mention in the input passage attends to the mention memory. This approach enables synthesis of and reasoning over many disparate sources of information within a single Transformer model. In experiments using a memory of 150 million Wikipedia mentions, TOME achieves strong performance on several open-domain knowledge-intensive tasks, including the claim veri\ufb01cation benchmarks HoVer and FEVER and several entity-based QA benchmarks. We also show that the model learns to attend to informative mentions without any direct supervision. Finally we demonstrate that the model can generalize to new unseen entities by updating the memory without retraining."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Decontextualization: Making Sentences Stand-Alone\nAbstract: Abstract Models for question answering, dialogue agents, and summarization often interpret the meaning of a sentence in a rich context and use that meaning in a new context. Taking excerpts of text can be problematic, as key pieces may not be explicit in a local window. We isolate and define the problem of sentence decontextualization: taking a sentence together with its context and rewriting it to be interpretable out of context, while preserving its meaning. We describe an annotation procedure, collect data on the Wikipedia corpus, and use the data to train models to automatically decontextualize sentences. We present preliminary studies that show the value of sentence decontextualization in a user-facing task, and as preprocessing for systems that perform document understanding. We argue that decontextualization is an important subtask in many downstream applications, and that the definitions and resources provided can benefit tasks that operate on sentences that occur in a richer context."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Luna: Linear Unified Nested Attention\nAbstract: The quadratic computational and memory complexities of the Transformer\u2019s attention mechanism have limited its scalability for modeling long sequences. In this paper, we propose Luna, a linear unified nested attention mechanism that approximates softmax attention with two nested linear attention functions, yielding only linear (as opposed to quadratic) time and space complexity. As compared to a more traditional attention mechanism, Luna introduces an additional sequence with a fixed length as input and an additional corresponding output, which allows Luna to perform attention operation linearly, while also storing adequate contextual information. We perform extensive evaluations on three benchmarks of sequence modeling tasks: long-context sequence modeling, neural machine translation and masked language modeling for large-scale pretraining. Competitive or even better experimental results demonstrate both the effectiveness and efficiency of Luna compared to a variety of strong baseline methods including the full-rank attention and other efficient sparse and dense attention methods. The implementation of our model is available at https://github.com/XuezheMax/fairseq-apollo."}, {"role": "assistant", "content": "3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Analyzing and Improving Representations with the Soft Nearest Neighbor Loss\nAbstract: We explore and expand the $\\textit{Soft Nearest Neighbor Loss}$ to measure the $\\textit{entanglement}$ of class manifolds in representation space: i.e., how close pairs of points from the same class are relative to pairs of points from different classes. We demonstrate several use cases of the loss. As an analytical tool, it provides insights into the evolution of class similarity structures during learning. Surprisingly, we find that $\\textit{maximizing}$ the entanglement of representations of different classes in the hidden layers is beneficial for discrimination in the final layer, possibly because it encourages representations to identify class-independent similarity structures. Maximizing the soft nearest neighbor loss in the hidden layers leads not only to improved generalization but also to better-calibrated estimates of uncertainty on outlier data. Data that is not from the training distribution can be recognized by observing that in the hidden layers, it has fewer than the normal number of neighbors from the predicted class."}, {"role": "assistant", "content": "2.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Rewriting a Deep Generative Model\nAbstract: A deep generative model such as a GAN learns to model a rich set of semantic and physical rules about the target distribution, but up to now, it has been obscure how such rules are encoded in the network, or how a rule could be changed. In this paper, we introduce a new problem setting: manipulation of specific rules encoded by a deep generative model. To address the problem, we propose a formulation in which the desired rule is changed by manipulating a layer of a deep network as a linear associative memory. We derive an algorithm for modifying one entry of the associative memory, and we demonstrate that several interesting structural rules can be located and modified within the layers of state-of-the-art generative models. We present a user interface to enable users to interactively change the rules of a generative model to achieve desired effects, and we show several proof-of-concept applications. Finally, results on multiple datasets demonstrate the advantage of our method against standard fine-tuning methods and edit transfer algorithms."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Understanding high- and low-quality URL Sharing on COVID-19 Twitter streams\nAbstract: This article investigates the prevalence of high and low quality URLs shared on Twitter when users discuss COVID-19. We distinguish between high quality health sources, traditional news sources, and low quality misinformation sources. We find that misinformation, in terms of tweets containing URLs from low quality misinformation websites, is shared at a higher rate than tweets containing URLs on high quality health information websites. However, both are a relatively small proportion of the overall conversation. In contrast, news sources are shared at a much higher rate. These findings lead us to analyze the network created by the URLs referenced on the webpages shared by Twitter users. When looking at the combined network formed by all three of the source types, we find that the high quality health information network, the low quality misinformation network, and the news information network are all well connected with a clear community structure. While high and low quality sites do have connections to each other, the connections to and from news sources are more common, highlighting the central brokerage role news sources play in this information ecosystem. Our findings suggest that while low quality URLs are not extensively shared in the COVID-19 Twitter conversation, a well connected community of low quality COVID-19 related information has emerged on the web, and both health and news sources are connecting to this community."}, {"role": "assistant", "content": "4.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Sample Complexity of Sinkhorn Divergences\nAbstract: Optimal transport (OT) and maximum mean discrepancies (MMD) are now routinely used in machine learning to compare probability measures. We focus in this paper on \\emph{Sinkhorn divergences} (SDs), a regularized variant of OT distances which can interpolate, depending on the regularization strength $\\varepsilon$, between OT ($\\varepsilon=0$) and MMD ($\\varepsilon=\\infty$). Although the tradeoff induced by that regularization is now well understood computationally (OT, SDs and MMD require respectively $O(n^3\\log n)$, $O(n^2)$ and $n^2$ operations given a sample size $n$), much less is known in terms of their \\emph{sample complexity}, namely the gap between these quantities, when evaluated using finite samples \\emph{vs.} their respective densities. Indeed, while the sample complexity of OT and MMD stand at two extremes, $1/n^{1/d}$ for OT in dimension $d$ and $1/\\sqrt{n}$ for MMD, that for SDs has only been studied empirically. In this paper, we \\emph{(i)} derive a bound on the approximation error made with SDs when approximating OT as a function of the regularizer $\\varepsilon$, \\emph{(ii)} prove that the optimizers of regularized OT are bounded in a Sobolev (RKHS) ball independent of the two measures and \\emph{(iii)} provide the first sample complexity bound for SDs, obtained,by reformulating SDs as a maximization problem in a RKHS. We thus obtain a scaling in $1/\\sqrt{n}$ (as in MMD), with a constant that depends however on $\\varepsilon$, making the bridge between OT and MMD complete."}, {"role": "assistant", "content": "2.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Towards Zero-shot Language Modeling\nAbstract: Can we construct a neural language model which is inductively biased towards learning human language? Motivated by this question, we aim at constructing an informative prior for held-out languages on the task of character-level, open-vocabulary language modelling. We obtain this prior as the posterior over network weights conditioned on the data from a sample of training languages, which is approximated through Laplace\u2019s method. Based on a large and diverse sample of languages, the use of our prior outperforms baseline models with an uninformative prior in both zero-shot and few-shot settings, showing that the prior is imbued with universal linguistic knowledge. Moreover, we harness broad language-specific information available for most languages of the world, i.e., features from typological databases, as distant supervision for held-out languages. We explore several language modelling conditioning techniques, including concatenation and meta-networks for parameter generation. They appear beneficial in the few-shot setting, but ineffective in the zero-shot setting. Since the paucity of even plain digital text affects the majority of the world\u2019s languages, we hope that these insights will broaden the scope of applications for language technology."}, {"role": "assistant", "content": "2.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Parameter Inference with Bifurcation Diagrams\nAbstract: Estimation of parameters in differential equation models can be achieved by applying learning algorithms to quantitative time-series data. However, sometimes it is only possible to measure qualitative changes of a system in response to a controlled condition. In dynamical systems theory, such change points are known as bifurcations and lie on a function of the controlled condition called the bifurcation diagram. In this work, we propose a gradient-based approach for inferring the parameters of differential equations that produce a user-specified bifurcation diagram. The cost function contains an error term that is minimal when the model bifurcations match the specified targets and a bifurcation measure which has gradients that push optimisers towards bifurcating parameter regimes. The gradients can be computed without the need to differentiate through the operations of the solver that was used to compute the diagram. We demonstrate parameter inference with minimal models which explore the space of saddle-node and pitchfork diagrams and the genetic toggle switch from synthetic biology. Furthermore, the cost landscape allows us to organise models in terms of topological and geometric equivalence."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Calibration, Entropy Rates, and Memory in Language Models\nAbstract: Building accurate language models that capture meaningful long-term dependencies is a core challenge in natural language processing. Towards this end, we present a calibration-based approach to measure long-term discrepancies between a generative sequence model and the true distribution, and use these discrepancies to improve the model. Empirically, we show that state-of-the-art language models, including LSTMs and Transformers, are \\emph{miscalibrated}: the entropy rates of their generations drift dramatically upward over time. We then provide provable methods to mitigate this phenomenon. Furthermore, we show how this calibration-based approach can also be used to measure the amount of memory that language models use for prediction."}, {"role": "assistant", "content": "2.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: LaMDA: Language Models for Dialog Applications\nAbstract: We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformerbased neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model\u2019s responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency. \u2217Work done while at Google. ar X iv :2 20 1. 08 23 9v 3 [ cs .C L ] 1 0 Fe b 20 22 Figure 1: Impact of model pre-training alone vs. with fine-tuning in LaMDA on dialog quality (left), and safety and factual grounding (right). The quality metric (SSI) corresponds to sensibleness, specificity, and interestingness. See Section 4 for more details on these metrics."}, {"role": "assistant", "content": "3.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: How well do you know your summarization datasets?\nAbstract: State-of-the-art summarization systems are trained and evaluated on massive datasets scraped from the web. Despite their prevalence, we know very little about the underlying characteristics (data noise, summarization complexity, etc.) of these datasets, and how these affect system performance and the reliability of automatic metrics like ROUGE. In this study, we manually analyse 600 samples from three popular summarization datasets. Our study is driven by a six-class typology which captures different noise types (missing facts, entities) and degrees of summarization difficulty (extractive, abstractive). We follow with a thorough analysis of 27 state-of-the-art summarization models and 5 popular metrics, and report our key insights: (1) Datasets have distinct data quality and complexity distributions, which can be traced back to their collection process. (2) The performance of models and reliability of metrics is dependent on sample complexity. (3) Faithful summaries often receive low scores because of the poor diversity of references. We release the code, annotated data and model outputs.1"}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: A Survey on Multimodal Disinformation Detection\nAbstract: Recent years have witnessed the proliferation of fake news, propaganda, misinformation, and disinformation online. While initially this was mostly about textual content, over time images and videos gained popularity, as they are much easier to consume, attract much more attention, and spread further than simple text. As a result, researchers started targeting different modalities and combinations thereof. As different modalities are studied in different research communities, with insufficient interaction, here we offer a survey that explores the state-of-the-art on multimodal disinformation detection covering various combinations of modalities: text, images, audio, video, network structure, and temporal information. Moreover, while some studies focused on factuality, others investigated how harmful the content is. While these two components in the definition of disinformation \u2013 (i) factuality and (ii) harmfulness, are equally important, they are typically studied in isolation. Thus, we argue for the need to tackle disinformation detection by taking into account multiple modalities as well as both factuality and harmfulness, in the same framework. Finally, we discuss current challenges and future research directions."}, {"role": "assistant", "content": "3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: PIGLeT: Language Grounding Through Neuro-Symbolic Interaction in a 3D World\nAbstract: We propose PIGLeT: a model that learns physical commonsense knowledge through interaction, and then uses this knowledge to ground language. We factorize PIGLeT into a physical dynamics model, and a separate language model. Our dynamics model learns not just what objects are but also what they do: glass cups break when thrown, plastic ones don\u2019t. We then use it as the interface to our language model, giving us a unified model of linguistic form and grounded meaning. PIGLeT can read a sentence, simulate neurally what might happen next, and then communicate that result through a literal symbolic representation, or natural language. Experimental results show that our model effectively learns world dynamics, along with how to communicate them. It is able to correctly forecast what happens next given an English sentence over 80% of the time, outperforming a 100x larger, text-to-text approach by over 10%. Likewise, its natural language summaries of physical interactions are also judged by humans as more accurate than LM alternatives. We present comprehensive analysis showing room for future work."}, {"role": "assistant", "content": "2.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Shortformer: Better Language Modeling using Shorter Inputs\nAbstract: Increasing the input length has been a driver of progress in language modeling with transformers. We identify conditions where shorter inputs are not harmful, and achieve perplexity and efficiency improvements through two new methods that decrease input length. First, we show that initially training a model on short subsequences before moving on to longer ones both reduces overall training time and, surprisingly, substantially improves perplexity. Second, we show how to improve the efficiency of recurrence methods in transformers, which let models condition on previously processed tokens when generating sequences that exceed the maximal length the transformer can handle at once. Existing methods require computationally expensive relative position embeddings; we introduce a simple alternative of adding absolute position embeddings to queries and keys instead of to word embeddings, which efficiently produces superior results. We show that these recurrent models also benefit from short input lengths. Combining these techniques speeds up training by a factor of 1.65, reduces memory usage, and substantially improves perplexity on WikiText-103, without adding any parameters."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Learning in High Dimension Always Amounts to Extrapolation\nAbstract: The notion of interpolation and extrapolation is fundamental in various fields from deep learning to function approximation. Interpolation occurs for a sample x whenever this sample falls inside or on the boundary of the given dataset\u2019s convex hull. Extrapolation occurs when x falls outside of that convex hull. One fundamental (mis)conception is that state-of-the-art algorithms work so well because of their ability to correctly interpolate training data. A second (mis)conception is that interpolation happens throughout tasks and datasets, in fact, many intuitions and theories rely on that assumption. We empirically and theoretically argue against those two points and demonstrate that on any high-dimensional (>100) dataset, interpolation almost surely never happens. Those results challenge the validity of our current interpolation/extrapolation definition as an indicator of generalization performances."}, {"role": "assistant", "content": "1.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Customizing Triggers with Concealed Data Poisoning\nAbstract: Adversarial attacks alter NLP model predictions by perturbing test-time inputs. However, it is much less understood whether, and how, predictions can be manipulated with small, concealed changes to the training data. In this work, we develop a new data poisoning attack that allows an adversary to control model predictions whenever a desired trigger phrase is present in the input. For instance, we insert 50 poison examples into a sentiment model's training set that causes the model to frequently predict Positive whenever the input contains \"James Bond\". Crucially, we craft these poison examples using a gradient-based procedure so that they do not mention the trigger phrase. We also apply our poison attack to language modeling (\"Apple iPhone\" triggers negative generations) and machine translation (\"iced coffee\" mistranslated as \"hot coffee\"). We conclude by proposing three defenses that can mitigate our attack at some cost in prediction accuracy or extra human annotation."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Semantic Change and Semantic Stability: Variation is Key\nAbstract: I survey some recent approaches to studying change in the lexicon, particularly change in meaning across phylogenies. I briefly sketch an evolutionary approach to language change and point out some issues in recent approaches to studying semantic change that rely on temporally stratified word embeddings. I draw illustrations from lexical cognate models in Pama-Nyungan to identify meaning classes most appropriate for lexical phylogenetic inference, particularly highlighting the importance of variation in studying change over time."}, {"role": "assistant", "content": "2.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Aspect-based Document Similarity for Research Papers\nAbstract: Traditional document similarity measures provide a coarse-grained distinction between similar and dissimilar documents. Typically, they do not consider in what aspects two documents are similar. This limits the granularity of applications like recommender systems that rely on document similarity. In this paper, we extend similarity with aspect information by performing a pairwise document classification task. We evaluate our aspect-based document similarity approach for research papers. Paper citations indicate the aspect-based similarity, i.e., the title of a section in which a citation occurs acts as a label for the pair of citing and cited paper. We apply a series of Transformer models such as RoBERTa, ELECTRA, XLNet, and BERT variations and compare them to an LSTM baseline. We perform our experiments on two newly constructed datasets of 172,073 research paper pairs from the ACL Anthology and CORD-19 corpus. According to our results, SciBERT is the best performing system with F1-scores of up to 0.83. A qualitative analysis validates our quantitative results and indicates that aspect-based document similarity indeed leads to more fine-grained recommendations."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection\nAbstract: Language models increasingly rely on massive 001 web dumps for diverse text data. However, 002 these sources are rife with undesirable content. 003 As such, resources like Wikipedia, books, and 004 news often serve as anchors for automatically 005 selecting web text most suitable for language 006 modeling, a process typically referred to as 007 quality filtering. Using a new dataset of U.S. 008 high school newspaper articles\u2014written by stu009 dents from across the country\u2014we investigate 010 whose language is preferred by the quality fil011 ter used for GPT-3. We find that newspapers 012 from larger schools, located in wealthier, edu013 cated, and urban ZIP codes are more likely to 014 be classified as high quality. We then demon015 strate that the filter\u2019s measurement of quality 016 is unaligned with other sensible metrics, such 017 as factuality or literary acclaim. We argue that 018 privileging any corpus as high quality entails a 019 language ideology, and more care is needed to 020 construct training corpora for language models, 021 with better transparency and justification for 022 the inclusion or exclusion of various texts. 023"}, {"role": "assistant", "content": "2.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Text Generation by Learning from Demonstrations\nAbstract: Current approaches to text generation largely rely on autoregressive models and maximum likelihood estimation. This paradigm leads to (i) diverse but low-quality samples due to mismatched learning objective and evaluation metric (likelihood vs. quality) and (ii) exposure bias due to mismatched history distributions (gold vs. model-generated). To alleviate these problems, we frame text generation as an offline reinforcement learning (RL) problem with expert demonstrations (i.e., the reference), where the goal is to maximize quality given model-generated histories. We propose GOLD (generation by off-policy learning from demonstrations): an easy-to-optimize algorithm that learns from the demonstrations by importance weighting. Intuitively, GOLD upweights confident tokens and downweights unconfident ones in the reference during training, avoiding optimization issues faced by prior RL approaches that rely on online data collection. According to both automatic and human evaluation, models trained by GOLD outperform those trained by MLE and policy gradient on summarization, question generation, and machine translation. Further, our models are less sensitive to decoding algorithms and alleviate exposure bias."}, {"role": "assistant", "content": "3.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: DART: Open-Domain Structured Data Record to Text Generation\nAbstract: We present DART, an open domain structured DAta Record to Text generation dataset with over 82k instances (DARTs). Data-to-text annotations can be a costly process, especially when dealing with tables which are the major source of structured data and contain nontrivial structures. To this end, we propose a procedure of extracting semantic triples from tables that encodes their structures by exploiting the semantic dependencies among table headers and the table title. Our dataset construction framework effectively merged heterogeneous sources from open domain semantic parsing and spoken dialogue systems by utilizing techniques including tree ontology annotation, question-answer pair to declarative sentence conversion, and predicate unification, all with minimum post-editing. We present systematic evaluation on DART as well as new state-of-the-art results on WebNLG 2017 to show that DART (1) poses new challenges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github.com/Yale-LILY/dart."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Revisiting the Uniform Information Density Hypothesis\nAbstract: The uniform information density (UID) hypothesis posits a preference among language users for utterances structured such that information is distributed uniformly across a signal. While its implications on language production have been well explored, the hypothesis potentially makes predictions about language comprehension and linguistic acceptability as well. Further, it is unclear how uniformity in a linguistic signal\u2014or lack thereof\u2014should be measured, and over which linguistic unit, e.g., the sentence or language level, this uniformity should hold. Here we investigate these facets of the UID hypothesis using reading time and acceptability data. While our reading time results are generally consistent with previous work, they are also consistent with a weakly super-linear effect of surprisal, which would be compatible with UID\u2019s predictions. For acceptability judgments, we find clearer evidence that non-uniformity in information density is predictive of lower acceptability. We then explore multiple operationalizations of UID, motivated by different interpretations of the original hypothesis, and analyze the scope over which the pressure towards uniformity is exerted. The explanatory power of a subset of the proposed operationalizations suggests that the strongest trend may be a regression towards a mean surprisal across the language, rather than the phrase, sentence, or document\u2014a finding that supports a typical interpretation of UID, namely that it is the byproduct of language users maximizing the use of a (hypothetical) communication channel."}, {"role": "assistant", "content": "2.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Thieves on Sesame Street! Model Extraction of BERT-based APIs\nAbstract: We study the problem of model extraction in natural language processing, in which an adversary with only query access to a victim model attempts to reconstruct a local copy of that model. Assuming that both the adversary and victim model fine-tune a large pretrained language model such as BERT (Devlin et al. 2019), we show that the adversary does not need any real training data to successfully mount the attack. In fact, the attacker need not even use grammatical or semantically meaningful queries: we show that random sequences of words coupled with task-specific heuristics form effective queries for model extraction on a diverse set of NLP tasks, including natural language inference and question answering. Our work thus highlights an exploit only made feasible by the shift towards transfer learning methods within the NLP community: for a query budget of a few hundred dollars, an attacker can extract a model that performs only slightly worse than the victim model. Finally, we study two defense strategies against model extraction---membership classification and API watermarking---which while successful against naive adversaries, are ineffective against more sophisticated ones."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Maximum Likelihood Constraint Inference for Inverse Reinforcement Learning\nAbstract: While most approaches to the problem of Inverse Reinforcement Learning (IRL) focus on estimating a reward function that best explains an expert agent's policy or demonstrated behavior on a control task, it is often the case that such behavior is more succinctly represented by a simple reward combined with a set of hard constraints. In this setting, the agent is attempting to maximize cumulative rewards subject to these given constraints on their behavior. We reformulate the problem of IRL on Markov Decision Processes (MDPs) such that, given a nominal model of the environment and a nominal reward function, we seek to estimate state, action, and feature constraints in the environment that motivate an agent's behavior. Our approach is based on the Maximum Entropy IRL framework, which allows us to reason about the likelihood of an expert agent's demonstrations given our knowledge of an MDP. Using our method, we can infer which constraints can be added to the MDP to most increase the likelihood of observing these demonstrations. We present an algorithm which iteratively infers the Maximum Likelihood Constraint to best explain observed behavior, and we evaluate its efficacy using both simulated behavior and recorded data of humans navigating around an obstacle."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Bandwidth Adaptive & Error Resilient MBR Exact Repair Regenerating Codes\nAbstract: Regenerating codes are efficient methods for distributed storage in storage networks, where node failures are common. They guarantee low cost data reconstruction and repair through accessing only a predefined number of arbitrarily chosen storage nodes in the network. In this paper, we consider two simultaneous extensions to the original regenerating codes framework introduced by Dimakis et al.; 1) both data reconstruction and repair are resilient to the presence of a certain number of erroneous nodes in the network and 2) the number of helper nodes in every repair is not fixed, but is a flexible parameter that can be selected during the run-time. We study the fundamental limits of required total repair bandwidth and provide an upper bound for the storage capacity of these codes under these assumptions. We then focus on the minimum repair bandwidth (MBR) case and derive the exact storage capacity by presenting explicit coding schemes with exact repair, which achieve the upper bound of the storage capacity in the considered setup. To this end, we first provide a more natural extension of the well-known product matrix (PM) MBR codes, modified to provide flexibility in choosing the number of helpers in each repair, and simultaneously be robust to erroneous nodes in the network. This is achieved by proving the non-singularity for a family of matrices in large enough finite fields. We next provide another extension of the PM codes, based on a novel repair scheme which enables flexibility in the number of helpers and robustness against erroneous nodes without any extra cost in field size compared with the original PM codes."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Contextualized Weak Supervision for Text Classification\nAbstract: Weakly supervised text classification based on a few user-provided seed words has recently attracted much attention from researchers. Existing methods mainly generate pseudo-labels in a context-free manner (e.g., string matching), therefore, the ambiguous, context-dependent nature of human language has been long overlooked. In this paper, we propose a novel framework ConWea, providing contextualized weak supervision for text classification. Specifically, we leverage contextualized representations of word occurrences and seed word information to automatically differentiate multiple interpretations of the same word, and thus create a contextualized corpus. This contextualized corpus is further utilized to train the classifier and expand seed words in an iterative manner. This process not only adds new contextualized, highly label-indicative keywords but also disambiguates initial seed words, making our weak supervision fully contextualized. Extensive experiments and case studies on real-world datasets demonstrate the necessity and significant advantages of using contextualized weak supervision, especially when the class labels are fine-grained."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: The Disagreement Deconvolution: Bringing Machine Learning Performance Metrics In Line With Reality\nAbstract: Machine learning classifiers for human-facing tasks such as comment toxicity and misinformation often score highly on metrics such as ROC AUC but are received poorly in practice. Why this gap? Today, metrics such as ROC AUC, precision, and recall are used to measure technical performance; however, human-computer interaction observes that evaluation of human-facing systems should account for people\u2019s reactions to the system. In this paper, we introduce a transformation that more closely aligns machine learning classification metrics with the values and methods of user-facing performance measures. The disagreement deconvolution takes in any multi-annotator (e.g., crowdsourced) dataset, disentangles stable opinions from noise by estimating intra-annotator consistency, and compares each test set prediction to the individual stable opinions from each annotator. Applying the disagreement deconvolution to existing social computing datasets, we find that current metrics dramatically overstate the performance of many human-facing machine learning tasks: for example, performance on a comment toxicity task is corrected from .95 to .73 ROC AUC."}, {"role": "assistant", "content": "3.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Technical Debt in the Peer-Review Documentation of R Packages: a rOpenSci Case Study\nAbstract: Context: Technical Debt (TD) is a metaphor used to describe code that is \"not quite right.\" Although TD studies have gained momentum, TD has yet to be studied as thoroughly in non-Object-Oriented (OO) or scientific software such as R. R is a multi-paradigm programming language, whose popularity in data science and statistical applications has amplified in recent years. Due to R\u2019s inherent ability to expand through user-contributed packages, several community-led organizations were created to organize and peer-review packages in a concerted effort to increase their quality. Nonetheless, it is well-known that most R users do not have a technical programming background, being from multiple disciplines. Objective: The goal of this study is to investigate TD in the documentation of the peer-review of R packages led by rOpenSci. Method: We collected over 5,000 comments from 157 packages that had been reviewed and approved to be published at rOpenSci. We manually analyzed a sample dataset of these comments posted by package authors, editors of rOpenSci, and reviewers during the review process to investigate the types of TD present in these reviews. Results: The findings of our study include (i) a taxonomy of TD derived from our analysis of the peer-reviews (ii) documentation debt as being the most prevalent type of debt (iii) different user roles are concerned with different types of TD. For instance, reviewers tend to report some types of TD more than other roles, and the types of TD they report are different from those reported by the authors of a package. Conclusion: TD analysis in scientific software or peer-review is almost non-existent. Our study is a pioneer but within the context of R packages. However, our findings can serve as a starting point for replication studies, given our public datasets, to perform similar analyses in other scientific software or to investigate the rationale behind our findings."}, {"role": "assistant", "content": "2.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: From partners to populations: A hierarchical Bayesian account of coordination and convention\nAbstract: Languages are powerful solutions to coordination problems: They provide stable, shared expectations about how the words we say correspond to the beliefs and intentions in our heads. Yet, language use in a variable and nonstationary social environment requires linguistic representations to be flexible: Old words acquire new ad hoc or partner-specific meanings on the fly. In this article, we introduce continual hierarchical adaptation through inference (CHAI), a hierarchical Bayesian theory of coordination and convention formation that aims to reconcile the long-standing tension between these two basic observations. We argue that the central computational problem of communication is not simply transmission, as in classical formulations, but continual learning and adaptation over multiple timescales. Partner-specific common ground quickly emerges from social inferences within dyadic interactions, while community-wide social conventions are stable priors that have been abstracted away from interactions with multiple partners. We present new empirical data alongside simulations showing how our model provides a computational foundation for several phenomena that have posed a challenge for previous accounts: (a) the convergence to more efficient referring expressions across repeated interaction with the same partner, (b) the gradual transfer of partner-specific common ground to strangers, and (c) the influence of communicative context on which conventions eventually form. (PsycInfo Database Record (c) 2022 APA, all rights reserved)."}, {"role": "assistant", "content": "2.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Predict Responsibly: Improving Fairness and Accuracy by Learning to Defer\nAbstract: In many machine learning applications, there are multiple decision-makers involved, both automated and human. The interaction between these agents often goes unaddressed in algorithmic development. In this work, we explore a simple version of this interaction with a two-stage framework containing an automated model and an external decision-maker. The model can choose to say PASS, and pass the decision downstream, as explored in rejection learning. We extend this concept by proposing \"learning to defer\", which generalizes rejection learning by considering the effect of other agents in the decision-making process. We propose a learning algorithm which accounts for potential biases held by external decision-makers in a system. Experiments demonstrate that learning to defer can make systems not only more accurate but also less biased. Even when working with inconsistent or biased users, we show that deferring models still greatly improve the accuracy and/or fairness of the entire system."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Pointer Graph Networks\nAbstract: Graph neural networks (GNNs) are typically applied to static graphs that are assumed to be known upfront. This static input structure is often informed purely by insight of the machine learning practitioner, and might not be optimal for the actual task the GNN is solving. In absence of reliable domain expertise, one might resort to inferring the latent graph structure, which is often difficult due to the vast search space of possible graphs. Here we introduce Pointer Graph Networks (PGNs) which augment sets or graphs with additional inferred edges for improved model expressivity. PGNs allow each node to dynamically point to another node, followed by message passing over these pointers. The sparsity of this adaptable graph structure makes learning tractable while still being sufficiently expressive to simulate complex algorithms. Critically, the pointing mechanism is directly supervised to model long-term sequences of operations on classical data structures, incorporating useful structural inductive biases from theoretical computer science. Qualitatively, we demonstrate that PGNs can learn parallelisable variants of pointer-based data structures, namely disjoint set unions and link/cut trees. PGNs generalise out-of-distribution to 5x larger test inputs on dynamic graph connectivity tasks, outperforming unrestricted GNNs and Deep Sets."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Adaptive Input Representations for Neural Language Modeling\nAbstract: We introduce adaptive input representations for neural language modeling which extend the adaptive softmax of Grave et al. (2017) to input representations of variable capacity. There are several choices on how to factorize the input and output layers, and whether to model words, characters or sub-word units. We perform a systematic comparison of popular choices for a self-attentional architecture. Our experiments show that models equipped with adaptive embeddings are more than twice as fast to train than the popular character input CNN while having a lower number of parameters. On the WikiText-103 benchmark we achieve 18.7 perplexity, an improvement of 10.5 perplexity compared to the previously best published result and on the Billion Word benchmark, we achieve 23.02 perplexity."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Symbolic Brittleness in Sequence Models: on Systematic Generalization in Symbolic Mathematics\nAbstract: Neural sequence models trained with maximum likelihood estimation have led to breakthroughs in many tasks, where success is defined by the gap between training and test performance. However, their ability to achieve stronger forms of generalization remains unclear. We consider the problem of symbolic mathematical integration, as it requires generalizing systematically beyond the test set. We develop a methodology for evaluating generalization that takes advantage of the problem domain\u2019s structure and access to a verifier. Despite promising in-distribution performance of sequenceto-sequence models in this domain, we demonstrate challenges in achieving robustness, compositionality, and outof-distribution generalization, through both carefully constructed manual test suites and a genetic algorithm that automatically finds large collections of failures in a controllable manner. Our investigation highlights the difficulty of generalizing well with the predominant modeling and learning approach, and the importance of evaluating beyond the test set, across different aspects of generalization."}, {"role": "assistant", "content": "3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Designing Informative Rating Systems for Online Platforms: Evidence from Two Experiments\nAbstract: Platforms critically rely on rating systems to learn the quality of market participants. In practice, however, these ratings are often highly inflated, drastically reducing the signal available to distinguish quality. We consider two questions: First, can rating systems better discriminate quality by altering the meaning and relative importance of the levels in the rating system? And second, if so, how should the platform optimize these choices in the design of the rating system? We first analyze the results of a randomized controlled trial on an online labor market in which an additional question was added to the feedback form. Between treatment conditions, we vary the question phrasing and answer choices. We further run an experiment on Amazon Mechanical Turk with similar structure, to confirm the labor market findings. Our tests reveal that current inflationary norms can in fact be countered by re-anchoring the meaning of the levels of the rating system. In particular, scales that are positive-skewed and provide specific interpretations for what each label means yield rating distributions that are much more informative about quality. Second, we develop a theoretical framework to optimize the design of a rating system by choosing answer labels and their numeric interpretations in a manner that maximizes the rate of convergence to the true underlying quality distribution. Finally, we run simulations with an empirically calibrated model and use these to study the implications for optimal rating system design. Our simulations demonstrate that our modeling and optimization approach can substantially improve the quality of information obtained over baseline designs. Overall, our study illustrates that rating systems that are informative in practice can be designed, and demonstrates how to design them in a principled manner."}, {"role": "assistant", "content": "3.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Disentangling Influence: Using Disentangled Representations to Audit Model Predictions\nAbstract: Motivated by the need to audit complex and black box models, there has been extensive research on quantifying how data features influence model predictions. Feature influence can be direct (a direct influence on model outcomes) and indirect (model outcomes are influenced via proxy features). Feature influence can also be expressed in aggregate over the training or test data or locally with respect to a single point. Current research has typically focused on one of each of these dimensions. In this paper, we develop disentangled influence audits, a procedure to audit the indirect influence of features. Specifically, we show that disentangled representations provide a mechanism to identify proxy features in the dataset, while allowing an explicit computation of feature influence on either individual outcomes or aggregate-level outcomes. We show through both theory and experiments that disentangled influence audits can both detect proxy features and show, for each individual or in aggregate, which of these proxy features affects the classifier being audited the most. In this respect, our method is more powerful than existing methods for ascertaining feature influence."}, {"role": "assistant", "content": "3.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: On Learning Rates and Schr\u00f6dinger Operators\nAbstract: The learning rate is perhaps the single most important parameter in the training of neural networks and, more broadly, in stochastic (nonconvex) optimization. Accordingly, there are numerous effective, but poorly understood, techniques for tuning the learning rate, including learning rate decay, which starts with a large initial learning rate that is gradually decreased. In this paper, we present a general theoretical analysis of the effect of the learning rate in stochastic gradient descent (SGD). Our analysis is based on the use of a learning-rate-dependent stochastic differential equation (lr-dependent SDE) that serves as a surrogate for SGD. For a broad class of objective functions, we establish a linear rate of convergence for this continuous-time formulation of SGD, highlighting the fundamental importance of the learning rate in SGD, and contrasting to gradient descent and stochastic gradient Langevin dynamics. Moreover, we obtain an explicit expression for the optimal linear rate by analyzing the spectrum of the Witten-Laplacian, a special case of the Schrodinger operator associated with the lr-dependent SDE. Strikingly, this expression clearly reveals the dependence of the linear convergence rate on the learning rate -- the linear rate decreases rapidly to zero as the learning rate tends to zero for a broad class of nonconvex functions, whereas it stays constant for strongly convex functions. Based on this sharp distinction between nonconvex and convex problems, we provide a mathematical interpretation of the benefits of using learning rate decay for nonconvex optimization."}, {"role": "assistant", "content": "3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: CSI: Novelty Detection via Contrastive Learning on Distributionally Shifted Instances\nAbstract: Novelty detection, i.e., identifying whether a given sample is drawn from outside the training distribution, is essential for reliable machine learning. To this end, there have been many attempts at learning a representation well-suited for novelty detection and designing a score based on such representation. In this paper, we propose a simple, yet effective method named contrasting shifted instances (CSI), inspired by the recent success on contrastive learning of visual representations. Specifically, in addition to contrasting a given sample with other instances as in conventional contrastive learning methods, our training scheme contrasts the sample with distributionally-shifted augmentations of itself. Based on this, we propose a new detection score that is specific to the proposed training scheme. Our experiments demonstrate the superiority of our method under various novelty detection scenarios, including unlabeled one-class, unlabeled multi-class and labeled multi-class settings, with various image benchmark datasets."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Advancing RNN Transducer Technology for Speech Recognition\nAbstract: We investigate a set of techniques for RNN Transducers (RNN-Ts) that were instrumental in lowering the word error rate on three different tasks (Switchboard 300 hours, conversational Spanish 780 hours and conversational Italian 900 hours). The techniques pertain to architectural changes, speaker adaptation, language model fusion, model combination and general training recipe. First, we introduce a novel multiplicative integration of the encoder and prediction network vectors in the joint network (as opposed to additive). Second, we discuss the applicability of i-vector speaker adaptation to RNN-Ts in conjunction with data perturbation. Third, we explore the effectiveness of the recently proposed density ratio language model fusion for these tasks. Last but not least, we describe the other components of our training recipe and their effect on recognition performance. We report a 5.9% and 12.5% word error rate on the Switchboard and CallHome test sets of the NIST Hub5 2000 evaluation and a 12.7% WER on the Mozilla CommonVoice Italian test set."}, {"role": "assistant", "content": "3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Environment-agnostic Multitask Learning for Natural Language Grounded Navigation\nAbstract: Recent research efforts enable study for natural language grounded navigation in photo-realistic environments, e.g., following natural language instructions or dialog. However, existing methods tend to overfit training data in seen environments and fail to generalize well in previously unseen environments. In order to close the gap between seen and unseen environments, we aim at learning a generalized navigation model from two novel perspectives: (1) we introduce a multitask navigation model that can be seamlessly trained on both Vision-Language Navigation (VLN) and Navigation from Dialog History (NDH) tasks, which benefits from richer natural language guidance and effectively transfers knowledge across tasks; (2) we propose to learn environment-agnostic representations for the navigation policy that are invariant among the environments seen during training, thus generalizing better on unseen environments. Extensive experiments show that our navigation model trained using environment-agnostic multitask learning significantly reduces the performance gap between seen and unseen environments and outperforms the baselines on unseen environments by 16% (relative measure on success rate) on VLN and 120% (goal progress) on NDH, establishing a new state-of-the-art for the NDH task. The code for training the navigation model using environment-agnostic multitask learning is available at this https URL."}, {"role": "assistant", "content": "4.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models\nAbstract: We provide the first exploration of text-to-text transformers (T5) sentence embeddings. Sentence embeddings are broadly useful for language processing tasks. While T5 achieves impressive performance on language tasks cast as sequence-to-sequence mapping problems, it is unclear how to produce sentence embeddings from encoder-decoder models. We investigate three methods for extracting T5 sentence embeddings: two utilize only the T5 encoder and one uses the full T5 encoder-decoder model. Our encoder-only models outperforms BERTbased sentence embeddings on both transfer tasks and semantic textual similarity (STS). Our encoder-decoder method achieves further improvement on STS. Scaling up T5 from millions to billions of parameters is found to produce consistent improvements on downstream tasks. Finally, we introduce a two-stage contrastive learning approach that achieves a new state-of-art on STS using sentence embeddings, outperforming both Sentence BERT (Reimers and Gurevych, 2019) and SimCSE (Gao et al., 2021)."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Pretrained Language Models for Text Generation: A Survey\nAbstract: Text generation has become one of the most important yet challenging tasks in natural language processing (NLP). The resurgence of deep learning has greatly advanced this field by neural generation models, especially the paradigm of pretrained language models (PLMs). In this paper, we present an overview of the major advances achieved in the topic of PLMs for text generation. As the preliminaries, we present the general task definition and briefly describe the mainstream architectures of PLMs for text generation. As the core content, we discuss how to adapt existing PLMs to model different input data and satisfy special properties in the generated text. We further summarize several important fine-tuning strategies for text generation. Finally, we present several future directions and conclude this paper. Our survey aims to provide text generation researchers a synthesis and pointer to related research."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: The Power of Scale for Parameter-Efficient Prompt Tuning\nAbstract: In this work, we explore \u201cprompt tuning,\u201d a simple yet effective mechanism for learning \u201csoft prompts\u201d to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signals from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3\u2019s few-shot learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method \u201ccloses the gap\u201d and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant because large models are costly to share and serve and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed \u201cprefix tuning\u201d of Li and Liang (2021) and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer and enables efficient \u201cprompt ensembling.\u201d We release code and model checkpoints to reproduce our experiments."}, {"role": "assistant", "content": "3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Summarising Historical Text in Modern Languages\nAbstract: We introduce the task of historical text summarisation, where documents in historical forms of a language are summarised in the corresponding modern language. This is a fundamentally important routine to historians and digital humanities researchers but has never been automated. We compile a high-quality gold-standard text summarisation dataset, which consists of historical German and Chinese news from hundreds of years ago summarised in modern German or Chinese. Based on cross-lingual transfer learning techniques, we propose a summarisation model that can be trained even with no cross-lingual (historical to modern) parallel data, and further benchmark it against state-of-the-art algorithms. We report automatic and human evaluations that distinguish the historic to modern language summarisation task from standard cross-lingual summarisation (i.e., modern to modern language), highlight the distinctness and value of our dataset, and demonstrate that our transfer learning approach outperforms standard cross-lingual benchmarks on this task."}, {"role": "assistant", "content": "3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Transferable Dialogue Systems and User Simulators\nAbstract: One of the difficulties in training dialogue systems is the lack of training data. We explore the possibility of creating dialogue data through the interaction between a dialogue system and a user simulator. Our goal is to develop a modelling framework that can incorporate new dialogue scenarios through self-play between the two agents. In this framework, we first pre-train the two agents on a collection of source domain dialogues, which equips the agents to converse with each other via natural language. With further fine-tuning on a small amount of target domain data, the agents continue to interact with the aim of improving their behaviors using reinforcement learning with structured reward functions. In experiments on the MultiWOZ dataset, two practical transfer learning problems are investigated: 1) domain adaptation and 2) single-to-multiple domain transfer. We demonstrate that the proposed framework is highly effective in bootstrapping the performance of the two agents in transfer learning. We also show that our method leads to improvements in dialogue system performance on complete datasets."}, {"role": "assistant", "content": "3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Searchable Hidden Intermediates for End-to-End Models of Decomposable Sequence Tasks\nAbstract: End-to-end approaches for sequence tasks are becoming increasingly popular. Yet for complex sequence tasks, like speech translation, systems that cascade several models trained on sub-tasks have shown to be superior, suggesting that the compositionality of cascaded systems simplifies learning and enables sophisticated search capabilities. In this work, we present an end-to-end framework that exploits compositionality to learn searchable hidden representations at intermediate stages of a sequence model using decomposed sub-tasks. These hidden intermediates can be improved using beam search to enhance the overall performance and can also incorporate external models at intermediate stages of the network to re-score or adapt towards out-of-domain data. One instance of the proposed framework is a Multi-Decoder model for speech translation that extracts the searchable hidden intermediates from a speech recognition sub-task. The model demonstrates the aforementioned benefits and outperforms the previous state-of-the-art by around +6 and +3 BLEU on the two test sets of Fisher-CallHome and by around +3 and +4 BLEU on the English-German and English-French test sets of MuST-C."}, {"role": "assistant", "content": "2.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: MuSiQue: Multi-hop Questions via Single-hop Question Composition\nAbstract: To build challenging multi-hop question answering datasets, we propose a bottom-up semi-automatic process of constructing multihop question via composition of single-hop questions. Constructing multi-hop questions as composition of single-hop questions allows us to exercise greater control over the quality of the resulting multi-hop questions. This process allows building a dataset with (i) connected reasoning where each step needs the answer from a previous step; (ii) minimal traintest leakage by eliminating even partial overlap of reasoning steps; (iii) variable number of hops and composition structures; and (iv) contrasting unanswerable questions by modifying the context. We use this process to construct a new multihop QA dataset: MuSiQue-Ans with 25K 2-4 hop questions using seed questions from 5 existing single-hop datasets. Our experiments demonstrate that MuSiQue-Ans is challenging for state-of-the-art QA models (e.g., human-machine gap of 30 F1 pts), significantly harder than existing datasets (2x humanmachine gap), and substantially less cheatable (e.g., a single-hop model is worse by 30 F1 pts). We also build an even more challenging dataset, MuSiQue-Full, consisting of answerable and unanswerable contrast question pairs, where model performance drops further by 13+ F1 pts.1"}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Eliciting and Enforcing Subjective Individual Fairness\nAbstract: We revisit the notion of individual fairness first proposed by Dwork et al. [2012], which asks that \"similar individuals should be treated similarly\". A primary difficulty with this definition is that it assumes a completely specified fairness metric for the task at hand. In contrast, we consider a framework for fairness elicitation, in which fairness is indirectly specified only via a sample of pairs of individuals who should be treated (approximately) equally on the task. We make no assumption that these pairs are consistent with any metric. We provide a provably convergent oracle-efficient algorithm for minimizing error subject to the fairness constraints, and prove generalization theorems for both accuracy and fairness. Since the constrained pairs could be elicited either from a panel of judges, or from particular individuals, our framework provides a means for algorithmically enforcing subjective notions of fairness. We report on preliminary findings of a behavioral study of subjective fairness using human-subject fairness constraints elicited on the COMPAS criminal recidivism dataset."}, {"role": "assistant", "content": "2.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Surface Form Competition: Why the Highest Probability Answer Isn\u2019t Always Right\nAbstract: Large language models have shown promising results in zero-shot settings. For example, they can perform multiple choice tasks simply by conditioning on a question and selecting the answer with the highest probability. However, ranking by string probability can be problematic due to surface form competition\u2014wherein different surface forms compete for probability mass, even if they represent the same underlying concept in a given context, e.g. \u201ccomputer\u201d and \u201cPC.\u201d Since probability mass is finite, this lowers the probability of the correct answer, due to competition from other strings that are valid answers (but not one of the multiple choice options). We introduce Domain Conditional Pointwise Mutual Information, an alternative scoring function that directly compensates for surface form competition by simply reweighing each option according to its a priori likelihood within the context of a specific task. It achieves consistent gains in zero-shot performance over both calibrated and uncalibrated scoring functions on all GPT-2 and GPT-3 models on a variety of multiple choice datasets."}, {"role": "assistant", "content": "3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Sharp asymptotic and finite-sample rates of convergence of empirical measures in Wasserstein distance\nAbstract: The Wasserstein distance between two probability measures on a metric space is a measure of closeness with applications in statistics, probability, and machine learning. In this work, we consider the fundamental question of how quickly the empirical measure obtained from $n$ independent samples from $\\mu$ approaches $\\mu$ in the Wasserstein distance of any order. We prove sharp asymptotic and finite-sample results for this rate of convergence for general measures on general compact metric spaces. Our finite-sample results show the existence of multi-scale behavior, where measures can exhibit radically different rates of convergence as $n$ grows."}, {"role": "assistant", "content": "2.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Fast Convergence of Regularized Learning in Games\nAbstract: We show that natural classes of regularized learning algorithms with a form of recency bias achieve faster convergence rates to approximate efficiency and to coarse correlated equilibria in multiplayer normal form games. When each player in a game uses an algorithm from our class, their individual regret decays at $O(T^{-3/4})$, while the sum of utilities converges to an approximate optimum at $O(T^{-1})$--an improvement upon the worst case $O(T^{-1/2})$ rates. We show a black-box reduction for any algorithm in the class to achieve $\\tilde{O}(T^{-1/2})$ rates against an adversary, while maintaining the faster rates against algorithms in the class. Our results extend those of [Rakhlin and Shridharan 2013] and [Daskalakis et al. 2014], who only analyzed two-player zero-sum games for specific algorithms."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Taming the Monster: A Fast and Simple Algorithm for Contextual Bandits\nAbstract: We present a new algorithm for the contextual bandit learning problem, where the learner repeatedly takes one of K actions in response to the observed context, and observes the reward only for that action. Our method assumes access to an oracle for solving fully supervised cost-sensitive classification problems and achieves the statistically optimal regret guarantee with only O(\u221aKT) oracle calls across all T rounds. By doing so, we obtain the most practical contextual bandit learning algorithm amongst approaches that work for general policy classes. We conduct a proof-of-concept experiment which demonstrates the excellent computational and statistical performance of (an online variant of) our algorithm relative to several strong baselines."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions\nAbstract: Although convolutional neural networks (CNNs) have achieved great success in computer vision, this work investigates a simpler, convolution-free backbone network use-fid for many dense prediction tasks. Unlike the recently-proposed Vision Transformer (ViT) that was designed for image classification specifically, we introduce the Pyramid Vision Transformer (PVT), which overcomes the difficulties of porting Transformer to various dense prediction tasks. PVT has several merits compared to current state of the arts. (1) Different from ViT that typically yields low-resolution outputs and incurs high computational and memory costs, PVT not only can be trained on dense partitions of an image to achieve high output resolution, which is important for dense prediction, but also uses a progressive shrinking pyramid to reduce the computations of large feature maps. (2) PVT inherits the advantages of both CNN and Transformer, making it a unified backbone for various vision tasks without convolutions, where it can be used as a direct replacement for CNN backbones. (3) We validate PVT through extensive experiments, showing that it boosts the performance of many downstream tasks, including object detection, instance and semantic segmentation. For example, with a comparable number of parameters, PVT+RetinaNet achieves 40.4 AP on the COCO dataset, surpassing ResNet50+RetinNet (36.3 AP) by 4.1 absolute AP (see Figure 2). We hope that PVT could, serre as an alternative and useful backbone for pixel-level predictions and facilitate future research."}, {"role": "assistant", "content": "4.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Benchmarking the Combinatorial Generalizability of Complex Query Answering on Knowledge Graphs\nAbstract: Complex Query Answering (CQA) is an important reasoning task on knowledge graphs. Current CQA learning models have been shown to be able to generalize from atomic operators to more complex formulas, which can be regarded as the combinatorial generalizability. In this paper, we present EFO-1-QA, a new dataset to benchmark the combinatorial generalizability of CQA models by including 301 different queries types, which is 20 times larger than existing datasets. Besides, our benchmark, for the first time, provide a benchmark to evaluate and analyze the impact of different operators and normal forms by using (a) 7 choices of the operator systems and (b) 9 forms of complex queries. Specifically, we provide the detailed study of the combinatorial generalizability of two commonly used operators, i.e., projection and intersection, and justify the impact of the forms of queries given the canonical choice of operators. Our code and data can provide an effective pipeline to benchmark CQA models. 2"}, {"role": "assistant", "content": "3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Adversarial Crowdsourcing Through Robust Rank-One Matrix Completion\nAbstract: We consider the problem of reconstructing a rank-one matrix from a revealed subset of its entries when some of the revealed entries are corrupted with perturbations that are unknown and can be arbitrarily large. It is not known which revealed entries are corrupted. We propose a new algorithm combining alternating minimization with extreme-value filtering and provide sufficient and necessary conditions to recover the original rank-one matrix. In particular, we show that our proposed algorithm is optimal when the set of revealed entries is given by an Erd\u0151s-Renyi random graph. These results are then applied to the problem of classification from crowdsourced data under the assumption that while the majority of the workers are governed by the standard single-coin David-Skene model (i.e., they output the correct answer with a certain probability), some of the workers can deviate arbitrarily from this model. In particular, the \"adversarial\" workers could even make decisions designed to make the algorithm output an incorrect answer. Extensive experimental results show our algorithm for this problem, based on rank-one matrix completion with perturbations, outperforms all other state-of-the-art methods in such an adversarial scenario."}, {"role": "assistant", "content": "3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Deep Neural Networks for Estimation and Inference: Application to Causal Effects and Other Semiparametric Estimands\nAbstract: We study deep neural networks and their use in semiparametric inference. We prove valid inference after first-step estimation with deep learning, a result new to the literature. We provide new rates of convergence for deep feedforward neural nets and, because our rates are sufficiently fast (in some cases minimax optimal), obtain valid semiparametric inference. Our estimation rates and semiparametric inference results handle the current standard architecture: fully connected feedforward neural networks (multi-layer perceptrons), with the now-common rectified linear unit activation function and a depth explicitly diverging with the sample size. We discuss other architectures as well, including fixed-width, very deep networks. We establish nonasymptotic bounds for these deep nets for nonparametric regression, covering the standard least squares and logistic losses in particular. We then apply our theory to develop semiparametric inference, focusing on treatment effects, expected welfare, and decomposition effects for concreteness. Inference in many other semiparametric contexts can be readily obtained. We demonstrate the effectiveness of deep learning with a Monte Carlo analysis and an empirical application to direct mail marketing."}, {"role": "assistant", "content": "1.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Partial Counterfactual Identification from Observational and Experimental Data\nAbstract: This paper investigates the problem of bounding counterfactual queries from an arbitrary collection of observational and experimental distributions and qualitative knowledge about the underlying data-generating model represented in the form of a causal diagram. We show that all counterfactual distributions in an arbitrary structural causal model (SCM) could be generated by a canonical family of SCMs with the same causal diagram where unobserved (exogenous) variables are discrete with a finite domain. Utilizing the canonical SCMs, we translate the problem of bounding counterfactuals into that of polynomial programming whose solution provides optimal bounds for the counterfactual query. Solving such polynomial programs is in general computationally expensive. We therefore develop effective Monte Carlo algorithms to approximate the optimal bounds from an arbitrary combination of observational and experimental data. Our algorithms are validated extensively on synthetic and real-world datasets. Introduction This paper studies the problem of inferring counterfactual queries from a combination of observations, experiments, and qualitative assumptions about the phenomenon under investigation. The assumptions are represented in the form of a causal diagram (Pearl 1995), which is a directed acyclic graph where arrows indicate the potential existence of functional relationships among corresponding variables; some variables are unobserved. This problem arises in diverse fields such as artificial intelligence, statistics, cognitive science, economics, and the health and social sciences. For example, when investigating the gender discrimination in college admission, one may ask \u201cwhat would the admission outcome be for a female applicant had she been a male?\u201d Such a counterfactual query contains conflicting information: in the real world the applicant is female, in the hypothetical world she was not. Therefore, it is not immediately clear how to design effective experimental procedures for evaluating counterfactuals, or how to compute them from observational data. The problem of identifying counterfactual distributions from the combination of data and a causal diagram has been studied in the causal inference literature. First, there exists a complete proof system for reasoning about counterfactual queries (Halpern 1998). While such a system, in prinPreprint. Under review. ciple, is sufficient in evaluating any identifiable counterfactual expression, it lacks a proof guideline that determines the feasibility of such evaluation efficiently. There are algorithms to determine whether a counterfactual distribution is inferrable from all possible controlled experiments (Shpitser and Pearl 2007), or a special type of counterfactual distributions, called path-specific effects, from observational (Shpitser and Sherman 2018) and experimental data (Avin, Shpitser, and Pearl 2005). Finally, there exist an algorithm that decides whether any nested counterfactual is identifiable an arbitrary combination of observational and experimental distributions (Correa, Lee, and Bareinboim 2021). In practice, however, the combination of quantitative knowledge and observed data does not always permit one to uniquely determine the target counterfactual query. In such cases, the counterfactual query is said to be non-identifiable. Partial identification methods concern with deriving informative bounds over the target counterfactual probability in non-identifiable settings. Several algorithms have been developed to bound counterfactual probabilities from the combination of observational and experimental data (Manski 1990; Robins 1989; Balke and Pearl 1994, 1997; Evans 2012; Richardson et al. 2014; Kallus and Zhou 2018, 2020; Finkelstein and Shpitser 2020; Kilbertus, Kusner, and Silva 2020; Zhang and Bareinboim 2021). In this work, we build on the approach introduced by Balke & Pearl in (Balke and Pearl 1994), which involves direct discretization of unobserved domains, also referred to as the canonical partitioning or the principal stratification (Frangakis and Rubin 2002; Pearl 2011). Consider the causal diagram in Fig. 1a, where X,Y, Z are binary variables in {0, 1}; U is an unobserved variable taking values in an arbitrary continuous domain. (Balke and Pearl 1994) showed that domains of U could be discretized into 16 equivalent classes without changing the original counterfactual distributions and the graphical structure in Fig. 1a. For instance, suppose that values of U are drawn from an arbitrary distribution P \u2217(u) over a continuous domain. It has been shown that the observational distribution P (x, y, z) could be reproduced by a generative model of the form P (x, y, z) = \u2211 u P (x|u, z)P (y|x, u)P (u)P (z), where P (u) is a discrete distribution over a finite domain {1, . . . , 16}. Using the finite-state representation of unobserved variables, (Balke and Pearl 1997) derived tight bounds on treatar X iv :2 11 0. 05 69 0v 1 [ cs .A I] 1 2 O ct 2 02 1 ment effects under a set of constraints called instrumental variables (e.g., Fig. 1a). (Chickering and Pearl 1997; Imbens and Rubin 1997) applied the parsimony of finite-state representation in a Bayesian framework, to obtain credible intervals for the posterior distribution of causal effects in noncompliance settings. Despite the optimality guarantees in their treatments, these bounds were only derived for specific settings. A systematic strategy for partial identification in an arbitrary causal diagram is still missing. There are significant challenges in bounding any counterfactual query in an arbitrary causal diagram given an arbitrary collection of observational and experimental data. Our goal in this paper is to overcome these challenges. We show that when inferring about counterfactual distributions (over finite observed variables) in an arbitrary causal diagram, one could restrict domains of unobserved variables to a finite space without loss of generality. This result allows us to develop novel partial identification algorithms to bound unknown counterfactual probabilities from an arbitrary combination of observational and experimental data. In some way, this paper can be seen as closing a long-standing open problem introduced by (Balke and Pearl 1994), where they solve a special bounding instance in the case of instrumental variables. More specifically, our contributions are as follows. (1) We introduce a special family of discrete structural causal models, and show that it could represent all categorical counterfactual distributions (with finite support) in an arbitrary causal diagram. (2) Using this result, we translate the partial identification task into an equivalent polynomial program. Solving such a program leads to bounds over target counterfactual probabilities that are provably optimal. (3) We develop an effective Monte Carlo algorithm to approximate optimal bounds from a finite number of observational and experimental data. Finally, our algorithms are validated extensively on synthetic and real-world datasets. Preliminaries We introduce in this section some basic notations and definitions that will be used throughout the paper. We use capital letters to denote variables (X), small letters for their values (x) and \u03a9X for their domains. For an arbitrary setX , let |X| be its cardinality. The probability distribution over variables X is denoted by P (X). For convenience, we consistently use P (x) as a shorthand for the probability P (X = x). Finally, the indicator function 1X=x returns 1 if an event X = x holds; otherwise, 1X=x is equal to 0. The basic semantical framework of our analysis rests on structural causal models (SCMs) (Pearl 2000, Ch. 7). An SCM M is a tuple \u3008V ,U ,F , P \u3009 where V is a set of endogenous variables and U is a set of exogenous variables. F is a set of functions where each fV \u2208 F decides values of an endogenous variable V \u2208 V taking as argument a combination of other variables in the system. That is, v \u2190 fV (paV , uV ),PAV \u2286 V , UV \u2286 U . Exogenous variables U \u2208 U are mutually independent, values of which are drawn from the exogenous distribution P (U). Naturally, M induces a joint distribution P (V ) over endogenous variables V , called the observational distribution. Each SCM M is also associated with a causal diagram Z X Y U1 U2"}, {"role": "assistant", "content": "3.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\nAbstract: While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train."}, {"role": "assistant", "content": "2.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Episodic Transformer for Vision-and-Language Navigation\nAbstract: Interaction and navigation defined by natural language instructions in dynamic environments pose significant challenges for neural agents. This paper focuses on addressing two challenges: handling long sequence of subtasks, and understanding complex human instructions. We propose Episodic Transformer (E.T.), a multimodal transformer that encodes language inputs and the full episode history of visual observations and actions. To improve training, we leverage synthetic instructions as an intermediate representation that decouples understanding the visual appearance of an environment from the variations of natural language instructions. We demonstrate that encoding the history with a transformer is critical to solve compositional tasks, and that pretraining and joint training with synthetic instructions further improve the performance. Our approach sets a new state of the art on the challenging ALFRED benchmark, achieving 38.4% and 8.5% task success rates on seen and unseen test splits."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Formalizing Trust in Artificial Intelligence: Prerequisites, Causes and Goals of Human Trust in AI\nAbstract: Trust is a central component of the interaction between people and AI, in that 'incorrect' levels of trust may cause misuse, abuse or disuse of the technology. But what, precisely, is the nature of trust in AI? What are the prerequisites and goals of the cognitive mechanism of trust, and how can we promote them, or assess whether they are being satisfied in a given interaction? This work aims to answer these questions. We discuss a model of trust inspired by, but not identical to, interpersonal trust (i.e., trust between people) as defined by sociologists. This model rests on two key properties: the vulnerability of the user; and the ability to anticipate the impact of the AI model's decisions. We incorporate a formalization of 'contractual trust', such that trust between a user and an AI model is trust that some implicit or explicit contract will hold, and a formalization of 'trustworthiness' (that detaches from the notion of trustworthiness in sociology), and with it concepts of 'warranted' and 'unwarranted' trust. We present the possible causes of warranted trust as intrinsic reasoning and extrinsic behavior, and discuss how to design trustworthy AI, how to evaluate whether trust has manifested, and whether it is warranted. Finally, we elucidate the connection between trust and XAI using our formalization."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Skill Induction and Planning with Latent Language\nAbstract: We present a framework for learning hierarchical policies from demonstrations, using sparse natural language annotations to guide the discovery of reusable skills for autonomous decision-making. We formulate a generative model of action sequences in which goals generate sequences of high-level subtask descriptions, and these descriptions generate sequences of low-level actions. We describe how to train this model using primarily unannotated demonstrations by parsing demonstrations into sequences of named high-level subtasks, using only a small number of seed annotations to ground language in action. In trained models, the space of natural language commands indexes a combinatorial library of skills; agents can use these skills to plan by generating high-level instruction sequences tailored to novel goals. We evaluate this approach in the ALFRED household simulation environment, providing natural language annotations for only 10% of demonstrations. It completes more than twice as many tasks as a standard approach to learning from demonstrations, matching the performance of instruction following models with access to ground-truth plans during both training and evaluation. 1"}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Few-shot Image Generation via Cross-domain Correspondence\nAbstract: Training generative models, such as GANs, on a target domain containing limited examples (e.g., 10) can easily result in overfitting. In this work, we seek to utilize a large source domain for pretraining and transfer the diversity information from source to target. We propose to preserve the relative similarities and differences between instances in the source via a novel cross-domain distance consistency loss. To further reduce overfitting, we present an anchor-based strategy to encourage different levels of realism over different regions in the latent space. With extensive results in both photorealistic and non-photorealistic domains, we demonstrate qualitatively and quantitatively that our few-shot model automatically discovers correspondences between source and target domains and generates more diverse and realistic images than previous methods."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: 1-Bit Matrix Completion\nAbstract: In this paper we develop a theory of matrix completion for the extreme case of noisy 1-bit observations. Instead of observing a subset of the real-valued entries of a matrix M, we obtain a small number of binary (1-bit) measurements generated according to a probability distribution determined by the real-valued entries of M. The central question we ask is whether or not it is possible to obtain an accurate estimate of M from this data. In general this would seem impossible, but we show that the maximum likelihood estimate under a suitable constraint returns an accurate estimate of M when ||M||_{\\infty} <= \\alpha, and rank(M) <= r. If the log-likelihood is a concave function (e.g., the logistic or probit observation models), then we can obtain this maximum likelihood estimate by optimizing a convex program. In addition, we also show that if instead of recovering M we simply wish to obtain an estimate of the distribution generating the 1-bit measurements, then we can eliminate the requirement that ||M||_{\\infty} <= \\alpha. For both cases, we provide lower bounds showing that these estimates are near-optimal. We conclude with a suite of experiments that both verify the implications of our theorems as well as illustrate some of the practical applications of 1-bit matrix completion. In particular, we compare our program to standard matrix completion methods on movie rating data in which users submit ratings from 1 to 5. In order to use our program, we quantize this data to a single bit, but we allow the standard matrix completion program to have access to the original ratings (from 1 to 5). Surprisingly, the approach based on binary data performs significantly better."}, {"role": "assistant", "content": "3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Indigenous language technologies in Canada: Assessment, challenges, and successes\nAbstract: In this article, we discuss which text, speech, and image technologies have been developed, and would be feasible to develop, for the approximately 60 Indigenous languages spoken in Canada. In particular, we concentrate on technologies that may be feasible to develop for most or all of these languages, not just those that may be feasible for the few most-resourced of these. We assess past achievements and consider future horizons for Indigenous language transliteration, text prediction, spell-checking, approximate search, machine translation, speech recognition, speaker diarization, speech synthesis, optical character recognition, and computer-aided language learning."}, {"role": "assistant", "content": "3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: End-to-end reconstruction meets data-driven regularization for inverse problems\nAbstract: We propose a new approach for learning end-to-end reconstruction operators based on unpaired training data for ill-posed inverse problems. The proposed method combines the classical variational framework with iterative unrolling and essentially seeks to minimize a weighted combination of the expected distortion in the measurement space and the Wasserstein-1 distance between the distributions of the reconstruction and the ground-truth. More specifically, the regularizer in the variational setting is parametrized by a deep neural network and learned simultaneously with the unrolled reconstruction operator. The variational problem is then initialized with the output of the reconstruction network and solved iteratively till convergence. Notably, it takes significantly fewer iterations to converge as compared to variational methods, thanks to the excellent initialization obtained via the unrolled operator. The resulting approach combines the computational efficiency of end-to-end unrolled reconstruction with the well-posedness and noise-stability guarantees of the variational setting. Moreover, we demonstrate with the example of image reconstruction in X-ray computed tomography (CT) that our approach outperforms state-of-the-art unsupervised methods and that it outperforms or is at least on par with state-of-the-art supervised data-driven reconstruction approaches."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: LXMERT: Learning Cross-Modality Encoder Representations from Transformers\nAbstract: Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly, the alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we build a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language encoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language semantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative pre-training tasks: masked language modeling, masked object prediction (feature regression and label classification), cross-modality matching, and image question answering. These tasks help in learning both intra-modality and cross-modality relationships. After fine-tuning from our pre-trained parameters, our model achieves the state-of-the-art results on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our pre-trained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR2, and improve the previous best result by 22% absolute (54% to 76%). Lastly, we demonstrate detailed ablation studies to prove that both our novel model components and pre-training strategies significantly contribute to our strong results. Code and pre-trained models publicly available at: https://github.com/airsplay/lxmert"}, {"role": "assistant", "content": "3.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Time Waits for No One! Analysis and Challenges of Temporal Misalignment\nAbstract: When an NLP model is trained on text data from one time period and tested or deployed on data from another, the resulting temporal misalignment can degrade end-task performance. In this work, we establish a suite of eight diverse tasks across different domains (social media, science papers, news, and reviews) and periods of time (spanning five years or more) to quantify the effects of temporal misalignment. Our study is focused on the ubiquitous setting where a pretrained model is optionally adapted through continued domainspecific pretraining, followed by task-specific finetuning. We establish a suite of tasks across multiple domains to study temporal misalignment in modern NLP systems. We find stronger effects of temporal misalignment on task performance than have been previously reported. We also find that, while temporal adaptation through continued pretraining can help, these gains are small compared to task-specific finetuning on data from the target time period. Our findings motivate continued research to improve temporal robustness of NLP models.1"}, {"role": "assistant", "content": "3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: The Trade-offs of Domain Adaptation for Neural Language Models\nAbstract: This work connects language model adaptation with concepts of machine learning theory. We consider a training setup with a large outof-domain set and a small in-domain set. We derive how the benefit of training a model on either set depends on the size of the sets and the distance between their underlying distributions. We analyze how out-of-domain pretraining before in-domain fine-tuning achieves better generalization than either solution independently. Finally, we present how adaptation techniques based on data selection, such as importance sampling, intelligent data selection and influence functions, can be presented in a common framework which highlights their similarity and also their subtle differences."}, {"role": "assistant", "content": "3.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Hierarchical Multi-head Attentive Network for Evidence-aware Fake News Detection\nAbstract: The widespread of fake news and misinformation in various domains ranging from politics, economics to public health has posed an urgent need to automatically fact-check information. A recent trend in fake news detection is to utilize evidence from external sources. However, existing evidence-aware fake news detection methods focused on either only word-level attention or evidence-level attention, which may result in suboptimal performance. In this paper, we propose a Hierarchical Multi-head Attentive Network to fact-check textual claims. Our model jointly combines multi-head word-level attention and multi-head document-level attention, which aid explanation in both word-level and evidence-level. Experiments on two real-word datasets show that our model outperforms seven state-of-the-art baselines. Improvements over baselines are from 6% to 18%. Our source code and datasets are released at https://github.com/nguyenvo09/EACL2021."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Carbon Emissions and Large Neural Network Training\nAbstract: David Patterson 1 , 2 , Joseph Gonzalez 2 , Quoc Le 1 , Chen Liang 1 , Lluis-Miquel Munguia 1 , Daniel Rothchild 2 , David So 1 , Maud Texier 1 , and Jeff Dean 1  {davidpatterson, qvl, crazydonkey, llmunguia, davidso, maudt, jeff}@google.com, {pattrsn, jegonzal, drothchild}@berkeley.edu  Abstract: The computation demand for machine learning (ML) has grown rapidly recently, which comes with a number of costs. Estimating the energy cost helps measure its environmental impact and finding greener strategies, yet it is challenging without detailed information . We calculate the energy use and carbon footprint of several recent large models\u2014 T5 , Meena , GShard , Switch Transformer , and GPT-3 \u2014and refine earlier estimates for the neural architecture search that found Evolved Transformer . We highlight the following opportunities to improve energy efficiency and CO 2  equivalent emissions ( CO 2 e ): \u25cf Large but sparsely activated DNNs can consume <1/10th the energy of large, dense DNNs without sacrificing accuracy despite using as many or even more parameters. \u25cf Geographic location matters for ML workload scheduling since the fraction of carbon-free energy and resulting CO 2 e vary ~5X-10X, even within the same country and the same organization. We are now optimizing where and when large models are trained. \u25cf Specific datacenter infrastructure matters, as Cloud datacenters can be ~1.4-2X more energy efficient than typical datacenters, and the ML-oriented accelerators inside them can be ~2-5X more effective than off-the-shelf systems. Remarkably, the choice of DNN, datacenter, and processor can reduce the carbon footprint up to ~100-1000X. These large factors also make retroactive estimates of energy cost difficult. To avoid miscalculations, we believe ML papers requiring large computational resources should make energy consumption and CO 2 e explicit when practical. We are working to be more transparent about energy use and CO 2 e in our future research. To help reduce the carbon footprint of ML, we believe energy usage and CO 2 e should be a key metric in evaluating models, and we are collaborating with MLPerf developers to include energy usage during training and inference in this industry standard benchmark."}, {"role": "assistant", "content": "1.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: All That\u2019s \u2018Human\u2019 Is Not Gold: Evaluating Human Evaluation of Generated Text\nAbstract: Human evaluations are typically considered the gold standard in natural language generation, but as models\u2019 fluency improves, how well can evaluators detect and judge machine-generated text? We run a study assessing non-experts\u2019 ability to distinguish between human- and machine-authored text (GPT2 and GPT3) in three domains (stories, news articles, and recipes). We find that, without training, evaluators distinguished between GPT3- and human-authored text at random chance level. We explore three approaches for quickly training evaluators to better identify GPT3-authored text (detailed instructions, annotated examples, and paired examples) and find that while evaluators\u2019 accuracy improved up to 55%, it did not significantly improve across the three domains. Given the inconsistent results across text domains and the often contradictory reasons evaluators gave for their judgments, we examine the role untrained human evaluations play in NLG evaluation and provide recommendations to NLG researchers for improving human evaluations of text generated from state-of-the-art models."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: MMM : Exploring Conditional Multi-Track Music Generation with the Transformer\nAbstract: We propose the Multi-Track Music Machine (MMM), a generative system based on the Transformer architecture that is capable of generating multi-track music. In contrast to previous work, which represents musical material as a single time-ordered sequence, where the musical events corresponding to different tracks are interleaved, we create a time-ordered sequence of musical events for each track and concatenate several tracks into a single sequence. This takes advantage of the Transformer's attention-mechanism, which can adeptly handle long-term dependencies. We explore how various representations can offer the user a high degree of control at generation time, providing an interactive demo that accommodates track-level and bar-level inpainting, and offers control over track instrumentation and note density."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Large Language Models Can Be Strong Differentially Private Learners\nAbstract: Differentially Private (DP) learning has seen limited success for building large deep learning models of text, and attempts at straightforwardly applying Differentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have resulted in large performance drops and high computational overhead. We show that this performance drop can be mitigated with (1) the use of large pretrained models; (2) hyperparameters that suit DP optimization; and (3) fine-tuning objectives aligned with the pretraining procedure. With these factors set right, we obtain private NLP models that outperform state-of-the-art private training approaches and strong nonprivate baselines\u2014by directly fine-tuning pretrained models with DP optimization on moderately-sized corpora. To address the computational challenge of running DP-SGD with large Transformers, we propose a memory saving technique that allows clipping in DP-SGD to run without instantiating per-example gradients for any layer in the model. The technique enables privately training Transformers with almost the same memory cost as non-private training at a modest run-time overhead. Contrary to conventional wisdom that DP optimization fails at learning high-dimensional models (due to noise that scales with dimension) empirical results reveal that private learning with pretrained models tends to not suffer from dimension-dependent performance degradation."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Applying the Transformer to Character-level Transduction\nAbstract: The transformer has been shown to outperform recurrent neural network-based sequence-to-sequence models in various word-level NLP tasks. Yet for character-level transduction tasks, e.g. morphological inflection generation and historical text normalization, there are few works that outperform recurrent models using the transformer. In an empirical study, we uncover that, in contrast to recurrent sequence-to-sequence models, the batch size plays a crucial role in the performance of the transformer on character-level tasks, and we show that with a large enough batch size, the transformer does indeed outperform recurrent models. We also introduce a simple technique to handle feature-guided character-level transduction that further improves performance. With these insights, we achieve state-of-the-art performance on morphological inflection and historical text normalization. We also show that the transformer outperforms a strong baseline on two other character-level transduction tasks: grapheme-to-phoneme conversion and transliteration."}, {"role": "assistant", "content": "4.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: ExpBERT: Representation Engineering with Natural Language Explanations\nAbstract: Suppose we want to specify the inductive bias that married couples typically go on honeymoons for the task of extracting pairs of spouses from text. In this paper, we allow model developers to specify these types of inductive biases as natural language explanations. We use BERT fine-tuned on MultiNLI to \u201cinterpret\u201d these explanations with respect to the input sentence, producing explanation-guided representations of the input. Across three relation extraction tasks, our method, ExpBERT, matches a BERT baseline but with 3\u201320x less labeled data and improves on the baseline by 3\u201310 F1 points with the same amount of labeled data."}, {"role": "assistant", "content": "4.5"}]}
