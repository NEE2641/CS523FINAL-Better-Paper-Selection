{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Can We Automate Scientific Reviewing?\nAbstract: The rapid development of science and technology has been accompanied by an exponential growth in peer-reviewed scientific publications. At the same time, the review of each paper is a laborious process that must be carried out by subject matter experts. Thus, providing high-quality reviews of this growing number of papers is a significant challenge. In this work, we ask the question \u201ccan we automate scientific reviewing?\u201d, discussing the possibility of using state-of-the-art natural language processing (NLP) models to generate first-pass peer reviews for scientific papers. Arguably the most difficult part of this is defining what a \u201cgood\u201d review is in the first place, so we first discuss possible evaluation measures for such reviews. We then collect a dataset of papers in the machine learning domain, annotate them with different aspects of content covered in each review, and train targeted \u2217Corresponding author. summarization models that take in papers to generate reviews. Comprehensive experimental results show that system-generated reviews tend to touch upon more aspects of the paper than human-written reviews, but the generated text can suffer from lower constructiveness for all aspects except the explanation of the core ideas of the papers, which are largely factually correct. We finally summarize eight challenges in the pursuit of a good review generation system together with potential solutions, which, hopefully, will inspire more future research on this subject. We make all code, and the dataset publicly available: https://github. com/neulab/ReviewAdvisor as well as a ReviewAdvisor system: http://review.nlpedia.ai/ (See demo screenshot in A.2). The review of this paper (without TL;QR section) written by the system of this paper can be found A.1"}, {"role": "assistant", "content": "Expertise rating: 4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Boosting Crowd Counting with Transformers\nAbstract: Significant progress on the crowd counting problem has been achieved by integrating larger context into convolutional neural networks (CNNs). This indicates that global scene context is essential, despite the seemingly bottom-up nature of the problem. This may be explained by the fact that context knowledge can adapt and improve local feature extraction to a given scene. In this paper, we therefore investigate the role of global context for crowd counting. Specifically, a pure transformer is used to extract features with global information from overlapping image patches. Inspired by classification, we add a context token to the input sequence, to facilitate information exchange with tokens corresponding to image patches throughout transformer layers. Due to the fact that transformers do not explicitly model the tried-and-true channel-wise interactions, we propose a token-attention module (TAM) to recalibrate encoded features through channel-wise attention informed by the context token. Beyond that, it is adopted to predict the total person count of the image through regression-token module (RTM). Extensive experiments demonstrate that our method achieves state-of-the-art performance on various datasets, including ShanghaiTech, UCF-QNRF, JHU-CROWD++ and NWPU. On the large-scale JHU-CROWD++ dataset, our method improves over the previous best results by 26.9% and 29.9% in terms of MAE and MSE, respectively."}, {"role": "assistant", "content": "Expertise rating: 1.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Distributionally Robust Language Modeling\nAbstract: Language models are generally trained on data spanning a wide range of topics (e.g., news, reviews, fiction), but they might be applied to an a priori unknown target distribution (e.g., restaurant reviews). In this paper, we first show that training on text outside the test distribution can degrade test performance when using standard maximum likelihood (MLE) training. To remedy this without the knowledge of the test distribution, we propose an approach which trains a model that performs well over a wide range of potential test distributions. In particular, we derive a new distributionally robust optimization (DRO) procedure which minimizes the loss of the model over the worst-case mixture of topics with sufficient overlap with the training distribution. Our approach, called topic conditional value at risk (topic CVaR), obtains a 5.5 point perplexity reduction over MLE when the language models are trained on a mixture of Yelp reviews and news and tested only on reviews."}, {"role": "assistant", "content": "Expertise rating: 3.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Direct Uncertainty Prediction for Medical Second Opinions\nAbstract: The issue of disagreements amongst human experts is a ubiquitous one in both machine learning and medicine. In medicine, this often corresponds to doctor disagreements on a patient diagnosis. In this work, we show that machine learning models can be trained to give uncertainty scores to data instances that might result in high expert disagreements. In particular, they can identify patient cases that would benefit most from a medical second opinion. Our central methodological finding is that Direct Uncertainty Prediction (DUP), training a model to predict an uncertainty score directly from the raw patient features, works better than Uncertainty Via Classification, the two-step process of training a classifier and postprocessing the output distribution to give an uncertainty score. We show this both with a theoretical result, and on extensive evaluations on a large scale medical imaging application."}, {"role": "assistant", "content": "Expertise rating: 3.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Measuring Coding Challenge Competence With APPS\nAbstract: While programming is one of the most broadly applicable skills in modern society, it is unclear how well state-of-the-art machine learning models can write code. Despite its importance, there has been surprisingly little work on evaluating code generation, and it can be difficult to assess code generation performance in an accurate and rigorous manner. To meet this challenge, we introduce APPS, a benchmark for code generation. Unlike prior work in more restricted settings, our benchmark measures the ability of models to take an arbitrary natural language specification and generate satisfactory Python code. Similar to how companies assess candidate software developers, we evaluate models by checking their generated code on test cases. Our benchmark includes 10,000 problems, which range from having simple oneline solutions to being substantial algorithmic challenges. We fine-tune large language models on both GitHub and our training set, and we find that the prevalence of syntax errors is decreasing exponentially as models improve. Recent models such as GPT-Neo can pass approximately 20% of the test cases of introductory problems, so we find that machine learning models are now beginning to learn how to code. As the social significance of automatic code generation increases over the coming years, our benchmark can provide an objective measure for tracking advancements. \u201cEverybody should learn to program a computer, because it teaches you how to think.\u201d \u2013 Steve Jobs"}, {"role": "assistant", "content": "Expertise rating: 4.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: A Diagnostic Study of Explainability Techniques for Text Classification\nAbstract: Recent developments in machine learning have introduced models that approach human performance at the cost of increased architectural complexity. Efforts to make the rationales behind the models' predictions transparent have inspired an abundance of new explainability techniques. Provided with an already trained model, they compute saliency scores for the words of an input instance. However, there exists no definitive guide on (i) how to choose such a technique given a particular application task and model architecture, and (ii) the benefits and drawbacks of using each such technique. In this paper, we develop a comprehensive list of diagnostic properties for evaluating existing explainability techniques. We then employ the proposed list to compare a set of diverse explainability techniques on downstream text classification tasks and neural network architectures. We also compare the saliency scores assigned by the explainability techniques with human annotations of salient input regions to find relations between a model's performance and the agreement of its rationales with human ones. Overall, we find that the gradient-based explanations perform best across tasks and model architectures, and we present further insights into the properties of the reviewed explainability techniques."}, {"role": "assistant", "content": "Expertise rating: 4.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Hausdorff dimension, heavy tails, and generalization in neural networks\nAbstract: Despite its success in a wide range of applications, characterizing the generalization properties of stochastic gradient descent (SGD) in non-convex deep learning problems is still an important challenge. While modeling the trajectories of SGD via stochastic differential equations (SDE) under heavy-tailed gradient noise has recently shed light over several peculiar characteristics of SGD, a rigorous treatment of the generalization properties of such SDEs in a learning theoretical framework is still missing. Aiming to bridge this gap, in this paper, we prove generalization bounds for SGD under the assumption that its trajectories can be well-approximated by a Feller process, which defines a rich class of Markov processes that include several recent SDE representations (both Brownian or heavy-tailed) as its special case. We show that the generalization error can be controlled by the Hausdorff dimension of the trajectories, which is intimately linked to the tail behavior of the driving process. Our results imply that heavier-tailed processes should achieve better generalization; hence, the tail-index of the process can be used as a notion of \u2018capacity metric\u2019. We support our theory with experiments on deep neural networks illustrating that the proposed capacity metric accurately estimates the generalization error, and it does not necessarily grow with the number of parameters unlike the existing capacity metrics in the literature."}, {"role": "assistant", "content": "Expertise rating: 2.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Distilling Knowledge from Reader to Retriever for Question Answering\nAbstract: The task of information retrieval is an important component of many natural language processing systems, such as open domain question answering. While traditional methods were based on hand-crafted features, continuous representations based on neural networks recently obtained competitive results. A challenge of using such methods is to obtain supervised data to train the retriever model, corresponding to pairs of query and support documents. In this paper, we propose a technique to learn retriever models for downstream tasks, inspired by knowledge distillation, and which does not require annotated pairs of query and documents. Our approach leverages attention scores of a reader model, used to solve the task based on retrieved documents, to obtain synthetic labels for the retriever. We evaluate our method on question answering, obtaining state-of-the-art results."}, {"role": "assistant", "content": "Expertise rating: 1.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Differentiable Weighted Finite-State Transducers\nAbstract: We introduce a framework for automatic differentiation with weighted finite-state transducers (WFSTs) allowing them to be used dynamically at training time. Through the separation of graphs from operations on graphs, this framework enables the exploration of new structured loss functions which in turn eases the encoding of prior knowledge into learning algorithms. We show how the framework can combine pruning and back-off in transition models with various sequence-level loss functions. We also show how to learn over the latent decomposition of phrases into word pieces. Finally, to demonstrate that WFSTs can be used in the interior of a deep neural network, we propose a convolutional WFST layer which maps lower-level representations to higher-level representations and can be used as a drop-in replacement for a traditional convolution. We validate these algorithms with experiments in handwriting recognition and speech recognition."}, {"role": "assistant", "content": "Expertise rating: 3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: A Persistent Spatial Semantic Representation for High-level Natural Language Instruction Execution\nAbstract: Natural language provides an accessible and expressive interface to specify long-term tasks for robotic agents. However, non-experts are likely to specify such tasks with high-level instructions, which abstract over specific robot actions through several layers of abstraction. We propose that key to bridging this gap between language and robot actions over long execution horizons are persistent representations. We propose a persistent spatial semantic representation method, and show how it enables building an agent that performs hierarchical reasoning to effectively execute long-term tasks. We evaluate our approach on the ALFRED benchmark and achieve state-of-the-art results, despite completely avoiding the commonly used step-by-step instructions. https://hlsm-alfred. github.io/"}, {"role": "assistant", "content": "Expertise rating: 4.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Relevance-guided Supervision for OpenQA with ColBERT\nAbstract: Abstract Systems for Open-Domain Question Answering (OpenQA) generally depend on a retriever for finding candidate passages in a large corpus and a reader for extracting answers from those passages. In much recent work, the retriever is a learned component that uses coarse-grained vector representations of questions and passages. We argue that this modeling choice is insufficiently expressive for dealing with the complexity of natural language questions. To address this, we define ColBERT-QA, which adapts the scalable neural retrieval model ColBERT to OpenQA. ColBERT creates fine-grained interactions between questions and passages. We propose an efficient weak supervision strategy that iteratively uses ColBERT to create its own training data. This greatly improves OpenQA retrieval on Natural Questions, SQuAD, and TriviaQA, and the resulting system attains state-of-the-art extractive OpenQA performance on all three datasets."}, {"role": "assistant", "content": "Expertise rating: 2.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: BERT Rediscovers the Classical NLP Pipeline\nAbstract: Pre-trained text encoders have rapidly advanced the state of the art on many NLP tasks. We focus on one such model, BERT, and aim to quantify where linguistic information is captured within the network. We find that the model represents the steps of the traditional NLP pipeline in an interpretable and localizable way, and that the regions responsible for each step appear in the expected sequence: POS tagging, parsing, NER, semantic roles, then coreference. Qualitative analysis reveals that the model can and often does adjust this pipeline dynamically, revising lower-level decisions on the basis of disambiguating information from higher-level representations."}, {"role": "assistant", "content": "Expertise rating: 3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Analyzing the Use of Character-Level Translation with Sparse and Noisy Datasets\nAbstract: This paper provides an analysis of character-level machine translation models used in pivot-based translation when applied to sparse and noisy datasets, such as crowdsourced movie subtitles. In our experiments, we find that such characterlevel models cut the number of untranslated words by over 40% and are especially competitive (improvements of 2-3 BLEU points) in the case of limited training data. We explore the impact of character alignment, phrase table filtering, bitext size and the choice of pivot language on translation quality. We further compare cascaded translation models to the use of synthetic training data via multiple pivots, and we find that the latter works significantly better. Finally, we demonstrate that neither word- nor character-BLEU correlate perfectly with human judgments, due to BLEU\u2019s sensitivity to length."}, {"role": "assistant", "content": "Expertise rating: 2.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Interpretable Neural Predictions with Differentiable Binary Variables\nAbstract: The success of neural networks comes hand in hand with a desire for more interpretability. We focus on text classifiers and make them more interpretable by having them provide a justification\u2013a rationale\u2013for their predictions. We approach this problem by jointly training two neural network models: a latent model that selects a rationale (i.e. a short and informative part of the input text), and a classifier that learns from the words in the rationale alone. Previous work proposed to assign binary latent masks to input positions and to promote short selections via sparsity-inducing penalties such as L0 regularisation. We propose a latent model that mixes discrete and continuous behaviour allowing at the same time for binary selections and gradient-based training without REINFORCE. In our formulation, we can tractably compute the expected value of penalties such as L0, which allows us to directly optimise the model towards a pre-specified text selection rate. We show that our approach is competitive with previous work on rationale extraction, and explore further uses in attention mechanisms."}, {"role": "assistant", "content": "Expertise rating: 2.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Beyond I.I.D.: Three Levels of Generalization for Question Answering on Knowledge Bases\nAbstract: Existing studies on question answering on knowledge bases (KBQA) mainly operate with the standard i.i.d. assumption, i.e., training distribution over questions is the same as the test distribution. However, i.i.d. may be neither achievable nor desirable on large-scale KBs because 1) true user distribution is hard to capture and 2) randomly sampling training examples from the enormous space would be data-inefficient. Instead, we suggest that KBQA models should have three levels of built-in generalization: i.i.d., compositional, and zero-shot. To facilitate the development of KBQA models with stronger generalization, we construct and release a new large-scale, high-quality dataset with 64,331 questions, GrailQA, and provide evaluation settings for all three levels of generalization. In addition, we propose a novel BERT-based KBQA model. The combination of our dataset and model enables us to thoroughly examine and demonstrate, for the first time, the key role of pre-trained contextual embeddings like BERT in the generalization of KBQA.1"}, {"role": "assistant", "content": "Expertise rating: 3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Human-in-the-Loop Interpretability Prior\nAbstract: We often desire our models to be interpretable as well as accurate. Prior work on optimizing models for interpretability has relied on easy-to-quantify proxies for interpretability, such as sparsity or the number of operations required. In this work, we optimize for interpretability by directly including humans in the optimization loop. We develop an algorithm that minimizes the number of user studies to find models that are both predictive and interpretable and demonstrate our approach on several data sets. Our human subjects results show trends towards different proxy notions of interpretability on different datasets, which suggests that different proxies are preferred on different tasks."}, {"role": "assistant", "content": "Expertise rating: 3.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Federated Learning Meets Fairness and Differential Privacy\nAbstract: Deep learning\u2019s unprecedented success raises several ethical concerns ranging from biased predictions to data privacy. Researchers tackle these issues by introducing fairness metrics, or federated learning, or differential privacy. A first, this work presents an ethical federated learning model, incorporating all three measures simultaneously. Experiments on the Adult, Bank and Dutch datasets highlight the resulting \u201cempirical interplay\u201d between accuracy, fairness, and privacy."}, {"role": "assistant", "content": "Expertise rating: 4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: On Nonconvex Optimization for Machine Learning: Gradients, Stochasticity, and Saddle Points\nAbstract: Gradient descent (GD) and stochastic gradient descent (SGD) are the workhorses of large-scale machine learning. While classical theory focused on analyzing the performance of these methods in convex optimization problems, the most notable successes in machine learning have involved nonconvex optimization, and a gap has arisen between theory and practice. Indeed, traditional analyses of GD and SGD show that both algorithms converge to stationary points efficiently. But these analyses do not take into account the possibility of converging to saddle points. More recent theory has shown that GD and SGD can avoid saddle points, but the dependence on dimension in these analyses is polynomial. For modern machine learning, where the dimension can be in the millions, such dependence would be catastrophic. We analyze perturbed versions of GD and SGD and show that they are truly efficient---their dimension dependence is only polylogarithmic. Indeed, these algorithms converge to second-order stationary points in essentially the same time as they take to converge to classical first-order stationary points."}, {"role": "assistant", "content": "Expertise rating: 4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Grounded Graph Decoding Improves Compositional Generalization in Question Answering\nAbstract: Question answering models struggle to generalize to novel compositions of training patterns, such to longer sequences or more complex test structures. Current end-to-end models learn a flat input embedding which can lose input syntax context. Prior approaches improve generalization by learning permutation invariant models, but these methods do not scale to more complex train-test splits. We propose Grounded Graph Decoding, a method to improve compositional generalization of language representations by grounding structured predictions with an attention mechanism. Grounding enables the model to retain syntax information from the input in thereby significantly improving generalization over complex inputs. By predicting a structured graph containing conjunctions of query clauses, we learn a group invariant representation without making assumptions on the target domain. Our model significantly outperforms state-ofthe-art baselines on the Compositional Freebase Questions (CFQ) dataset, a challenging benchmark for compositional generalization in question answering. Moreover, we effectively solve the MCD1 split with 98% accuracy. All source is available at https:// github.com/gaiyu0/cfq."}, {"role": "assistant", "content": "Expertise rating: 1.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: FNet: Mixing Tokens with Fourier Transforms\nAbstract: We show that Transformer encoder architectures can be massively sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that \u201cmix\u201d input tokens. These linear transformations, along with simple nonlinearities in feed-forward layers, are sufficient to model semantic relationships in several text classification tasks. Perhaps most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92% of the accuracy of BERT on the GLUE benchmark, but pre-trains and runs up to seven times faster on GPUs and twice as fast on TPUs. The resulting model, which we name FNet, scales very efficiently to long inputs, matching the accuracy of the most accurate \u201cefficient\u201d Transformers on the Long Range Arena benchmark, but training and running faster across all sequence lengths on GPUs and relatively shorter sequence lengths on TPUs. Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes: for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts."}, {"role": "assistant", "content": "Expertise rating: 2.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Reliable Decisions with Threshold Calibration\nAbstract: Decision makers rely on probabilistic forecasts to predict the loss of different decision rules before deployment. When the forecasted probabilities match the true frequencies, predicted losses will be accurate. Although perfect forecasts are typically impossible, probabilities can be calibrated to match the true frequencies on average. However, we find that this average notion of calibration, which is typically used in practice, does not necessarily guarantee accurate decision loss prediction. Specifically in the regression setting, the loss of threshold decisions, which are decisions based on whether the forecasted outcome falls above or below a cutoff, might not be predicted accurately. We propose a stronger notion of calibration called threshold calibration, which is exactly the condition required to ensure that decision loss is predicted accurately for threshold decisions. We provide an efficient algorithm which takes an uncalibrated forecaster as input and provably outputs a threshold-calibrated forecaster. Our procedure allows downstream decision makers to confidently estimate the loss of any threshold decision under any threshold loss function. Empirically, threshold calibration improves decision loss prediction without compromising on the quality of the decisions in two real-world settings: hospital scheduling decisions and resource allocation decisions."}, {"role": "assistant", "content": "Expertise rating: 2.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Do We Know What We Don't Know? Studying Unanswerable Questions beyond SQuAD 2.0\nAbstract: Understanding when a text snippet does not provide a sought after information is an essential part of natural language understanding. Recent work (SQuAD 2.0, Rajpurkar et al., 2018) has attempted to make some progress in this direction by enriching the SQuAD dataset for the Extractive QA task with unanswerable questions. However, as we show, the performance of a top system trained on SQuAD 2.0 drops considerably in out-of-domain scenarios, limiting its use in practical situations. In order to study this we build an out-of-domain corpus, focusing on simple event-based questions and distinguish between two types of IDK questions: competitive questions, where the context includes an entity of the same type as the expected answer, and simpler, noncompetitive questions where there is no entity of the same type in the context. We find that SQuAD 2.0-based models fail even in the case of the simpler questions. We then analyze the similarities and differences between the IDK phenomenon in Extractive QA and the Recognizing Textual Entailments task (RTE, Dagan et al., 2013) and investigate the extent to which the latter can be used to improve the performance.1"}, {"role": "assistant", "content": "Expertise rating: 4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: BASE Layers: Simplifying Training of Large, Sparse Models\nAbstract: We introduce a new balanced assignment of experts (BASE) layer for large language models that greatly simplifies existing high capacity sparse layers. Sparse layers can dramatically improve the efficiency of training and inference by routing each token to specialized expert modules that contain only a small fraction of the model parameters. However, it can be difficult to learn balanced routing functions that make full use of the available experts; existing approaches typically use routing heuristics or auxiliary expert-balancing loss functions. In contrast, we formulate token-to-expert allocation as a linear assignment problem, allowing an optimal assignment in which each expert receives an equal number of tokens. This optimal assignment scheme improves efficiency by guaranteeing balanced compute loads, and also simplifies training by not requiring any new hyperparameters or auxiliary losses. Code is publicly released."}, {"role": "assistant", "content": "Expertise rating: 2.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: A Cordial Sync: Going Beyond Marginal Policies for Multi-Agent Embodied Tasks\nAbstract: Autonomous agents must learn to collaborate. It is not scalable to develop a new centralized agent every time a task's difficulty outpaces a single agent's abilities. While multi-agent collaboration research has flourished in gridworld-like environments, relatively little work has considered visually rich domains. Addressing this, we introduce the novel task FurnMove in which agents work together to move a piece of furniture through a living room to a goal. Unlike existing tasks, FurnMove requires agents to coordinate at every timestep. We identify two challenges when training agents to complete FurnMove: existing decentralized action sampling procedures do not permit expressive joint action policies and, in tasks requiring close coordination, the number of failed actions dominates successful actions. To confront these challenges we introduce SYNC-policies (synchronize your actions coherently) and CORDIAL (coordination loss). Using SYNC-policies and CORDIAL, our agents achieve a 58% completion rate on FurnMove, an impressive absolute gain of 25 percentage points over competitive decentralized baselines. Our dataset, code, and pretrained models are available at this https URL ."}, {"role": "assistant", "content": "Expertise rating: 3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Preventing undesirable behavior of intelligent machines\nAbstract: Making well-behaved algorithms Machine learning algorithms are being used in an ever-increasing number of applications, and many of these applications affect quality of life. Yet such algorithms often exhibit undesirable behavior, from various types of bias to causing financial loss or delaying medical diagnoses. In standard machine learning approaches, the burden of avoiding this harmful behavior is placed on the user of the algorithm, who most often is not a computer scientist. Thomas et al. introduce a general framework for algorithm design in which this burden is shifted from the user to the designer of the algorithm. The researchers illustrate the benefits of their approach using examples in gender fairness and diabetes management. Science, this issue p. 999 A machine learning algorithm design framework shifts the burden of avoiding undesirable behavior from user to designer. Intelligent machines using machine learning algorithms are ubiquitous, ranging from simple data analysis and pattern recognition tools to complex systems that achieve superhuman performance on various tasks. Ensuring that they do not exhibit undesirable behavior\u2014that they do not, for example, cause harm to humans\u2014is therefore a pressing problem. We propose a general and flexible framework for designing machine learning algorithms. This framework simplifies the problem of specifying and regulating undesirable behavior. To show the viability of this framework, we used it to create machine learning algorithms that precluded the dangerous behavior caused by standard machine learning algorithms in our experiments. Our framework for designing machine learning algorithms simplifies the safe and responsible application of machine learning."}, {"role": "assistant", "content": "Expertise rating: 3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Visually Grounded Neural Syntax Acquisition\nAbstract: We present the Visually Grounded Neural Syntax Learner (VG-NSL), an approach for learning syntactic representations and structures without any explicit supervision. The model learns by looking at natural images and reading paired captions. VG-NSL generates constituency parse trees of texts, recursively composes representations for constituents, and matches them with images. We define concreteness of constituents by their matching scores with images, and use it to guide the parsing of text. Experiments on the MSCOCO data set show that VG-NSL outperforms various unsupervised parsing approaches that do not use visual grounding, in terms of F1 scores against gold parse trees. We find that VGNSL is much more stable with respect to the choice of random initialization and the amount of training data. We also find that the concreteness acquired by VG-NSL correlates well with a similar measure defined by linguists. Finally, we also apply VG-NSL to multiple languages in the Multi30K data set, showing that our model consistently outperforms prior unsupervised approaches."}, {"role": "assistant", "content": "Expertise rating: 1.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Scalable Gradients for Stochastic Differential Equations\nAbstract: The adjoint sensitivity method scalably computes gradients of solutions to ordinary differential equations. We generalize this method to stochastic differential equations, allowing time-efficient and constant-memory computation of gradients with high-order adaptive solvers. Specifically, we derive a stochastic differential equation whose solution is the gradient, a memory-efficient algorithm for caching noise, and conditions under which numerical solutions converge. In addition, we combine our method with gradient-based stochastic variational inference for latent stochastic differential equations. We use our method to fit stochastic dynamics defined by neural networks, achieving competitive performance on a 50-dimensional motion capture dataset."}, {"role": "assistant", "content": "Expertise rating: 4.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks\nAbstract: We present ALFRED (Action Learning From Realistic Environments and Directives), a benchmark for learning a mapping from natural language instructions and egocentric vision to sequences of actions for household tasks. ALFRED includes long, compositional tasks with non-reversible state changes to shrink the gap between research benchmarks and real-world applications. ALFRED consists of expert demonstrations in interactive visual environments for 25k natural language directives. These directives contain both high-level goals like \u201cRinse off a mug and place it in the coffee maker.\u201d and low-level language instructions like \u201cWalk to the coffee maker on the right.\u201d ALFRED tasks are more complex in terms of sequence length, action space, and language than existing vision- and-language task datasets. We show that a baseline model based on recent embodied vision-and-language tasks performs poorly on ALFRED, suggesting that there is significant room for developing innovative grounded visual language understanding models with this benchmark."}, {"role": "assistant", "content": "Expertise rating: 4.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: HINT3: Raising the bar for Intent Detection in the Wild\nAbstract: Intent Detection systems in the real world are exposed to complexities of imbalanced datasets containing varying perception of intent, unintended correlations and domain-specific aberrations. To facilitate benchmarking which can reflect near real-world scenarios, we introduce 3 new datasets created from live chatbots in diverse domains. Unlike most existing datasets that are crowdsourced, our datasets contain real user queries received by the chatbots and facilitates penalising unwanted correlations grasped during the training process. We evaluate 4 NLU platforms and a BERT based classifier and find that performance saturates at inadequate levels on test sets because all systems latch on to unintended patterns in training data."}, {"role": "assistant", "content": "Expertise rating: 5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Training Dynamics for Text Summarization Models\nAbstract: Pre-trained language models (e.g. BART) have shown impressive results when fine-tuned on large summarization datasets. However, little is understood about this fine-tuning process, including what knowledge is retained from pre-training models or how content selection and generation strategies are learnt across iterations. In this work, we analyze the training dynamics for generation models, focusing on news summarization. Across different datasets (CNNDM, XSUM, MEDIASUM) and summary properties, such as abstractiveness and hallucination, we study what the model learns at different stages of its fine-tuning process. We find that properties such as copy behavior are learnt earlier in the training process and these observations are robust across domains. On the other hand, factual errors, such as hallucination of unsupported facts, are learnt in the later stages, and this behavior is more varied across domains. Based on these observations, we explore complementary approaches for modifying training: first, disregarding high-loss tokens that are challenging to learn and second, disregarding low-loss tokens that are learnt very quickly. This simple training modification allows us to configure our model to achieve different goals, such as improving factuality or improving abstractiveness."}, {"role": "assistant", "content": "Expertise rating: 3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: How Deep Are the Fakes? Focusing on Audio Deepfake: A Survey\nAbstract: Deepfake is content or material that is synthetically generated or manipulated using artificial intelligence (AI) methods, to be passed off as real and can include audio, video, image, and text synthesis. This survey has been conducted with a different perspective compared to existing survey papers, that mostly focus on just video and image deepfakes. This survey not only evaluates generation and detection methods in the different deepfake categories, but mainly focuses on audio deepfakes that are overlooked in most of the existing surveys. This paper\u2019s most important contribution is to critically analyze and provide a unique source of audio deepfake research, mostly ranging from 2016 to 2020. To the best of our knowledge, this is the first survey focusing on audio deepfakes in English. This survey provides readers with a summary of 1) different deepfake categories 2) how they could be created and detected 3) the most recent trends in this domain and shortcomings in detection methods 4) audio deepfakes, how they are created and detected in more detail which is the main focus of this paper. We found that Generative Adversarial Networks(GAN), Convolutional Neural Networks (CNN), and Deep Neural Networks (DNN) are common ways of creating and detecting deepfakes. In our evaluation of over 140 methods we found that the majority of the focus is on video deepfakes and in particular in the generation of video deepfakes. We found that for text deepfakes there are more generation methods but very few robust methods for detection, including fake news detection, which has become a controversial area of research because of the potential of heavy overlaps with human generation of fake content. This paper is an abbreviated version of the full survey and reveals a clear need to research audio deepfakes and particularly detection of audio deepfakes."}, {"role": "assistant", "content": "Expertise rating: 1.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation\nAbstract: Abstract Pipelined NLP systems have largely been superseded by end-to-end neural modeling, yet nearly all commonly used models still require an explicit tokenization step. While recent tokenization approaches based on data-derived subword lexicons are less brittle than manually engineered tokenizers, these techniques are not equally suited to all languages, and the use of any fixed vocabulary may limit a model\u2019s ability to adapt. In this paper, we present Canine, a neural encoder that operates directly on character sequences\u2014without explicit tokenization or vocabulary\u2014and a pre-training strategy that operates either directly on characters or optionally uses subwords as a soft inductive bias. To use its finer-grained input effectively and efficiently, Canine combines downsampling, which reduces the input sequence length, with a deep transformer stack, which encodes context. Canine outperforms a comparable mBert model by 5.7 F1 on TyDi QA, a challenging multilingual benchmark, despite having fewer model parameters."}, {"role": "assistant", "content": "Expertise rating: 3.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: MAUVE: Measuring the Gap Between Neural Text and Human Text using Divergence Frontiers\nAbstract: As major progress is made in open-ended text generation, measuring how close machine-generated text is to human language remains a critical open problem. We introduce MAUVE, a comparison measure for open-ended text generation, which directly compares the learnt distribution from a text generation model to the distribution of human-written text using divergence frontiers. MAUVE scales up to modern text generation models by computing information divergences in a quantized embedding space. Through an extensive empirical study on three open-ended generation tasks, we find that MAUVE identifies known properties of generated text, scales naturally with model size, and correlates with human judgments, with fewer restrictions than existing distributional evaluation metrics."}, {"role": "assistant", "content": "Expertise rating: 2.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: SEQ\u02c63: Differentiable Sequence-to-Sequence-to-Sequence Autoencoder for Unsupervised Abstractive Sentence Compression\nAbstract: Neural sequence-to-sequence models are currently the dominant approach in several natural language processing tasks, but require large parallel corpora. We present a sequence-to-sequence-to-sequence autoencoder (SEQ\u02c63), consisting of two chained encoder-decoder pairs, with words used as a sequence of discrete latent variables. We apply the proposed model to unsupervised abstractive sentence compression, where the first and last sequences are the input and reconstructed sentences, respectively, while the middle sequence is the compressed sentence. Constraining the length of the latent word sequences forces the model to distill important information from the input. A pretrained language model, acting as a prior over the latent sequences, encourages the compressed sentences to be human-readable. Continuous relaxations enable us to sample from categorical distributions, allowing gradient-based optimization, unlike alternatives that rely on reinforcement learning. The proposed model does not require parallel text-summary pairs, achieving promising results in unsupervised sentence compression on benchmark datasets."}, {"role": "assistant", "content": "Expertise rating: 3.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Conversion from Paninian Karakas to Universal Dependencies for Hindi Dependency Treebank\nAbstract: Universal Dependencies (UD) are gaining much attention of late for systematic evaluation of cross-lingual techniques for crosslingual dependency parsing. In this paper we present our work in line with UD. Our contribution to this is manifold. We extend UD to Indian languages through conversion of P\u0101n\u1ecbnian Dependencies to UD for the Hindi Dependency Treebank (HDTB). We discuss the differences in annotation in both the schemes, present parsing experiments for both the formalisms and empirically evaluate their weaknesses and strengths for Hindi. We produce an automatically converted Hindi Treebank conforming to the international standard UD scheme, making it useful as a resource for multilingual language technology."}, {"role": "assistant", "content": "Expertise rating: 4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: How Does Fine-tuning Affect the Geometry of Embedding Space: A Case Study on Isotropy\nAbstract: It is widely accepted that fine-tuning pretrained language models usually brings about performance improvements in downstream tasks. However, there are limited studies on the reasons behind this effectiveness, particularly from the viewpoint of structural changes in the embedding space. Trying to fill this gap, in this paper, we analyze the extent to which the isotropy of the embedding space changes after fine-tuning. We demonstrate that, even though isotropy is a desirable geometrical property, fine-tuning does not necessarily result in isotropy enhancements. Moreover, local structures in pre-trained contextual word representations (CWRs), such as those encoding token types or frequency, undergo a massive change during fine-tuning. Our experiments show dramatic growth in the number of elongated directions in the embedding space, which, in contrast to pre-trained CWRs, carry the essential linguistic knowledge in the fine-tuned embedding space, making existing isotropy enhancement methods ineffective."}, {"role": "assistant", "content": "Expertise rating: 5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Privacy-Aware Text Rewriting\nAbstract: Biased decisions made by automatic systems have led to growing concerns in research communities. Recent work from the NLP community focuses on building systems that make fair decisions based on text. Instead of relying on unknown decision systems or human decision-makers, we argue that a better way to protect data providers is to remove the trails of sensitive information before publishing the data. In light of this, we propose a new privacy-aware text rewriting task and explore two privacy-aware back-translation methods for the task, based on adversarial training and approximate fairness risk. Our extensive experiments on three real-world datasets with varying demographical attributes show that our methods are effective in obfuscating sensitive attributes. We have also observed that the fairness risk method retains better semantics and fluency, while the adversarial training method tends to leak less sensitive information."}, {"role": "assistant", "content": "Expertise rating: 4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: XORing Elephants: Novel Erasure Codes for Big Data\nAbstract: Distributed storage systems for large clusters typically use replication to provide reliability. Recently, erasure codes have been used to reduce the large storage overhead of three-replicated systems. Reed-Solomon codes are the standard design choice and their high repair cost is often considered an unavoidable price to pay for high storage efficiency and high reliability. \n \nThis paper shows how to overcome this limitation. We present a novel family of erasure codes that are efficiently repairable and offer higher reliability compared to Reed-Solomon codes. We show analytically that our codes are optimal on a recently identified tradeoff between locality and minimum distance. \n \nWe implement our new codes in Hadoop HDFS and compare to a currently deployed HDFS module that uses Reed-Solomon codes. Our modified HDFS implementation shows a reduction of approximately 2\u00d7 on the repair disk I/O and repair network traffic. The disadvantage of the new coding scheme is that it requires 14% more storage compared to Reed-Solomon codes, an overhead shown to be information theoretically optimal to obtain locality. Because the new codes repair failures faster, this provides higher reliability, which is orders of magnitude higher compared to replication."}, {"role": "assistant", "content": "Expertise rating: 5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Inconsistency in Conference Peer Review: Revisiting the 2014 NeurIPS Experiment\nAbstract: In this paper we revisit the 2014 NeurIPS experiment that examined inconsistency in conference peer review. We determine that 50% of the variation in reviewer quality scores was subjective in origin. Further, with seven years passing since the experiment we find that for accepted papers, there is no correlation between quality scores and impact of the paper as measured as a function of citation count. We trace the fate of rejected papers, recovering where these papers were eventually published. For these papers we find a correlation between quality scores and impact. We conclude that the reviewing process for the 2014 conference was good for identifying poor papers, but poor for identifying good papers. We give some suggestions for improving the reviewing process but also warn against removing the subjective element. Finally, we suggest that the real conclusion of the experiment is that the community should place less onus on the notion of \u2018top-tier conference publications\u2019 when assessing the quality of individual researchers."}, {"role": "assistant", "content": "Expertise rating: 4.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Simple, Scalable Adaptation for Neural Machine Translation\nAbstract: Fine-tuning pre-trained Neural Machine Translation (NMT) models is the dominant approach for adapting to new languages and domains. However, fine-tuning requires adapting and maintaining a separate model for each target task. We propose a simple yet efficient approach for adaptation in NMT. Our proposed approach consists of injecting tiny task specific adapter layers into a pre-trained model. These lightweight adapters, with just a small fraction of the original model size, adapt the model to multiple individual tasks simultaneously. We evaluate our approach on two tasks: (i) Domain Adaptation and (ii) Massively Multilingual NMT. Experiments on domain adaptation demonstrate that our proposed approach is on par with full fine-tuning on various domains, dataset sizes and model capacities. On a massively multilingual dataset of 103 languages, our adaptation approach bridges the gap between individual bilingual models and one massively multilingual model for most language pairs, paving the way towards universal machine translation."}, {"role": "assistant", "content": "Expertise rating: 4.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Identifying Unknown Unknowns in the Open World: Representations and Policies for Guided Exploration\nAbstract: Predictive models deployed in the real world may assign incorrect labels to instances with high confidence. Such errors or unknown unknowns are rooted in model incompleteness, and typically arise because of the mismatch between training data and the cases encountered at test time. As the models are blind to such errors, input from an oracle is needed to identify these failures. In this paper, we formulate and address the problem of informed discovery of unknown unknowns of any given predictive model where unknown unknowns occur due to systematic biases in the training data. We propose a model-agnostic methodology which uses feedback from an oracle to both identify unknown unknowns and to intelligently guide the discovery. We employ a two-phase approach which first organizes the data into multiple partitions based on the feature similarity of instances and the confidence scores assigned by the predictive model, and then utilizes an explore-exploit strategy for discovering unknown unknowns across these partitions. We demonstrate the efficacy of our framework by varying the underlying causes of unknown unknowns across various applications. To the best of our knowledge, this paper presents the first algorithmic approach to the problem of discovering unknown unknowns of predictive models."}, {"role": "assistant", "content": "Expertise rating: 4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Bifurcated Backbone Strategy for RGB-D Salient Object Detection\nAbstract: Multi-level feature fusion is a fundamental topic in computer vision. It has been exploited to detect, segment and classify objects at various scales. When multi-level features meet multi-modal cues, the optimal feature aggregation and multi-modal learning strategy become a hot potato. In this paper, we leverage the inherent multi-modal and multi-level nature of RGB-D salient object detection to devise a novel Bifurcated Backbone Strategy Network (BBS-Net). Our architecture, is simple, efficient, and backbone-independent. In particular, first, we propose to regroup the multi-level features into teacher and student features using a bifurcated backbone strategy (BBS). Second, we introduce a depth-enhanced module (DEM) to excavate informative depth cues from the channel and spatial views. Then, RGB and depth modalities are fused in a complementary way. Extensive experiments show that BBS-Net significantly outperforms 18 state-of-the-art (SOTA) models on eight challenging datasets under five evaluation measures, demonstrating the superiority of our approach (~4% improvement in S-measure $vs$ . the top-ranked model: DMRA). In addition, we provide a comprehensive analysis on the generalization ability of different RGB-D datasets and provide a powerful training set for future research. The complete algorithm, benchmark results, and post-processing toolbox are publicly available at https://github.com/zyjwuyan/BBS-Net."}, {"role": "assistant", "content": "Expertise rating: 2.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Stochastic optimization under distributional drift\nAbstract: We consider the problem of minimizing a convex function that is evolving according to unknown and possibly stochastic dynamics, which may depend jointly on time and on the decision variable itself. Such problems abound in the machine learning and signal processing literature, under the names of concept drift, stochastic tracking, and performative prediction. We provide novel non-asymptotic convergence guarantees for stochastic algorithms with iterate averaging, focusing on bounds valid both in expectation and with high probability. The efficiency estimates we obtain clearly decouple the contributions of optimization error, gradient noise, and time drift. Notably, we show that the tracking efficiency of the proximal stochastic gradient method depends only logarithmically on the initialization quality, when equipped with a stepdecay schedule. Numerical experiments illustrate our results."}, {"role": "assistant", "content": "Expertise rating: 3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: De Novo Molecule Design Through Molecular Generative Model Conditioned by 3D Information of Protein Binding Sites\nAbstract: De novo molecule design through molecular generative model is gaining increasing attention in recent years. Here a novel generative model was proposed by integrating the 3D structural information of the protein binding pocket into the conditional RNN (cRNN) model to control the generation of drug-like molecules. In this model, the composition of protein binding pocket is effectively characterized through a coarse-grain strategy and the three-dimensional information of the pocket can be represented by the sorted eigenvalues of the coulomb matrix (EGCM) of the coarse-grained atoms composing the binding pocket. In current work, we used our EGCM method and a previously reported binding pocket descriptor DeeplyTough to train cRNN models and compared their performance. It has been shown that the molecules generated with the control of protein environment information have a clear tendency on generating compounds with higher similarity to the original X-ray bound ligand than normal RNN model and also achieving better performance in terms of docking scores. Our results demonstrate the potential application of EGCM controlled generative model for the targeted molecule generation and guided exploration on the drug-like chemical space."}, {"role": "assistant", "content": "Expertise rating: 1.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: nmT5 - Is parallel data still relevant for pre-training massively multilingual language models?\nAbstract: Recently, mT5 - a massively multilingual version of T5 - leveraged a unified text-to-text format to attain state-of-the-art results on a wide variety of multilingual NLP tasks. In this paper, we investigate the impact of incorporating parallel data into mT5 pre-training. We find that multi-tasking language modeling with objectives such as machine translation during pre-training is a straightforward way to improve performance on downstream multilingual and cross-lingual tasks. However, the gains start to diminish as the model capacity increases, suggesting that parallel data might not be as essential for larger models. At the same time, even at larger model sizes, we find that pre-training with parallel data still provides benefits in the limited labelled data regime"}, {"role": "assistant", "content": "Expertise rating: 4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Automatic Phrase Recognition in Historical German\nAbstract: Due to a lack of annotated data, theories of historical syntax are often based on very small, manually compiled data sets. To enable the empirical evaluation of existing hypotheses, the present study explores the automatic recognition of phrases in historical German. Using modern and historical treebanks, training data for a neural sequence labeling tool and a probabilistic parser is created, and both methods are compared on a variety of data sets. The evaluation shows that the unlexicalized parser outperforms the sequence labeling approach, achieving F1-scores of 87%\u201391% on modern German and between 73% and 85% on different historical corpora. An error analysis indicates that accuracy decreases especially for longer phrases, but most of the errors concern incorrect phrase boundaries, suggesting further potential for improvement."}, {"role": "assistant", "content": "Expertise rating: 4.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: BERT-GT: Cross-sentence n-ary relation extraction with BERT and Graph Transformer\nAbstract: MOTIVATION\nA biomedical relation statement is commonly expressed in multiple sentences and consists of many concepts, including gene, disease, chemical, and mutation. To automatically extract information from biomedical literature, existing biomedical text-mining approaches typically formulate the problem as a cross-sentence n-ary relation-extraction task that detects relations among n entities across multiple sentences, and use either a graph neural network (GNN) with long short-term memory (LSTM) or an attention mechanism. Recently, Transformer has been shown to outperform LSTM on many natural language processing (NLP) tasks.\n\n\nRESULTS\nIn this work, we propose a novel architecture that combines Bidirectional Encoder Representations from Transformers with Graph Transformer (BERT-GT), through integrating a neighbor-attention mechanism into the BERT architecture. Unlike the original Transformer architecture, which utilizes the whole sentence(s) to calculate the attention of the current token, the neighbor-attention mechanism in our method calculates its attention utilizing only its neighbor tokens. Thus, each token can pay attention to its neighbor information with little noise. We show that this is critically important when the text is very long, as in cross-sentence or abstract-level relation-extraction tasks. Our benchmarking results show improvements of 5.44% and 3.89% in accuracy and F1-measure over the state-of-the-art on n-ary and chemical-protein relation datasets, suggesting BERT-GT is a robust approach that is applicable to other biomedical relation extraction tasks or datasets.\n\n\nAVAILABILITY AND IMPLEMENTATION\nthe source code of BERT-GT will be made freely available at https://github.com/ncbi-nlp/bert_gt upon publication."}, {"role": "assistant", "content": "Expertise rating: 4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Unsupervised Question Decomposition for Question Answering\nAbstract: We aim to improve question answering (QA) by decomposing hard questions into simpler sub-questions that existing QA systems are capable of answering. Since labeling questions with decompositions is cumbersome, we take an unsupervised approach to produce sub-questions, also enabling us to leverage millions of questions from the internet. Specifically, we propose an algorithm for One-to-N Unsupervised Sequence transduction (ONUS) that learns to map one hard, multi-hop question to many simpler, single-hop sub-questions. We answer sub-questions with an off-the-shelf QA model and give the resulting answers to a recomposition model that combines them into a final answer. We show large QA improvements on HotpotQA over a strong baseline on the original, out-of-domain, and multi-hop dev sets. ONUS automatically learns to decompose different kinds of questions, while matching the utility of supervised and heuristic decomposition methods for QA and exceeding those methods in fluency. Qualitatively, we find that using sub-questions is promising for shedding light on why a QA system makes a prediction."}, {"role": "assistant", "content": "Expertise rating: 3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Against Interpretability: a Critical Examination of the Interpretability Problem in Machine Learning\nAbstract: The usefulness of machine learning algorithms has led to their widespread adoption prior to the development of a conceptual framework for making sense of them. One common response to this situation is to say that machine learning suffers from a \u201cblack box problem.\u201d That is, machine learning algorithms are \u201copaque\u201d to human users, failing to be \u201cinterpretable\u201d or \u201cexplicable\u201d in terms that would render categorization procedures \u201cunderstandable.\u201d The purpose of this paper is to challenge the widespread agreement about the existence and importance of a black box problem. The first section argues that \u201cinterpretability\u201d and cognates lack precise meanings when applied to algorithms. This makes the concepts difficult to use when trying to solve the problems that have motivated the call for interpretability (etc.). Furthermore, since there is no adequate account of the concepts themselves, it is not possible to assess whether particular technical features supply formal definitions of those concepts. The second section argues that there are ways of being a responsible user of these algorithms that do not require interpretability (etc.). In many cases in which a black box problem is cited, interpretability is a means to a further end such as justification or non-discrimination. Since addressing these problems need not involve something that looks like an \u201cinterpretation\u201d (etc.) of an algorithm, the focus on interpretability artificially constrains the solution space by characterizing one possible solution as the problem itself. Where possible, discussion should be reformulated in terms of the ends of interpretability."}, {"role": "assistant", "content": "Expertise rating: 2.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Fast and Efficient MMD-based Fair PCA via Optimization over Stiefel Manifold\nAbstract: This paper defines fair principal component analysis (PCA) as minimizing the maximum mean discrepancy (MMD) between dimensionality-reduced conditional distributions of different protected classes. The incorporation of MMD naturally leads to an exact and tractable mathematical formulation of fairness with good statistical properties. We formulate the problem of fair PCA subject to MMD constraints as a non-convex optimization over the Stiefel manifold and solve it using the Riemannian Exact Penalty Method with Smoothing (REPMS; Liu and Boumal, 2019). Importantly, we provide local optimality guarantees and explicitly show the theoretical effect of each hyperparameter in practical settings, extending previous results. Experimental comparisons based on synthetic and UCI datasets show that our approach outperforms prior work in explained variance, fairness, and runtime."}, {"role": "assistant", "content": "Expertise rating: 3.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Global Convergence and Variance-Reduced Optimization for a Class of Nonconvex-Nonconcave Minimax Problems\nAbstract: Nonconvex minimax problems appear frequently in emerging machine learning applications, such as generative adversarial networks and adversarial learning. Simple algorithms such as the gradient descent ascent (GDA) are the common practice for solving these nonconvex games and receive lots of empirical success. Yet, it is known that these vanilla GDA algorithms with constant step size can potentially diverge even in the convex setting. In this work, we show that for a subclass of nonconvex-nonconcave objectives satisfying a so-called two-sided Polyak-\u0141ojasiewicz inequality, the alternating gradient descent ascent (AGDA) algorithm converges globally at a linear rate and the stochastic AGDA achieves a sublinear rate. We further develop a variance reduced algorithm that attains a provably faster rate than AGDA when the problem has the finite-sum structure."}, {"role": "assistant", "content": "Expertise rating: 4.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Finding Regions of Heterogeneity in Decision-Making via Expected Conditional Covariance\nAbstract: Individuals often make different decisions when faced with the same context, due to personal preferences and background. For instance, judges may vary in their leniency towards certain drug-related offenses, and doctors may vary in their preference for how to start treatment for certain types of patients. With these examples in mind, we present an algorithm for identifying types of contexts (e.g., types of cases or patients) with high inter-decision-maker disagreement. We formalize this as a causal inference problem, seeking a region where the assignment of decision-maker has a large causal effect on the decision. Our algorithm finds such a region by maximizing an empirical objective, and we give a generalization bound for its performance. In a semi-synthetic experiment, we show that our algorithm recovers the correct region of heterogeneity accurately compared to baselines. Finally, we apply our algorithm to real-world healthcare datasets, recovering variation that aligns with existing clinical knowledge."}, {"role": "assistant", "content": "Expertise rating: 4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Towards Zero-Label Language Learning\nAbstract: This paper explores zero-label learning in Natural Language Processing (NLP), whereby no human-annotated data is used anywhere during training and models are trained purely on synthetic data. At the core of our framework is a novel approach for better leveraging the powerful pretrained language models. Specifically, inspired by the recent success of fewshot inference on GPT-3, we present a training data creation procedure named Unsupervised Data Generation (UDG), which leverages fewshot prompts to synthesize high-quality training data without real human annotations. Our method enables zero-label learning as we train task-specific models solely on the synthetic data, yet we achieve better or comparable results from strong baseline models trained on human-labeled data. Furthermore, when mixed with labeled data, our approach serves as a highly effective data augmentation procedure, achieving new state-of-the-art results on the SuperGLUE benchmark1."}, {"role": "assistant", "content": "Expertise rating: 2.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Frustratingly Simple Few-Shot Object Detection\nAbstract: Detecting rare objects from a few examples is an emerging problem. Prior works show meta-learning is a promising approach. But, fine-tuning techniques have drawn scant attention. We find that fine-tuning only the last layer of existing detectors on rare classes is crucial to the few-shot object detection task. Such a simple approach outperforms the meta-learning methods by roughly 2~20 points on current benchmarks and sometimes even doubles the accuracy of the prior methods. However, the high variance in the few samples often leads to the unreliability of existing benchmarks. We revise the evaluation protocols by sampling multiple groups of training examples to obtain stable comparisons and build new benchmarks based on three datasets: PASCAL VOC, COCO and LVIS. Again, our fine-tuning approach establishes a new state of the art on the revised benchmarks. The code as well as the pretrained models are available at this https URL."}, {"role": "assistant", "content": "Expertise rating: 3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Charagram: Embedding Words and Sentences via Character n-grams\nAbstract: We present Charagram embeddings, a simple approach for learning character-based compositional models to embed textual sequences. A word or sentence is represented using a character n-gram count vector, followed by a single nonlinear transformation to yield a low-dimensional embedding. We use three tasks for evaluation: word similarity, sentence similarity, and part-of-speech tagging. We demonstrate that Charagram embeddings outperform more complex architectures based on character-level recurrent and convolutional neural networks, achieving new state-of-the-art performance on several similarity tasks."}, {"role": "assistant", "content": "Expertise rating: 3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Neural Controlled Differential Equations for Irregular Time Series\nAbstract: Neural ordinary differential equations are an attractive option for modelling temporal dynamics. However, a fundamental issue is that the solution to an ordinary differential equation is determined by its initial condition, and there is no mechanism for adjusting the trajectory based on subsequent observations. Here, we demonstrate how this may be resolved through the well-understood mathematics of \\emph{controlled differential equations}. The resulting \\emph{neural controlled differential equation} model is directly applicable to the general setting of partially-observed irregularly-sampled multivariate time series, and (unlike previous work on this problem) it may utilise memory-efficient adjoint-based backpropagation even across observations. We demonstrate that our model achieves state-of-the-art performance against similar (ODE or RNN based) models in empirical studies on a range of datasets. Finally we provide theoretical results demonstrating universal approximation, and that our model subsumes alternative ODE models."}, {"role": "assistant", "content": "Expertise rating: 5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: RecipeQA: A Challenge Dataset for Multimodal Comprehension of Cooking Recipes\nAbstract: Understanding and reasoning about cooking recipes is a fruitful research direction towards enabling machines to interpret procedural text. In this work, we introduce RecipeQA, a dataset for multimodal comprehension of cooking recipes. It comprises of approximately 20K instructional recipes with multiple modalities such as titles, descriptions and aligned set of images. With over 36K automatically generated question-answer pairs, we design a set of comprehension and reasoning tasks that require joint understanding of images and text, capturing the temporal flow of events and making sense of procedural knowledge. Our preliminary results indicate that RecipeQA will serve as a challenging test bed and an ideal benchmark for evaluating machine comprehension systems. The data and leaderboard are available at http://hucvl.github.io/recipeqa."}, {"role": "assistant", "content": "Expertise rating: 2.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Knowledge Enhanced Multi-modal Fake News Detection\nAbstract: Recent years have witnessed the significant damage caused by various types of fake news. Although considerable effort has been applied to address this issue and much progress has been made on detecting fake news, most existing approaches mainly rely on the textual content and/or social context, while knowledge-level information\u2014entities extracted from the news content and the relations between them\u2014is much less explored. Within the limited work on knowledge-based fake news detection, an external knowledge graph is often required, which may introduce additional problems: it is quite common for entities and relations, especially with respect to new concepts, to be missing in existing knowledge graphs, and both entity prediction and link prediction are open research questions themselves. Therefore, in this work, we investigate knowledge-based fake news detection that does not require any external knowledge graph. Specifically, our contributions include: (1) transforming the problem of detecting fake news into a subgraph classification task\u2014entities and relations are extracted from each news item to form a single knowledge graph, where a news item is represented by a subgraph. Then a graph neural network (GNN) model is trained to classify each subgraph/news item. (2) Further improving the performance of this model through a simple but effective multi-modal technique that combines extracted knowledge, textual content and social context. Experiments on multiple datasets with thousands of labelled news items demonstrate that our knowledge-based algorithm outperforms existing counterpart methods, and its performance can be further boosted by the multi-modal approach."}, {"role": "assistant", "content": "Expertise rating: 4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Generating Wikipedia by Summarizing Long Sequences\nAbstract: We show that generating English Wikipedia articles can be approached as a multi- document summarization of source documents. We use extractive summarization to coarsely identify salient information and a neural abstractive model to generate the article. For the abstractive model, we introduce a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoder- decoder architectures used in sequence transduction. We show that this model can generate fluent, coherent multi-sentence paragraphs and even whole Wikipedia articles. When given reference documents, we show it can extract relevant factual information as reflected in perplexity, ROUGE scores and human evaluations."}, {"role": "assistant", "content": "Expertise rating: 4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: A Universal Music Translation Network\nAbstract: We present a method for translating music across musical instruments, genres, and styles. This method is based on a multi-domain wavenet autoencoder, with a shared encoder and a disentangled latent space that is trained end-to-end on waveforms. Employing a diverse training dataset and large net capacity, the domain-independent encoder allows us to translate even from musical domains that were not seen during training. The method is unsupervised and does not rely on supervision in the form of matched samples between domains or musical transcriptions. We evaluate our method on NSynth, as well as on a dataset collected from professional musicians, and achieve convincing translations, even when translating from whistling, potentially enabling the creation of instrumental music by untrained humans."}, {"role": "assistant", "content": "Expertise rating: 3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Interpretable Entity Representations through Large-Scale Typing\nAbstract: In standard methodology for natural language processing, entities in text are typically embedded in dense vector spaces with pre-trained models. The embeddings produced this way are effective when fed into downstream models, but they require end-task fine-tuning and are fundamentally difficult to interpret. In this paper, we present an approach to creating entity representations that are human readable and achieve high performance on entity-related tasks out of the box. Our representations are vectors whose values correspond to posterior probabilities over fine-grained entity types, indicating the confidence of a typing model\u2019s decision that the entity belongs to the corresponding type. We obtain these representations using a fine-grained entity typing model, trained either on supervised ultra-fine entity typing data (Choi et al. 2018) or distantly-supervised examples from Wikipedia. On entity probing tasks involving recognizing entity identity, our embeddings used in parameter-free downstream models achieve competitive performance with ELMo- and BERT-based embeddings in trained models. We also show that it is possible to reduce the size of our type set in a learning-based way for particular domains. Finally, we show that these embeddings can be post-hoc modified through a small number of rules to incorporate domain knowledge and improve performance."}, {"role": "assistant", "content": "Expertise rating: 4.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Compression, Transduction, and Creation: A Unified Framework for Evaluating Natural Language Generation\nAbstract: Natural language generation (NLG) spans a broad range of tasks, each of which serves for specific objectives and desires different properties of generated text. The complexity makes automatic evaluation of NLG particularly challenging. Previous work has typically focused on a single task and developed individual evaluation metrics based on specific intuitions. In this paper, we propose a unifying perspective based on the nature of information change in NLG tasks, including compression (e.g., summarization), transduction (e.g., text rewriting), and creation (e.g., dialog). _Information alignment_ between input, context, and output text plays a common central role in characterizing the generation. With automatic alignment prediction models, we develop a family of interpretable metrics that are suitable for evaluating key aspects of different NLG tasks, often without need of gold reference data. Experiments show the uniformly designed metrics achieve stronger or comparable correlations with human judgement compared to state-of-the-art metrics in each of diverse tasks, including text summarization, style transfer, and knowledge-grounded dialog."}, {"role": "assistant", "content": "Expertise rating: 2.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: An Efficient Algorithm for Easy-First Non-Directional Dependency Parsing\nAbstract: We present a novel deterministic dependency parsing algorithm that attempts to create the easiest arcs in the dependency structure first in a non-directional manner. Traditional deterministic parsing algorithms are based on a shift-reduce framework: they traverse the sentence from left-to-right and, at each step, perform one of a possible set of actions, until a complete tree is built. A drawback of this approach is that it is extremely local: while decisions can be based on complex structures on the left, they can look only at a few words to the right. In contrast, our algorithm builds a dependency tree by iteratively selecting the best pair of neighbours to connect at each parsing step. This allows incorporation of features from already built structures both to the left and to the right of the attachment point. The parser learns both the attachment preferences and the order in which they should be performed. The result is a deterministic, best-first, O(nlogn) parser, which is significantly more accurate than best-first transition based parsers, and nears the performance of globally optimized parsing models."}, {"role": "assistant", "content": "Expertise rating: 4.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Counterfactual Invariance to Spurious Correlations: Why and How to Pass Stress Tests\nAbstract: Informally, a \u2018spurious correlation\u2019 is the dependence of a model on some aspect of the input data that an analyst thinks shouldn\u2019t matter. In machine learning, these have a know-it-when-you-see-it character; e.g., changing the gender of a sentence\u2019s subject changes a sentiment predictor\u2019s output. To check for spurious correlations, we can \u2018stress test\u2019 models by perturbing irrelevant parts of input data and seeing if model predictions change. In this paper, we study stress testing using the tools of causal inference. We introduce counterfactual invariance as a formalization of the requirement that changing irrelevant parts of the input shouldn\u2019t change model predictions. We connect counterfactual invariance to outof-domain model performance, and provide practical schemes for learning (approximately) counterfactual invariant predictors (without access to counterfactual examples). It turns out that both the means and implications of counterfactual invariance depend fundamentally on the true underlying causal structure of the data\u2014in particular, whether the label causes the features or the features cause the label. Distinct causal structures require distinct regularization schemes to induce counterfactual invariance. Similarly, counterfactual invariance implies different domain shift guarantees depending on the underlying causal structure. This theory is supported by empirical results on text classification."}, {"role": "assistant", "content": "Expertise rating: 2.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: On a Utilitarian Approach to Privacy Preserving Text Generation\nAbstract: Differentially-private mechanisms for text generation typically add carefully calibrated noise to input words and use the nearest neighbor to the noised input as the output word. When the noise is small in magnitude, these mechanisms are susceptible to reconstruction of the original sensitive text. This is because the nearest neighbor to the noised input is likely to be the original input. To mitigate this empirical privacy risk, we propose a novel class of differentially private mechanisms that parameterizes the nearest neighbor selection criterion in traditional mechanisms. Motivated by Vickrey auction, where only the second highest price is revealed and the highest price is kept private, we balance the choice between the first and the second nearest neighbors in the proposed class of mechanisms using a tuning parameter. This parameter is selected by empirically solving a constrained optimization problem for maximizing utility, while maintaining the desired privacy guarantees. We argue that this empirical measurement framework can be used to align different mechanisms along a common benchmark for their privacy-utility tradeoff, particularly when different distance metrics are used to calibrate the amount of noise added. Our experiments on real text classification datasets show up to 50% improvement in utility compared to the existing state-of-the-art with the same empirical privacy guarantee."}, {"role": "assistant", "content": "Expertise rating: 4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Erasure Coding in Windows Azure Storage\nAbstract: Windows Azure Storage (WAS) is a cloud storage system that provides customers the ability to store seemingly limitless amounts of data for any duration of time. WAS customers have access to their data from anywhere, at any time, and only pay for what they use and store. To provide durability for that data and to keep the cost of storage low, WAS uses erasure coding. \n \nIn this paper we introduce a new set of codes for erasure coding called Local Reconstruction Codes (LRC). LRC reduces the number of erasure coding fragments that need to be read when reconstructing data fragments that are offline, while still keeping the storage overhead low. The important benefits of LRC are that it reduces the bandwidth and I/Os required for repair reads over prior codes, while still allowing a significant reduction in storage overhead. We describe how LRC is used in WAS to provide low overhead durable storage with consistently low read latencies."}, {"role": "assistant", "content": "Expertise rating: 5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Argument Mining Driven Analysis of Peer-Reviews\nAbstract: Peer reviewing is a central process in modern research and essential for ensuring high quality and reliability of published work. At the same time, it is a time-consuming process and increasing interest in emerging fields often results in a high review workload, especially for senior researchers in this area. How to cope with this problem is an open question and it is vividly discussed across all major conferences. In this work, we propose an Argument Mining based approach for the assistance of editors, meta-reviewers, and reviewers. We demonstrate that the decision process in the field of scientific publications is driven by arguments and automatic argument identification is helpful in various use-cases. One of our findings is that arguments used in the peer-review process differ from arguments in other domains making the transfer of pre-trained models difficult. Therefore, we provide the community with a new peer-review dataset from different computer science conferences with annotated arguments. In our extensive empirical evaluation, we show that Argument Mining can be used to efficiently extract the most relevant parts from reviews, which are paramount for the publication decision. The process remains interpretable since the extracted arguments can be highlighted in a review without detaching them from their context."}, {"role": "assistant", "content": "Expertise rating: 3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Crowdsourced Labeling for Worker-Task Specialization Model\nAbstract: We consider crowdsourced labeling under a $d$-type worker-task specialization model, where each worker and task is associated with one particular type among a finite set of types and a worker provides a more reliable answer to tasks of the matched type than to tasks of unmatched types. We design an inference algorithm that recovers binary task labels (up to any given recovery accuracy) by using worker clustering, worker skill estimation and weighted majority voting. The designed inference algorithm does not require any information about worker/task types, and achieves any targeted recovery accuracy with the best known performance (minimum number of queries per task). 11This work was supported in part by National Research Foundation of Korea under Grant 2017R1E1A1A01076340; in part by the Ministry of Science and ICT, South Korea, under the ITRC support program under Grant IITP-2021-2018-0-01402; and in part by the Institute of Information and Communications Technology Planning & Evaluation (IITP) grant funded by the Korea Government MSIT under Grant 2020-0-00626."}, {"role": "assistant", "content": "Expertise rating: 4.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: AdapterFusion: Non-Destructive Task Composition for Transfer Learning\nAbstract: Sequential fine-tuning and multi-task learning are methods aiming to incorporate knowledge from multiple tasks; however, they suffer from catastrophic forgetting and difficulties in dataset balancing. To address these shortcomings, we propose AdapterFusion, a new two stage learning algorithm that leverages knowledge from multiple tasks. First, in the knowledge extraction stage we learn task specific parameters called adapters, that encapsulate the task-specific information. We then combine the adapters in a separate knowledge composition step. We show that by separating the two stages, i.e., knowledge extraction and knowledge composition, the classifier can effectively exploit the representations learned from multiple tasks in a non-destructive manner. We empirically evaluate AdapterFusion on 16 diverse NLU tasks, and find that it effectively combines various types of knowledge at different layers of the model. We show that our approach outperforms traditional strategies such as full fine-tuning as well as multi-task learning. Our code and adapters are available at AdapterHub.ml."}, {"role": "assistant", "content": "Expertise rating: 4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: A Framework of Severity for Harmful Content Online\nAbstract: The proliferation of harmful content on online social media platforms has necessitated empirical understandings of experiences of harm online and the development of practices for harm mitigation. Both understandings of harm and approaches to mitigating that harm, often through content moderation, have implicitly embedded frameworks of prioritization-what forms of harm should be researched, how policy on harmful content should be implemented, and how harmful content should be moderated. To aid efforts of better understanding the variety of online harms, how they relate to one another, and how to prioritize harms relevant to research, policy, and practice, we present a theoretical framework of severity for harmful online content. By employing a grounded theory approach, we developed a framework of severity based on interviews and card-sorting activities conducted with 52 participants over the course of ten months. Through our analysis, we identified four Types of Harm (physical, emotional, relational, and financial) and eight Dimensions along which the severity of harm can be understood (perspectives, intent, agency, experience, scale, urgency, vulnerability, sphere). We describe how our framework can be applied to both research and policy settings towards deeper understandings of specific forms of harm (e.g., harassment) and prioritization frameworks when implementing policies encompassing many forms of harm."}, {"role": "assistant", "content": "Expertise rating: 2.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: What Neural Networks Memorize and Why: Discovering the Long Tail via Influence Estimation\nAbstract: Deep learning algorithms are well-known to have a propensity for fitting the training data very well and often fit even outliers and mislabeled data points. Such fitting requires memorization of training data labels, a phenomenon that has attracted significant research interest but has not been given a compelling explanation so far. A recent work of Feldman (2019) proposes a theoretical explanation for this phenomenon based on a combination of two insights. First, natural image and data distributions are (informally) known to be long-tailed, that is have a significant fraction of rare and atypical examples. Second, in a simple theoretical model such memorization is necessary for achieving close-to-optimal generalization error when the data distribution is long-tailed. However, no direct empirical evidence for this explanation or even an approach for obtaining such evidence were given. \nIn this work we design experiments to test the key ideas in this theory. The experiments require estimation of the influence of each training example on the accuracy at each test example as well as memorization values of training examples. Estimating these quantities directly is computationally prohibitive but we show that closely-related subsampled influence and memorization values can be estimated much more efficiently. Our experiments demonstrate the significant benefits of memorization for generalization on several standard benchmarks. They also provide quantitative and visually compelling evidence for the theory put forth in (Feldman, 2019)."}, {"role": "assistant", "content": "Expertise rating: 3.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Accelerating Reinforcement Learning with a Directional-Gaussian-Smoothing Evolution Strategy\nAbstract: The objective of reinforcement learning (RL) is to find an optimal strategy for solving a dynamical control problem. Evolution strategy (ES) has been shown great promise in many challenging reinforcement learning (RL) tasks, where the underlying dynamical system is only accessible as a black box such that adjoint methods cannot be used. However, existing ES methods have two limitations that hinder its applicability in RL. First, most existing methods rely on Monte Carlo based gradient estimators to generate search directions. Due to low accuracy of Monte Carlo estimators, the RL training suffers from slow convergence and requires more iterations to reach the optimal solution. Second, the landscape of the reward function can be deceptive and may contain many local maxima, causing ES algorithms to prematurely converge and be unable to explore other parts of the parameter space with potentially greater rewards. In this work, we employ a Directional Gaussian Smoothing Evolutionary Strategy (DGS-ES) to accelerate RL training, which is well-suited to address these two challenges with its ability to (i) provide gradient estimates with high accuracy, and (ii) find nonlocal search direction which lays stress on large-scale variation of the reward function and disregards local fluctuation. Through several benchmark RL tasks demonstrated herein, we show that the DGS-ES method is highly scalable, possesses superior wall-clock time, and achieves competitive reward scores to other popular policy gradient and ES approaches."}, {"role": "assistant", "content": "Expertise rating: 1.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Fastformer: Additive Attention Can Be All You Need\nAbstract: Transformer is a powerful model for text understanding. However, it is inefficient due to its quadratic complexity to input sequence length. Although there are many methods on Transformer acceleration, they are still either inefficient on long sequences or not effective enough. In this paper, we propose Fastformer, which is an efficient Transformer model based on additive attention. In Fastformer, instead of modeling the pair-wise interactions between tokens, we first use additive attention mechanism to model global contexts, and then further transform each token representation based on its interaction with global context representations. In this way, Fastformer can achieve effective context modeling with linear complexity. Extensive experiments on five datasets show that Fastformer is much more efficient than many existing Transformer models and can meanwhile achieve comparable or even better long text modeling performance."}, {"role": "assistant", "content": "Expertise rating: 3.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Mesh-TensorFlow: Deep Learning for Supercomputers\nAbstract: Batch-splitting (data-parallelism) is the dominant distributed Deep Neural Network (DNN) training strategy, due to its universal applicability and its amenability to Single-Program-Multiple-Data (SPMD) programming. However, batch-splitting suffers from problems including the inability to train very large models (due to memory constraints), high latency, and inefficiency at small batch sizes. All of these can be solved by more general distribution strategies (model-parallelism). Unfortunately, efficient model-parallel algorithms tend to be complicated to discover, describe, and to implement, particularly on large clusters. We introduce Mesh-TensorFlow, a language for specifying a general class of distributed tensor computations. Where data-parallelism can be viewed as splitting tensors and operations along the \"batch\" dimension, in Mesh-TensorFlow, the user can specify any tensor-dimensions to be split across any dimensions of a multi-dimensional mesh of processors. A Mesh-TensorFlow graph compiles into a SPMD program consisting of parallel operations coupled with collective communication primitives such as Allreduce. We use Mesh-TensorFlow to implement an efficient data-parallel, model-parallel version of the Transformer sequence-to-sequence model. Using TPU meshes of up to 512 cores, we train Transformer models with up to 5 billion parameters, surpassing state of the art results on WMT'14 English-to-French translation task and the one-billion-word language modeling benchmark. Mesh-Tensorflow is available at this https URL ."}, {"role": "assistant", "content": "Expertise rating: 1.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Unsupervised Multi-hop Question Answering by Question Generation\nAbstract: Obtaining training data for multi-hop question answering (QA) is time-consuming and resource-intensive. We explore the possibility to train a well-performed multi-hop QA model without referencing any human-labeled multi-hop question-answer pairs, i.e., unsupervised multi-hop QA. We propose MQA-QG, an unsupervised framework that can generate human-like multi-hop training data from both homogeneous and heterogeneous data sources. MQA-QG generates questions by first selecting/generating relevant information from each data source and then integrating the multiple information to form a multi-hop question. Using only generated training data, we can train a competent multi-hop QA which achieves 61% and 83% of the supervised learning performance for the HybridQA and the HotpotQA dataset, respectively. We also show that pretraining the QA system with the generated data would greatly reduce the demand for human-annotated training data. Our codes are publicly available at https://github.com/teacherpeterpan/Unsupervised-Multi-hop-QA."}, {"role": "assistant", "content": "Expertise rating: 4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Characterizing Fairness Over the Set of Good Models Under Selective Labels\nAbstract: Algorithmic risk assessments are used to inform decisions in a wide variety of high-stakes settings. Often multiple predictive models deliver similar overall performance but differ markedly in their predictions for individual cases, an empirical phenomenon known as the \u201cRashomon Effect.\u201d These models may have different properties over various groups, and therefore have different predictive fairness properties. We develop a framework for characterizing predictive fairness properties over the set of models that deliver similar overall performance, or \u201cthe set of good models.\u201d Our framework addresses the empirically relevant challenge of selectively labelled data in the setting where the selection decision and outcome are unconfounded given the observed data features. Our framework can be used to 1) audit for predictive bias; or 2) replace an existing model with one that has better fairness properties. We illustrate these use cases on a recidivism prediction task and a real-world credit-scoring task."}, {"role": "assistant", "content": "Expertise rating: 3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Which Training Methods for GANs do actually Converge?\nAbstract: Recent work has shown local convergence of GAN training for absolutely continuous data and generator distributions. In this paper, we show that the requirement of absolute continuity is necessary: we describe a simple yet prototypical counterexample showing that in the more realistic case of distributions that are not absolutely continuous, unregularized GAN training is not always convergent. Furthermore, we discuss regularization strategies that were recently proposed to stabilize GAN training. Our analysis shows that GAN training with instance noise or zero-centered gradient penalties converges. On the other hand, we show that Wasserstein-GANs and WGAN-GP with a finite number of discriminator updates per generator update do not always converge to the equilibrium point. We discuss these results, leading us to a new explanation for the stability problems of GAN training. Based on our analysis, we extend our convergence results to more general GANs and prove local convergence for simplified gradient penalties even if the generator and data distribution lie on lower dimensional manifolds. We find these penalties to work well in practice and use them to learn high-resolution generative image models for a variety of datasets with little hyperparameter tuning."}, {"role": "assistant", "content": "Expertise rating: 4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Tiered Reasoning for Intuitive Physics: Toward Verifiable Commonsense Language Understanding\nAbstract: Large-scale, pre-trained language models (LMs) have achieved human-level performance on a breadth of language understanding tasks. However, evaluations only based on end task performance shed little light on machines\u2019 true ability in language understanding and reasoning. In this paper, we highlight the importance of evaluating the underlying reasoning process in addition to end performance. Toward this goal, we introduce Tiered Reasoning for Intuitive Physics (TRIP), a novel commonsense reasoning dataset with dense annotations that enable multi-tiered evaluation of machines\u2019 reasoning process. Our empirical results show that while large LMs can achieve high end performance, they struggle to support their predictions with valid supporting evidence. The TRIP dataset and our baseline results will motivate verifiable evaluation of commonsense reasoning and facilitate future research toward developing better language understanding and reasoning models."}, {"role": "assistant", "content": "Expertise rating: 1.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: An embarrassingly simple approach to zero-shot learning\nAbstract: Zero-shot learning consists in learning how to recognise new concepts by just having a description of them. Many sophisticated approaches have been proposed to address the challenges this problem comprises. In this paper we describe a zero-shot learning approach that can be implemented in just one line of code, yet it is able to outperform state of the art approaches on standard datasets. The approach is based on a more general framework which models the relationships between features, attributes, and classes as a two linear layers network, where the weights of the top layer are not learned but are given by the environment. We further provide a learning bound on the generalisation error of this kind of approaches, by casting them as domain adaptation methods. In experiments carried out on three standard real datasets, we found that our approach is able to perform significantly better than the state of art on all of them, obtaining a ratio of improvement up to 17%."}, {"role": "assistant", "content": "Expertise rating: 3.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Multi-Input Attention for Unsupervised OCR Correction\nAbstract: We propose a novel approach to OCR post-correction that exploits repeated texts in large corpora both as a source of noisy target outputs for unsupervised training and as a source of evidence when decoding. A sequence-to-sequence model with attention is applied for single-input correction, and a new decoder with multi-input attention averaging is developed to search for consensus among multiple sequences. We design two ways of training the correction model without human annotation, either training to match noisily observed textual variants or bootstrapping from a uniform error model. On two corpora of historical newspapers and books, we show that these unsupervised techniques cut the character and word error rates nearly in half on single inputs and, with the addition of multi-input decoding, can rival supervised methods."}, {"role": "assistant", "content": "Expertise rating: 5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Approximate Gaussian process inference for the drift function in stochastic differential equations\nAbstract: We introduce a nonparametric approach for estimating drift functions in systems of stochastic differential equations from incomplete observations of the state vector. Using a Gaussian process prior over the drift as a function of the state vector, we develop an approximate EM algorithm to deal with the unobserved, latent dynamics between observations. The posterior over states is approximated by a piecewise linearized process and the MAP estimation of the drift is facilitated by a sparse Gaussian process regression."}, {"role": "assistant", "content": "Expertise rating: 5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Modeling Strong and Human-Like Gameplay with KL-Regularized Search\nAbstract: We consider the task of building strong but humanlike policies in multi-agent decision-making problems, given examples of human behavior. Imitation learning is effective at predicting human actions but may not match the strength of expert humans, while self-play learning and search techniques (e.g. AlphaZero) lead to strong performance but may produce policies that are difficult for humans to understand and coordinate with. We show in chess and Go that regularizing search based on the KL divergence from an imitationlearned policy results in higher human prediction accuracy and stronger performance than imitation learning alone. We then introduce a novel regret minimization algorithm that is regularized based on the KL divergence from an imitation-learned policy, and show that using this algorithm for search in no-press Diplomacy yields a policy that matches the human prediction accuracy of imitation learning while being substantially stronger."}, {"role": "assistant", "content": "Expertise rating: 1.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: SCARLET: Explainable Attention based Graph Neural Network for Fake News spreader prediction\nAbstract: False information and true information fact checking it, often co-exist in social networks, each competing to influence people in their spread paths. An efficient strategy here to contain false information is to proactively identify if nodes in the spread path are likely to endorse false information (i.e. further spread it) or refutation information (thereby help contain false information spreading). In this paper, we propose SCARLET (truSt and Credibility bAsed gRaph neuraL nEtwork model using aTtention) to predict likely action of nodes in the spread path. We aggregate trust and credibility features from a node\u2019s neighborhood using historical behavioral data and network structure and explain how features of a spreader\u2019s neighborhood vary. Using real world Twitter datasets, we show that the model is able to predict false information spreaders with an accuracy of over 87%."}, {"role": "assistant", "content": "Expertise rating: 2.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Simple and Effective Multi-Paragraph Reading Comprehension\nAbstract: We consider the problem of adapting neural paragraph-level question answering models to the case where entire documents are given as input. Our proposed solution trains models to produce well calibrated confidence scores for their results on individual paragraphs. We sample multiple paragraphs from the documents during training, and use a shared-normalization training objective that encourages the model to produce globally correct output. We combine this method with a state-of-the-art pipeline for training models on document QA data. Experiments demonstrate strong performance on several document QA datasets. Overall, we are able to achieve a score of 71.3 F1 on the web portion of TriviaQA, a large improvement from the 56.7 F1 of the previous best system."}, {"role": "assistant", "content": "Expertise rating: 2.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Creating a Multitrack Classical Music Performance Dataset for Multimodal Music Analysis: Challenges, Insights, and Applications\nAbstract: We introduce a dataset for facilitating audio-visual analysis of music performances. The dataset comprises 44 simple multi-instrument classical music pieces assembled from coordinated but separately recorded performances of individual tracks. For each piece, we provide the musical score in MIDI format, the audio recordings of the individual tracks, the audio and video recording of the assembled mixture, and ground-truth annotation files including frame-level and note-level transcriptions. We describe our methodology for the creation of the dataset, particularly highlighting our approaches to address the challenges involved in maintaining synchronization and expressiveness. We demonstrate the high quality of synchronization achieved with our proposed approach by comparing the dataset with existing widely used music audio datasets. We anticipate that the dataset will be useful for the development and evaluation of existing music information retrieval (MIR) tasks, as well as for novel multimodal tasks. We benchmark two existing MIR tasks (multipitch analysis and score-informed source separation) on the dataset and compare them with other existing music audio datasets. In addition, we consider two novel multimodal MIR tasks (visually informed multipitch analysis and polyphonic vibrato analysis) enabled by the dataset and provide evaluation measurements and baseline systems for future comparisons (from our recent work). Finally, we propose several emerging research directions that the dataset enables."}, {"role": "assistant", "content": "Expertise rating: 3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Aligning Faithful Interpretations with their Social Attribution\nAbstract: Abstract We find that the requirement of model interpretations to be faithful is vague and incomplete. With interpretation by textual highlights as a case study, we present several failure cases. Borrowing concepts from social science, we identify that the problem is a misalignment between the causal chain of decisions (causal attribution) and the attribution of human behavior to the interpretation (social attribution). We reformulate faithfulness as an accurate attribution of causality to the model, and introduce the concept of aligned faithfulness: faithful causal chains that are aligned with their expected social behavior. The two steps of causal attribution and social attribution together complete the process of explaining behavior. With this formalization, we characterize various failures of misaligned faithful highlight interpretations, and propose an alternative causal chain to remedy the issues. Finally, we implement highlight explanations of the proposed causal format using contrastive explanations."}, {"role": "assistant", "content": "Expertise rating: 4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Revisiting the Primacy of English in Zero-shot Cross-lingual Transfer\nAbstract: Despite their success, large pre-trained multilingual models have not completely alleviated the need for labeled data, which is cumbersome to collect for all target languages. Zero-shot cross-lingual transfer is emerging as a practical solution: pre-trained models later fine-tuned on one transfer language exhibit surprising performance when tested on many target languages. English is the dominant source language for transfer, as reinforced by popular zero-shot benchmarks. However, this default choice has not been systematically vetted. In our study, we compare English against other transfer languages for fine-tuning, on two pretrained multilingual models (mBERT and mT5) and multiple classification and question answering tasks. We find that other high-resource languages such as German and Russian often transfer more effectively, especially when the set of target languages is diverse or unknown a priori. Unexpectedly, this can be true even when the training sets were automatically translated from English. This finding can have immediate impact on multilingual zero-shot systems, and should inform future benchmark designs."}, {"role": "assistant", "content": "Expertise rating: 4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers\nAbstract: Recurrent neural networks (RNNs), temporal convolutions, and neural differential equations (NDEs) are popular families of deep learning models for time-series data, each with unique strengths and tradeoffs in modeling power and computational efficiency. We introduce a simple sequence model inspired by control systems that generalizes these approaches while addressing their shortcomings. The Linear State-Space Layer (LSSL) maps a sequence u 7\u2192 y by simply simulating a linear continuous-time state-space representation \u1e8b = Ax+ Bu, y = Cx+Du. Theoretically, we show that LSSL models are closely related to the three aforementioned families of models and inherit their strengths. For example, they generalize convolutions to continuous-time, explain common RNN heuristics, and share features of NDEs such as time-scale adaptation. We then incorporate and generalize recent theory on continuous-time memorization to introduce a trainable subset of structured matrices A that endow LSSLs with long-range memory. Empirically, stacking LSSL layers into a simple deep neural network obtains state-of-the-art results across time series benchmarks for long dependencies in sequential image classification, real-world healthcare regression tasks, and speech. On a difficult speech classification task with length-16000 sequences, LSSL outperforms prior approaches by 24 accuracy points, and even outperforms baselines that use handcrafted features on 100x shorter sequences."}, {"role": "assistant", "content": "Expertise rating: 1.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Escape saddle points by a simple gradient-descent based algorithm\nAbstract: Escaping saddle points is a central research topic in nonconvex optimization. In this paper, we propose a simple gradient-based algorithm such that for a smooth function f : Rn ! R, it outputs an \u270f-approximate second-order stationary point in \u00d5(log n/\u270f1.75) iterations. Compared to the previous state-of-the-art algorithms by Jin et al. with \u00d5(log n/\u270f2) or \u00d5(log n/\u270f1.75) iterations, our algorithm is polynomially better in terms of log n and matches their complexities in terms of 1/\u270f. For the stochastic setting, our algorithm outputs an \u270f-approximate second-order stationary point in \u00d5(log n/\u270f4) iterations. Technically, our main contribution is an idea of implementing a robust Hessian power method using only gradients, which can find negative curvature near saddle points and achieve the polynomial speedup in log n compared to the perturbed gradient descent methods. Finally, we also perform numerical experiments that support our results."}, {"role": "assistant", "content": "Expertise rating: 1.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Using word embeddings to improve the privacy of clinical notes\nAbstract: Abstract Objective In this work, we introduce a privacy technique for anonymizing clinical notes that guarantees all private health information is secured (including sensitive data, such as family history, that are not adequately covered by current techniques). Materials and Methods We employ a new \u201crandom replacement\u201d paradigm (replacing each token in clinical notes with neighboring word vectors from the embedding space) to achieve 100% recall on the removal of sensitive information, unachievable with current \u201csearch-and-secure\u201d paradigms. We demonstrate the utility of this paradigm on multiple corpora in a diverse set of classification tasks. Results We empirically evaluate the effect of our anonymization technique both on upstream and downstream natural language processing tasks to show that our perturbations, while increasing security (ie, achieving 100% recall on any dataset), do not greatly impact the results of end-to-end machine learning approaches. Discussion As long as current approaches utilize precision and recall to evaluate deidentification algorithms, there will remain a risk of overlooking sensitive information. Inspired by differential privacy, we sought to make it statistically infeasible to recreate the original data, although at the cost of readability. We hope that the work will serve as a catalyst to further research into alternative deidentification methods that can address current weaknesses. Conclusion Our proposed technique can secure clinical texts at a low cost and extremely high recall with a readability trade-off while remaining useful for natural language processing classification tasks. We hope that our work can be used by risk-averse data holders to release clinical texts to researchers."}, {"role": "assistant", "content": "Expertise rating: 4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Reporting an Experience on Design and Implementation of e-Health Systems on Azure Cloud\nAbstract: Electronic Health e-Health technology has brought the world with significant transformation from traditional paper-based medical practice to Information and Communication Technologies ICT-based systems for automatic management storage, processing, and archiving of information. Traditionally e-Health systems have been designed to operate within stovepipes on dedicated networks, physical computers, and locally managed software platforms that make it susceptible to many serious limitations including: 1 lack of on-demand scalability during critical situations; 2 high administrative overheads and costs; and 3 in-efficient resource utilization and energy consumption due to lack of automation. In this paper, we present an approach to migrate the ICT systems in the e-Health sector from traditional in-house Client/Server C/S architecture to the virtualized cloud computing environment. To this end, we developed two cloud-based e-Health applications Medical Practice Management System and Telemedicine Practice System for demonstrating how cloud services can be leveraged for developing and deploying such applications. The Windows Azure cloud computing platform is selected as an example public cloud platform for our study. We conducted several performance evaluation experiments to understand the Quality Service QoS tradeoffs of our applications under variable workload on Azure."}, {"role": "assistant", "content": "Expertise rating: 1.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Deciding Fast and Slow: The Role of Cognitive Biases in AI-assisted Decision-making\nAbstract: Several strands of research have aimed to bridge the gap between artificial intelligence (AI) and human decision-makers in AI-assisted decision-making, where humans are the consumers of AI model predictions and the ultimate decision-makers in high-stakes applications. However, people's perception and understanding are often distorted by their cognitive biases, such as confirmation bias, anchoring bias, availability bias, to name a few. In this work, we use knowledge from the field of cognitive science to account for cognitive biases in the human-AI collaborative decision-making setting, and mitigate their negative effects on collaborative performance. To this end, we mathematically model cognitive biases and provide a general framework through which researchers and practitioners can understand the interplay between cognitive biases and human-AI accuracy. We then focus specifically on anchoring bias, a bias commonly encountered in human-AI collaboration. We implement a time-based de-anchoring strategy and conduct our first user experiment that validates its effectiveness in human-AI collaborative decision-making. With this result, we design a time allocation strategy for a resource-constrained setting that achieves optimal human-AI collaboration under some assumptions. We, then, conduct a second user experiment which shows that our time allocation strategy with explanation can effectively de-anchor the human and improve collaborative performance when the AI model has low confidence and is incorrect."}, {"role": "assistant", "content": "Expertise rating: 4.25"}]}
