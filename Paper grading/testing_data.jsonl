{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: SLAM: A Unified Encoder for Speech and Language Modeling via Speech-Text Joint Pre-Training\nAbstract: Unsupervised pre-training is now the predominant approach for both text and speech understanding. Self-attention models pre-trained on large amounts of unannotated data have been hugely successful when fine-tuned on downstream tasks from a variety of domains and languages. This paper takes the universality of unsupervised language pre-training one step further, by unifying speech and text pre-training within a single model. We build a single encoder with the BERT objective on unlabeled text together with the w2v-BERT objective on unlabeled speech. To further align our model representations across modalities, we leverage alignment losses, specifically Translation Language Modeling (TLM) and Speech Text Matching (STM) that make use of supervised speech-text recognition data. We demonstrate that incorporating both speech and text data during pre-training can significantly improve downstream quality on CoVoST 2 speech translation, by around 1 BLEU compared to single-modality pre-trained models, while retaining close to SotA performance on LibriSpeech and SpeechStew ASR tasks. On four GLUE tasks and text-normalization, we observe evidence of capacity limitations and interference between the two modalities, leading to degraded performance compared to an equivalent text-only model, while still being competitive with BERT. Through extensive empirical analysis we also demonstrate the importance of the choice of objective function for speech pre-training, and the beneficial effect of adding additional supervised signals on the quality of the learned representations."}, {"role": "assistant", "content": "3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Thinking Like Transformers\nAbstract: What is the computational model behind a Transformer? Where recurrent neural networks have direct parallels in finite state machines, allowing clear discussion and thought around architecture variants or trained models, Transformers have no such familiar parallel. In this paper we aim to change that, proposing a computational model for the transformer-encoder in the form of a programming language. We map the basic components of a transformer-encoder\u2014attention and feed-forward computation\u2014into simple primitives, around which we form a programming language: the Restricted Access Sequence Processing Language (RASP). We show how RASP can be used to program solutions to tasks that could conceivably be learned by a Transformer, and how a Transformer can be trained to mimic a RASP solution. In particular, we provide RASP programs for histograms, sorting, and Dyck-languages. We further use our model to relate their difficulty in terms of the number of required layers and attention heads: analyzing a RASP program implies a maximum number of heads and layers necessary to encode a task in a transformer. Finally, we see how insights gained from our abstraction might be used to explain phenomena seen in recent works."}, {"role": "assistant", "content": "3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation\nAbstract: Pre-trained models for Natural Languages (NL) like BERT and GPT have been recently shown to transfer well to Programming Languages (PL) and largely benefit a broad set of code-related tasks. Despite their success, most current methods either rely on an encoder-only (or decoder-only) pre-training that is suboptimal for generation (resp. understanding) tasks or process the code snippet in the same way as NL, neglecting the special characteristics of PL such as token types. We present CodeT5, a unified pre-trained encoder-decoder Transformer model that better leverages the code semantics conveyed from the developer-assigned identifiers. Our model employs a unified framework to seamlessly support both code understanding and generation tasks and allows for multi-task learning. Besides, we propose a novel identifier-aware pre-training task that enables the model to distinguish which code tokens are identifiers and to recover them when they are masked. Furthermore, we propose to exploit the user-written code comments with a bimodal dual generation task for better NL-PL alignment. Comprehensive experiments show that CodeT5 significantly outperforms prior methods on understanding tasks such as code defect detection and clone detection, and generation tasks across various directions including PL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better capture semantic information from code. Our code and pre-trained models are released at https://github.com/salesforce/CodeT5."}, {"role": "assistant", "content": "4.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Consistent Accelerated Inference via Confident Adaptive Transformers\nAbstract: We develop a novel approach for confidently accelerating inference in the large and expensive multilayer Transformers that are now ubiquitous in natural language processing (NLP). Amortized or approximate computational methods increase efficiency, but can come with unpredictable performance costs. In this work, we present CATs \u2013 Confident Adaptive Transformers \u2013 in which we simultaneously increase computational efficiency, while guaranteeing a specifiable degree of consistency with the original model with high confidence. Our method trains additional prediction heads on top of intermediate layers, and dynamically decides when to stop allocating computational effort to each input using a meta consistency classifier. To calibrate our early prediction stopping rule, we formulate a unique extension of conformal prediction. We demonstrate the effectiveness of this approach on four classification and regression tasks."}, {"role": "assistant", "content": "2.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Data Protection in AI Services\nAbstract: Advances in artificial intelligence (AI) have shaped today\u2019s user services, enabling enhanced personalization and better support. As such AI-based services inevitably require user data, the resulting privacy implications are de facto the unacceptable face of this technology. In this article, we categorize and survey the cutting-edge research on privacy and data protection in the context of personalized AI services. We further review the different protection approaches at three different levels, namely, the management, system, and AI levels\u2014showing that (i) not all of them meet our identified requirements of evolving AI services and that (ii) many challenges are addressed separately or fragmentarily by different research communities. Finally, we highlight open research challenges and future directions in data protection research, especially that comprehensive protection requires more interdisciplinary research and a combination of approaches at different levels."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Don't let Ricci v. DeStefano Hold You Back: A Bias-Aware Legal Solution to the Hiring Paradox\nAbstract: Companies that try to address inequality in employment face a hiring paradox. Failing to address workforce imbalance can result in legal sanctions and scrutiny, but proactive measures to address these issues might result in the same legal conflict. Recent run-ins of Microsoft and Wells Fargo with the Labor Department\u2019s Office of Federal Contract Compliance Programs (OFCCP) are not isolated and are likely to persist. To add to the confusion, existing scholarship on Ricci v. DeStefano often deems solutions to this paradox impossible. Circumventive practices such as the 4/5ths rule further illustrate tensions between too little action and too"}, {"role": "assistant", "content": "2.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Piggyback: Adding Multiple Tasks to a Single, Fixed Network by Learning to Mask\nAbstract: This work presents a method for adding multiple tasks to a single, fixed deep neural network without affecting performance on already learned tasks. By building upon concepts from network quantization and sparsification, we learn binary masks that \"piggyback\", or are applied to an existing network to provide good performance on a new task. These masks are learned in an end-to-end differentiable fashion, and incur a low overhead of 1 bit per network parameter, per task. Even though the underlying network is fixed, the ability to mask certain weights allows for the learning of a large number of filters. We show improved performance on a variety of classification tasks, including those with large domain shifts from the natural images of ImageNet. Unlike prior work, we can augment the capabilities of a network without suffering from catastrophic forgetting or competition between tasks, while incurring the least overhead per added task. We demonstrate the applicability of our method to multiple architectures, and obtain accuracies comparable with individual networks trained per task."}, {"role": "assistant", "content": "1.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Simple Questions Generate Named Entity Recognition Datasets\nAbstract: Recent named entity recognition (NER) models 001 often rely on human-annotated datasets requir- 002 ing the vast engagement of professional knowl- 003 edge on the target domain and entities. This 004 work introduces an ask-to-generate approach, 005 which automatically generates NER datasets 006 by asking simple natural language questions 007 to an open-domain question answering system 008 (e.g., \u201c Which disease?\u201d ). Without relying on in- 009 domain resources such as in-domain sentences 010 and labels, our models solely trained on the gen- 011 erated datasets largely outperform strong low- 012 resource models on six popular NER bench- 013 marks by 20.8 F1 score. Our models also show 014 competitive performance with rich-resource 015 models that additionally leverage in-domain 016 dictionaries provided by domain experts. In 017 few-shot NER, we outperform the previous best 018 model by 5.2 F1 score on three benchmarks and 019 achieve new state-of-the-art performance. 1 020"}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Uncovering Latent Biases in Text: Method and Application to Peer Review\nAbstract: Quantifying systematic disparities in numerical quantities such as employment rates and wages between population subgroups provides compelling evidence for the existence of societal biases. However, biases in the text written for members of different subgroups (such as in recommendation letters for male and non-male candidates), though widely reported anecdotally, remain challenging to quantify. In this work, we introduce a novel framework to quantify bias in text caused by the visibility of subgroup membership indicators. We develop a nonparametric estimation and inference procedure to estimate this bias. We then formalize an identification strategy to causally link the estimated bias to the visibility of subgroup membership indicators, provided observations from time periods both before and after an identity-hiding policy change. We identify an application wherein \"ground truth\" bias can be inferred to evaluate our framework, instead of relying on synthetic or secondary data. Specifically, we apply our framework to quantify biases in the text of peer reviews from a reputed machine learning conference before and after the conference adopted a double-blind reviewing policy. We show evidence of biases in the review ratings that serves as \"ground truth\", and show that our proposed framework accurately detects these biases from the review text without having access to the review ratings."}, {"role": "assistant", "content": "4.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Deep Learning Based Resource Assignment for Wireless Networks\nAbstract: This letter studies a deep learning approach for binary assignment problems in wireless networks, which identifies binary variables for permutation matrices. This poses challenges in designing a structure of a neural network and its training strategies for generating feasible assignment solutions. To this end, this letter develop a new Sinkhorn neural network which learns a non-convex projection task onto a set of permutation matrices. An unsupervised training algorithm is proposed where the Sinkhorn neural network can be applied to network assignment problems. Numerical results demonstrate the effectiveness of the proposed method in various network scenarios."}, {"role": "assistant", "content": "1.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Compacter: Efficient Low-Rank Hypercomplex Adapter Layers\nAbstract: Adapting large-scale pretrained language models to downstream tasks via fine-tuning is the standard method for achieving state-of-the-art performance on NLP benchmarks. However, fine-tuning all weights of models with millions or billions of parameters is sample-inefficient, unstable in low-resource settings, and wasteful as it requires storing a separate copy of the model for each task. Recent work has developed parameter-efficient fine-tuning methods, but these approaches either still require a relatively large number of parameters or underperform standard fine-tuning. In this work, we propose COMPACTER, a method for fine-tuning large-scale language models with a better trade-off between task performance and the number of trainable parameters than prior work. COMPACTER accomplishes this by building on top of ideas from adapters, low-rank optimization, and parameterized hypercomplex multiplication layers. Specifically, COMPACTER inserts task-specific weight matrices into a pretrained model\u2019s weights, which are computed efficiently as a sum of Kronecker products between shared \u201cslow\u201d weights and \u201cfast\u201d rank-one matrices defined per COMPACTER layer. By only training 0.047% of a pretrained model\u2019s parameters, COMPACTER performs on par with standard fine-tuning on GLUE and outperforms standard fine-tuning on SuperGLUE and low-resource settings. Our code is publicly available at https://github.com/rabeehk/compacter."}, {"role": "assistant", "content": "4.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Everyday Algorithm Auditing: Understanding the Power of Everyday Users in Surfacing Harmful Algorithmic Behaviors\nAbstract: A growing body of literature has proposed formal approaches to audit algorithmic systems for biased and harmful behaviors. While formal auditing approaches have been greatly impactful, they often suffer major blindspots, with critical issues surfacing only in the context of everyday use once systems are deployed. Recent years have seen many cases in which everyday users of algorithmic systems detect and raise awareness about harmful behaviors that they encounter in the course of their everyday interactions with these systems. However, to date little academic attention has been granted to these bottom-up, user-driven auditing processes. In this paper, we propose and explore the concept of everyday algorithm auditing, a process in which users detect, understand, and interrogate problematic machine behaviors via their day-to-day interactions with algorithmic systems. We argue that everyday users are powerful in surfacing problematic machine behaviors that may elude detection via more centrally-organized forms of auditing, regardless of users' knowledge about the underlying algorithms. We analyze several real-world cases of everyday algorithm auditing, drawing lessons from these cases for the design of future platforms and tools that facilitate such auditing behaviors. Finally, we discuss work that lies ahead, toward bridging the gaps between formal auditing approaches and the organic auditing behaviors that emerge in everyday use of algorithmic systems."}, {"role": "assistant", "content": "4.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Evolution Strategies for Approximate Solution of Bayesian Games\nAbstract: We address the problem of solving complex Bayesian games, characterized by high-dimensional type and action spaces, many (> 2) players, and general-sum payoffs. Our approach applies to symmetric one-shot Bayesian games, with no given analytic structure. We represent agent strategies in parametric form as neural networks, and apply natural evolution strategies (NES) (Wierstra et al. 2014) for deep model optimization. For pure equilibrium computation, we formulate the problem as bi-level optimization, and employ NES in an iterative algorithm to implement both inner-loop best response optimization and outer-loop regret minimization. In simple games including firstand second-price auctions, it is capable of recovering known analytic solutions. For mixed equilibrium computation, we adopt an incremental strategy generation framework, with NES as strategy generator producing a finite sequence of approximate best-response strategies. We then calculate equilibria over this finite strategy set via a model-based optimization process. Both our pure and mixed equilibrium computation methods employ NES to efficiently search for strategies over the function space, given only black-box simulation access to noisy payoff samples. We experimentally demonstrate the efficacy of all methods on two simultaneous sealed-bid auction games with distinct type distributions, and observe that the solutions exhibit qualitatively different behavior in these two environments."}, {"role": "assistant", "content": "2.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Balancing Accuracy and Fairness for Interactive Recommendation with Reinforcement Learning\nAbstract: Fairness in recommendation has attracted increasing attention due to bias and discrimination possibly caused by traditional recommenders. In Interactive Recommender Systems (IRS), user preferences and the system\u2019s fairness status are constantly changing over time. Existing fairness-aware recommenders mainly consider fairness in static settings. Directly applying existing methods to IRS will result in poor recommendation. To resolve this problem, we propose a reinforcement learning based framework, FairRec, to dynamically maintain a long-term balance between accuracy and fairness in IRS. User preferences and the system\u2019s fairness status are jointly compressed into the state representation to generate recommendations. FairRec aims at maximizing our designed cumulative reward that combines accuracy and fairness. Extensive experiments validate that FairRec can improve fairness, while preserving good recommendation quality."}, {"role": "assistant", "content": "1.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Adaptive Gradient Methods Converge Faster with Over-Parameterization (and you can do a line-search)\nAbstract: As adaptive gradient methods are typically used for training over-parameterized models capable of exactly fitting the data, we study their convergence in this interpolation setting. Under this assumption, we prove that constant step-size, zero-momentum variants of Adam and AMSGrad can converge to the minimizer at the O(1/T) rate for smooth, convex functions. When this assumption is only approximately satisfied, we show that these methods converge to a neighbourhood of the solution. On the other hand, we show that Adagrad is robust to the violation of interpolation and converges to the minimizer at the optimal rate. However, we demonstrate that even for simple, convex problems satisfying interpolation, the empirical performance of these methods heavily depends on the step-size and requires tuning. We alleviate this problem by making use of stochastic line-search methods (SLS) and Polyak's step-sizes (SPS) to help these methods adapt to the function's local smoothness. We prove that adaptive methods used in conjunction with these techniques do not require knowledge of problem-dependent constants and retain the convergence guarantees of their constant step-size counterparts. Experimentally, we show that using SLS or SPS consistently improves the convergence of adaptive methods across tasks; from binary classification with kernel mappings to classification with deep neural networks. Furthermore, our empirical results show that Adagrad equipped with SLS generalizes better than SGD."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Implementation Matters in Deep Policy Gradients: A Case Study on PPO and TRPO\nAbstract: We study the roots of algorithmic progress in deep policy gradient algorithms through a case study on two popular algorithms: Proximal Policy Optimization (PPO) and Trust Region Policy Optimization (TRPO). Specifically, we investigate the consequences of \"code-level optimizations:\" algorithm augmentations found only in implementations or described as auxiliary details to the core algorithm. Seemingly of secondary importance, such optimizations turn out to have a major impact on agent behavior. Our results show that they (a) are responsible for most of PPO's gain in cumulative reward over TRPO, and (b) fundamentally change how RL methods function. These insights show the difficulty and importance of attributing performance gains in deep reinforcement learning. Code for reproducing our results is available at this https URL ."}, {"role": "assistant", "content": "4.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Microsoft Speaker Diarization System for the Voxceleb Speaker Recognition Challenge 2020\nAbstract: This paper describes the Microsoft speaker diarization system for monaural multi-talker recordings in the wild, evaluated at the diarization track of the VoxCeleb Speaker Recognition Challenge (VoxSRC) 2020. We will first explain our system design to address issues in handling real multi-talker recordings. We then present the details of the components, which include Res2Net-based speaker embedding extractor, conformer-based continuous speech separation with leakage filtering, and a modified DOVER (short for Diarization Output Voting Error Reduction) method for system fusion. We evaluate the systems with the data set provided by VoxSRC challenge 2020, which contains real-life multi-talker audio collected from YouTube. Our best system achieves 3.71% and 6.23% of the diarization error rate (DER) on development set and evaluation set, respectively, being ranked the 1st at the diarization track of the challenge."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: ELLA: Exploration through Learned Language Abstraction\nAbstract: Building agents capable of understanding language instructions is critical to effective and robust human-AI collaboration. Recent work focuses on training these instruction following agents via reinforcement learning in environments with synthetic language; however, these instructions often define long-horizon, sparsereward tasks, and learning policies requires many episodes of experience. To this end, we introduce ELLA: Exploration through Learned Language Abstraction, a reward shaping approach that correlates high-level instructions with simpler low-level instructions to enrich the sparse rewards afforded by the environment. ELLA has two key elements: 1) A termination classifier that identifies when agents complete low-level instructions, and 2) A relevance classifier that correlates low-level instructions with success on high-level tasks. We learn the termination classifier offline from pairs of instructions and terminal states. Notably, in departure from prior work in language and abstraction, we learn the relevance classifier online, without relying on an explicit decomposition of high-level instructions to low-level instructions. On a suite of complex grid world environments with varying instruction complexities and reward sparsity, ELLA shows a significant gain in sample efficiency across several environments compared to competitive language-based reward shaping and no-shaping methods."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Shallow pooling for sparse labels\nAbstract: Recent years have seen enormous gains in core information retrieval tasks, including document and passage ranking. Datasets and leaderboards, and in particular the MS MARCO datasets, illustrate the dramatic improvements achieved by modern neural rankers. When compared with traditional information retrieval test collections, such as those developed by TREC, the MS MARCO datasets employ substantially more queries \u2014 thousands vs. dozens \u2013 with substantially fewer known relevant items per query \u2014 often just one. For example, 94% of the nearly seven thousand queries in the MS MARCO passage ranking development set have only a single known relevant passage, and no query has more than four. Given the sparsity of these relevance labels, the MS MARCO leaderboards track improvements with mean reciprocal rank (MRR). In essence, a relevant item is treated as the \u201cright answer\u201d, with rankers scored on their ability to place this item as high in the ranking as possible. In working with these sparse labels, we have observed that the top items returned by a ranker often appear superior to judged relevant items. Others have reported the same observation. To test this observation, we employed crowdsourced workers to make preference judgments between the top item returned by a modern neural ranking stack and a judged relevant item for the nearly seven thousand queries in the passage ranking development set. The results support our observation. If we imagine a perfect ranker under MRR, with a score of 1 on all queries, our preference judgments indicate that a searcher would prefer the top result from a modern neural ranking stack more frequently than the top result from the imaginary perfect ranker, making our neural ranker \u201cbetter than perfect\u201d. To understand the implications for the leaderboard, we pooled the top document from available runs near the top of the passage ranking leaderboard for over 500 queries. We employed crowdsourced workers to make preference judgments over these pools and re-evaluated the runs. Our results support our concerns that current MS MARCO datasets may no longer be able to recognize genuine improvements in rankers. In future, if rankers are measured against a single \u201cright answer\u201d, this answer should be the best answer or most preferred answer, and maintained with ongoing judgments. Since only the best answer is required, this ongoing maintenance might be performed with shallow pooling. When a previously unjudged document is surfaced as the top item in a ranking, it can directly compared with the previous known best answer. 1 ar X iv :2 10 9. 00 06 2v 1 [ cs .I R ] 3 1 A ug 2 02 1"}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Automatic Rule Generation for Time Expression Normalization\nAbstract: The understanding of time expressions includes two sub-tasks: recognition and normalization. In recent years, significant progress has been made in the recognition of time expressions while research on normalization has lagged behind. Existing SOTA normalization methods highly rely on rules or grammars designed by experts, which limits their performance on emerging corpora, such as social media texts. In this paper, we model time expression normalization as a sequence of operations to construct the normalized temporal value, and we present a novel method called ARTime, which can automatically generate normalization rules from training data without expert interventions. Specifically, ARTime automatically captures possible operation sequences from annotated data and generates normalization rules on time expressions with common surface forms. The experimental results show that ARTime can significantly surpass SOTA methods on the Tweets benchmark, and achieves competitive results with existing expert-engineered rule methods on the TempEval-3 benchmark."}, {"role": "assistant", "content": "1.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: On NMT Search Errors and Model Errors: Cat Got Your Tongue?\nAbstract: We report on search errors and model errors in neural machine translation (NMT). We present an exact inference procedure for neural sequence models based on a combination of beam search and depth-first search. We use our exact search to find the global best model scores under a Transformer base model for the entire WMT15 English-German test set. Surprisingly, beam search fails to find these global best model scores in most cases, even with a very large beam size of 100. For more than 50% of the sentences, the model in fact assigns its global best score to the empty translation, revealing a massive failure of neural models in properly accounting for adequacy. We show by constraining search with a minimum translation length that at the root of the problem of empty translations lies an inherent bias towards shorter translations. We conclude that vanilla NMT in its current form requires just the right amount of beam search errors, which, from a modelling perspective, is a highly unsatisfactory conclusion indeed, as the model often prefers an empty translation."}, {"role": "assistant", "content": "3.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Entity, Relation, and Event Extraction with Contextualized Span Representations\nAbstract: We examine the capabilities of a unified, multi-task framework for three information extraction tasks: named entity recognition, relation extraction, and event extraction. Our framework (called DyGIE++) accomplishes all tasks by enumerating, refining, and scoring text spans designed to capture local (within-sentence) and global (cross-sentence) context. Our framework achieves state-of-the-art results across all tasks, on four datasets from a variety of domains. We perform experiments comparing different techniques to construct span representations. Contextualized embeddings like BERT perform well at capturing relationships among entities in the same or adjacent sentences, while dynamic span graph updates model long-range cross-sentence relationships. For instance, propagating span representations via predicted coreference links can enable the model to disambiguate challenging entity mentions. Our code is publicly available at https://github.com/dwadden/dygiepp and can be easily adapted for new tasks or datasets."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Distantly-Supervised Evidence Retrieval Enables Question Answering without Evidence Annotation\nAbstract: Open-domain question answering answers a question based on evidence retrieved from a large corpus. State-of-the-art neural approaches require intermediate evidence annotations for training. However, such intermediate annotations are expensive, and methods that rely on them cannot transfer to the more common setting, where only question\u2013 answer pairs are available. This paper investigates whether models can learn to find evidence from a large corpus, with only distant supervision from answer labels for model training, thereby generating no additional annotation cost. We introduce a novel approach (DISTDR) that iteratively improves over a weak retriever by alternately finding evidence from the up-to-date model and encouraging the model to learn the most likely evidence. Without using any evidence labels, DISTDR is on par with fully-supervised state-of-theart methods on both multi-hop and singlehop QA benchmarks. Our analysis confirms that DISTDR finds more accurate evidence over iterations, which leads to model improvements. The code is available at https:// github.com/henryzhao5852/DistDR."}, {"role": "assistant", "content": "3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: How can I choose an explainer? An Application-grounded Evaluation of Post-hoc Explanations\nAbstract: There have been several research works proposing new Explainable AI (XAI) methods designed to generate model explanations having specific properties, or desiderata, such as fidelity, robustness, or human-interpretability. However, explanations are seldom evaluated based on their true practical impact on decision-making tasks. Without that assessment, explanations might be chosen that, in fact, hurt the overall performance of the combined system of ML model + end-users. This study aims to bridge this gap by proposing XAI Test, an application-grounded evaluation methodology tailored to isolate the impact of providing the end-user with different levels of information. We conducted an experiment following XAI Test to evaluate three popular post-hoc explanation methods \u2013 LIME, SHAP, and TreeInterpreter \u2013 on a real-world fraud detection task, with real data, a deployed ML model, and fraud analysts. During the experiment, we gradually increased the information provided to the fraud analysts in three stages: Data Only, i.e., just transaction data without access to model score nor explanations, Data + ML Model Score, and Data + ML Model Score + Explanations. Using strong statistical analysis, we show that, in general, these popular explainers have a worse impact than desired. Some of the conclusion highlights include: i) showing Data Only results in the highest decision accuracy and the slowest decision time among all variants tested, ii) all the explainers improve accuracy over the Data + ML Model Score variant but still result in lower accuracy when compared with Data Only; iii) LIME was the least preferred by users, probably due to its substantially lower variability of explanations from case to"}, {"role": "assistant", "content": "4.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning\nAbstract: Although pretrained language models can be fine-tuned to produce state-of-the-art results for a very wide range of language understanding tasks, the dynamics of this process are not well understood, especially in the low data regime. Why can we use relatively vanilla gradient descent algorithms (e.g., without strong regularization) to tune a model with hundreds of millions of parameters on datasets with only hundreds or thousands of labeled examples? In this paper, we argue that analyzing fine-tuning through the lens of intrinsic dimension provides us with empirical and theoretical intuitions to explain this remarkable phenomenon. We empirically show that common pre-trained models have a very low intrinsic dimension; in other words, there exists a low dimension reparameterization that is as effective for fine-tuning as the full parameter space. For example, by optimizing only 200 trainable parameters randomly projected back into the full space, we can tune a RoBERTa model to achieve 90% of the full parameter performance levels on MRPC. Furthermore, we empirically show that pre-training implicitly minimizes intrinsic dimension and, perhaps surprisingly, larger models tend to have lower intrinsic dimension after a fixed number of pre-training updates, at least in part explaining their extreme effectiveness. Lastly, we connect intrinsic dimensionality with low dimensional task representations and compression based generalization bounds to provide intrinsic-dimension-based generalization bounds that are independent of the full parameter count."}, {"role": "assistant", "content": "2.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Lower Bounds on the Total Variation Distance Between Mixtures of Two Gaussians\nAbstract: Mixtures of high dimensional Gaussian distributions have been studied extensively in statistics and learning theory. While the total variation distance appears naturally in the sample complexity of distribution learning, it is analytically difficult to obtain tight lower bounds for mixtures. Exploiting a connection between total variation distance and the characteristic function of the mixture, we provide fairly tight functional approximations. This enables us to derive new lower bounds on the total variation distance between two-component Gaussian mixtures with a shared covariance matrix."}, {"role": "assistant", "content": "2.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: On the Pareto Frontier of Regret Minimization and Best Arm Identification in Stochastic Bandits\nAbstract: We study the Pareto frontier of two archetypal objectives in stochastic bandits, namely, regret minimization (RM) and best arm identification (BAI) with a fixed horizon. It is folklore that the balance between exploitation and exploration is crucial for both RM and BAI, but exploration is more critical in achieving the optimal performance for the latter objective. To make this precise, we first design and analyze the BOBW-LIL\u2019UCB(\u03b3) algorithm, which achieves order-wise optimal performance for RM or BAI under different values of \u03b3. Complementarily, we show that no algorithm can simultaneously perform optimally for both the RM and BAI objectives. More precisely, we establish non-trivial lower bounds on the regret achievable by any algorithm with a given BAI failure probability. This analysis shows that in some regimes BOBW-LIL\u2019UCB(\u03b3) achieves Pareto-optimality up to constant or small terms. Numerical experiments further demonstrate that when applied to difficult instances, BOBW-LIL\u2019UCB outperforms a close competitor UCB\u03b1 (Degenne et al., 2019), which is designed for RM and BAI with a fixed confidence."}, {"role": "assistant", "content": "4.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Control Synthesis for High-Dimensional Systems With Counting Constraints\nAbstract: General purpose correct-by-construction synthesis methods are limited to systems with low dimensionality or simple specifications. In this work we consider highly symmetrical counting problems and exploit the symmetry to synthesize provably correct controllers for systems with tens of thousands of states. The key ingredients of the solution are an aggregate abstraction procedure for mildly heterogeneous systems and subsequent formulation of counting constraints as linear inequalities."}, {"role": "assistant", "content": "3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Rethinking Spatial Dimensions of Vision Transformers\nAbstract: Vision Transformer (ViT) extends the application range of transformers from language processing to computer vision tasks as being an alternative architecture against the existing convolutional neural networks (CNN). Since the transformer-based architecture has been innovative for computer vision modeling, the design convention towards an effective architecture has been less studied yet. From the successful design principles of CNN, we investigate the role of spatial dimension conversion and its effectiveness on transformer-based architecture. We particularly attend to the dimension reduction principle of CNNs; as the depth increases, a conventional CNN increases channel dimension and decreases spatial dimensions. We empirically show that such a spatial dimension reduction is beneficial to a transformer architecture as well, and propose a novel Pooling-based Vision Transformer (PiT) upon the original ViT model. We show that PiT achieves the improved model capability and generalization performance against ViT. Throughout the extensive experiments, we further show PiT outperforms the baseline on several tasks such as image classification, object detection, and robustness evaluation. Source codes and ImageNet models are available at https://github.com/naver-ai/pit."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Quantifying Voter Biases in Online Platforms\nAbstract: In content-based online platforms, use of aggregate user feedback (say, the sum of votes) is commonplace as the \"gold standard\" for measuring content quality. Use of vote aggregates, however, is at odds with the existing empirical literature, which suggests that voters are susceptible to different biases-reputation (e.g., of the poster), social influence (e.g., votes thus far), and position (e.g., answer position). Our goal is to quantify, in an observational setting, the degree of these biases in online platforms. Specifically, what are the causal effects of different impression signals-such as the reputation of the contributing user, aggregate vote thus far, and position of content-on a participant's vote on content? We adopt an instrumental variable (IV) framework to answer this question. We identify a set of candidate instruments, carefully analyze their validity, and then use the valid instruments to reveal the effects of the impression signals on votes. Our empirical study using log data from Stack Exchange websites shows that the bias estimates from our IV approach differ from the bias estimates from the ordinary least squares (OLS) method. In particular, OLS underestimates reputation bias (1.6-2.2x for gold badges) and position bias (up to 1.9x for the initial position) and overestimates social influence bias (1.8-2.3x for initial votes). The implications of our work include: redesigning user interface to avoid voter biases; making changes to platforms' policy to mitigate voter biases; detecting other forms of biases in online platforms."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Reliable Causal Discovery with Improved Exact Search and Weaker Assumptions\nAbstract: Many of the causal discovery methods rely on the faithfulness assumption to guarantee asymptotic correctness. However, the assumption can be approximately violated in many ways, leading to sub-optimal solutions. Although there is a line of research in Bayesian network structure learning that focuses on weakening the assumption, such as exact search methods with well-defined score functions, they do not scale well to large graphs. In this work, we introduce several strategies to improve the scalability of exact score-based methods in the linear Gaussian setting. In particular, we develop a super-structure estimation method based on the support of inverse covariance matrix which requires assumptions that are strictly weaker than faithfulness, and apply it to restrict the search space of exact search. We also propose a local search strategy that performs exact search on the local clusters formed by each variable and its neighbors within two hops in the superstructure. Numerical experiments validate the efficacy of the proposed procedure, and demonstrate that it scales up to hundreds of nodes with a high accuracy."}, {"role": "assistant", "content": "3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers\nAbstract: Readers of academic research papers often read with the goal of answering specific questions. Question Answering systems that can answer those questions can make consumption of the content much more efficient. However, building such tools requires data that reflect the difficulty of the task arising from complex reasoning about claims made in multiple parts of a paper. In contrast, existing information-seeking question answering datasets usually contain questions about generic factoid-type information. We therefore present Qasper, a dataset of 5049 questions over 1585 Natural Language Processing papers. Each question is written by an NLP practitioner who read only the title and abstract of the corresponding paper, and the question seeks information present in the full text. The questions are then answered by a separate set of NLP practitioners who also provide supporting evidence to answers. We find that existing models that do well on other QA tasks do not perform well on answering these questions, underperforming humans by at least 27 F1 points when answering them from entire papers, motivating further research in document-grounded, information-seeking QA, which our dataset is designed to facilitate."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Contrastive Learning for Unpaired Image-to-Image Translation\nAbstract: In image-to-image translation, each patch in the output should reflect the content of the corresponding patch in the input, independent of domain. We propose a straightforward method for doing so -- maximizing mutual information between the two, using a framework based on contrastive learning. The method encourages two elements (corresponding patches) to map to a similar point in a learned feature space, relative to other elements (other patches) in the dataset, referred to as negatives. We explore several critical design choices for making contrastive learning effective in the image synthesis setting. Notably, we use a multilayer, patch-based approach, rather than operate on entire images. Furthermore, we draw negatives from within the input image itself, rather than from the rest of the dataset. We demonstrate that our framework enables one-sided translation in the unpaired image-to-image translation setting, while improving quality and reducing training time. In addition, our method can even be extended to the training setting where each \"domain\" is only a single image."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Multi-GCN: Graph Convolutional Networks for Multi-View Networks, with Applications to Global Poverty\nAbstract: With the rapid expansion of mobile phone networks in developing countries, large-scale graph machine learning has gained sudden relevance in the study of global poverty. Recent applications range from humanitarian response and poverty estimation to urban planning and epidemic containment. Yet the vast majority of computational tools and algorithms used in these applications do not account for the multi-view nature of social networks: people are related in myriad ways, but most graph learning models treat relations as binary. In this paper, we develop a graph-based convolutional network for learning on multi-view networks. We show that this method outperforms state-of-the-art semi-supervised learning algorithms on three different prediction tasks using mobile phone datasets from three different developing countries. We also show that, while designed specifically for use in poverty research, the algorithm also outperforms existing benchmarks on a broader set of learning tasks on multi-view networks, including node labelling in citation networks."}, {"role": "assistant", "content": "3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Change Point Detection in Time Series Data Using Autoencoders With a Time-Invariant Representation\nAbstract: Change point detection (CPD) aims to locate abrupt property changes in time series data. Recent CPD methods demonstrated the potential of using deep learning techniques, but often lack the ability to identify more subtle changes in the autocorrelation statistics of the signal and suffer from a high false alarm rate. To address these issues, we employ an autoencoder-based methodology with a novel loss function, through which the used autoencoders learn a partially time-invariant representation that is tailored for CPD. The result is a flexible method that allows the user to indicate whether change points should be sought in the time domain, frequency domain or both. Detectable change points include abrupt changes in the slope, mean, variance, autocorrelation function and frequency spectrum. We demonstrate that our proposed method is consistently highly competitive or superior to baseline methods on diverse simulated and real-life benchmark data sets. Finally, we mitigate the issue of false detection alarms through the use of a postprocessing procedure that combines a matched filter and a newly proposed change point score. We show that this combination drastically improves the performance of our method as well as all baseline methods."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Entity Linking in 100 Languages\nAbstract: We propose a new formulation for multilingual entity linking, where language-specific mentions resolve to a language-agnostic Knowledge Base. We train a dual encoder in this new setting, building on prior work with improved feature representation, negative mining, and an auxiliary entity-pairing task, to obtain a single entity retrieval model that covers 100+ languages and 20 million entities. The model outperforms state-of-the-art results from a far more limited cross-lingual linking task. Rare entities and low-resource languages pose challenges at this large-scale, so we advocate for an increased focus on zero- and few-shot evaluation. To this end, we provide Mewsli-9, a large new multilingual dataset (this http URL) matched to our setting, and show how frequency-based analysis provided key insights for our model and training enhancements."}, {"role": "assistant", "content": "4.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Neural Ordinary Differential Equations\nAbstract: We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: MindCraft: Theory of Mind Modeling for Situated Dialogue in Collaborative Tasks\nAbstract: An ideal integration of autonomous agents in a human world implies that they are able to collaborate on human terms. In particular, theory of mind plays an important role in maintaining common ground during human collaboration and communication. To enable theory of mind modeling in situated interactions, we introduce a fine-grained dataset of collaborative tasks performed by pairs of human subjects in the 3D virtual blocks world of Minecraft. It provides information that captures partners\u2019 beliefs of the world and of each other as an interaction unfolds, bringing abundant opportunities to study human collaborative behaviors in situated language communication. As a first step towards our goal of developing embodied AI agents able to infer belief states of collaborative partners in situ, we build and present results on computational models for several theory of mind tasks."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Telling Stories through Multi-User Dialogue by Modeling Character Relations\nAbstract: This paper explores character-driven story continuation, in which the story emerges through characters\u2019 first- and second-person narration as well as dialogue\u2014requiring models to select language that is consistent with a character\u2019s persona and their relationships with other characters while following and advancing the story. We hypothesize that a multi-task model that trains on character dialogue plus character relationship information improves transformer-based story continuation. To this end, we extend the Critical Role Dungeons and Dragons Dataset (Rameshkumar and Bailey, 2020)\u2014consisting of dialogue transcripts of people collaboratively telling a story while playing the role-playing game Dungeons and Dragons\u2014with automatically extracted relationships between each pair of interacting characters as well as their personas. A series of ablations lend evidence to our hypothesis, showing that our multi-task model using character relationships improves story continuation accuracy over strong baselines."}, {"role": "assistant", "content": "2.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: On the Complementarity of Data Selection and Fine Tuning for Domain Adaptation\nAbstract: Domain adaptation of neural networks commonly relies on three training phases: pretraining, selected data training and then fine tuning. Data selection improves target domain generalization by training further on pretraining data identified by relying on a small sample of target domain data. This work examines the benefit of data selection for language modeling and machine translation. Our experiments assess the complementarity of selection with fine tuning and result in practical recommendations: (i) selected data must be similar to the fine-tuning domain but not so much as to erode the complementary effect of fine-tuning; (ii) there is a trade-off between selecting little data for fast but limited progress or much data for slow but long lasting progress; (iii) data selection can be applied early during pretraining, with performance gains comparable to long pretraining session; (iv) data selection from domain classifiers is often more effective than the popular contrastive data selection method."}, {"role": "assistant", "content": "3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer\nAbstract: The recent \u201cText-to-Text Transfer Transformer\u201d (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent \u201caccidental translation\u201d in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available."}, {"role": "assistant", "content": "2.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Vision-based Detection of Acoustic Timed Events: a Case Study on Clarinet Note Onsets\nAbstract: Acoustic events often have a visual counterpart. Knowledge of visual information can aid the understanding of complex auditory scenes, even when only a stereo mixdown is available in the audio domain, \\eg identifying which musicians are playing in large musical ensembles. In this paper, we consider a vision-based approach to note onset detection. As a case study we focus on challenging, real-world clarinetist videos and carry out preliminary experiments on a 3D convolutional neural network based on multiple streams and purposely avoiding temporal pooling. We release an audiovisual dataset with 4.5 hours of clarinetist videos together with cleaned annotations which include about 36,000 onsets and the coordinates for a number of salient points and regions of interest. By performing several training trials on our dataset, we learned that the problem is challenging. We found that the CNN model is highly sensitive to the optimization algorithm and hyper-parameters, and that treating the problem as binary classification may prevent the joint optimization of precision and recall. To encourage further research, we publicly share our dataset, annotations and all models and detail which issues we came across during our preliminary experiments."}, {"role": "assistant", "content": "3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: proScript: Partially Ordered Scripts Generation via Pre-trained Language Models\nAbstract: Scripts standardized event sequences describing typical everyday activities have been shown to help understand narratives by providing expectations, resolving ambiguity, and filling in unstated information. However, to date they have proved hard to author or extract from text. In this work, we demonstrate for the first time that pre-trained neural language models (LMs) can be be finetuned to generate high-quality scripts, at varying levels of granularity, for a wide range of everyday scenarios (e.g., bake a cake). To do this, we collected a large (6.4k), crowdsourced partially ordered scripts (named proScript), which is substantially larger than prior datasets, and developed models that generate scripts with combining language generation and structure prediction. We define two complementary tasks: (i) edge prediction: given a scenario and unordered events, organize the events into a valid (possibly partial-order) script, and (ii) script generation: given only a scenario, generate events and organize them into a (possibly partial-order) script. Our experiments show that our models perform well (e.g., F1=75.7 on task (i)), illustrating a new approach to overcoming previous barriers to script collection. We also show that there is still significant room for improvement toward human level performance. Together, our tasks, dataset, and models offer a new research direction for learning script knowledge."}, {"role": "assistant", "content": "4.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Predictors of L2 word learning accuracy: A big data investigation\nAbstract: What makes some words harder to learn than others in a second language? Although some robust factors have been identified based on small scale experimental studies, many relevant factors are difficult to study in such experiments due to the amount of data necessary to test them. Here, we investigate what factors affect the ease of learning of a word in a second language using a large data set of users learning English as a second language through the Duolingo mobile app. In a regression analysis, we test and confirm the well-studied effect of cognate status on word learning accuracy. Furthermore, we find significant effects for both cross-linguistic semantic alignment and English semantic density, two novel predictors derived from large scale distributional models of lexical semantics. Finally, we provide data on several other psycholinguistically plausible word level predictors. We conclude with a discussion of the limits, benefits and future research potential of using big data for investigating second language learning."}, {"role": "assistant", "content": "3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Searching for More Efficient Dynamic Programs\nAbstract: Computational models of human language often involve combinatorial problems. For instance, a probabilistic parser may marginalize over exponentially many trees to make predictions. Algorithms for such problems often employ dynamic programming and are not always unique. Finding one with optimal asymptotic runtime can be unintuitive, timeconsuming, and error-prone. Our work aims to automate this laborious process. Given an initial correct declarative program, we search for a sequence of semantics-preserving transformations to improve its running time as much as possible. To this end, we describe a set of program transformations, a simple metric for assessing the efficiency of a transformed program, and a heuristic search procedure to improve this metric. We show that in practice, automated search\u2014like the mental search performed by human programmers\u2014can find substantial improvements to the initial program. Empirically, we show that many speed-ups described in the NLP literature could have been discovered automatically by our system."}, {"role": "assistant", "content": "4.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Counterfactual VQA: A Cause-Effect Look at Language Bias\nAbstract: VQA models may tend to rely on language bias as a shortcut and thus fail to sufficiently learn the multi-modal knowledge from both vision and language. Recent debiasing methods proposed to exclude the language prior during inference. However, they fail to disentangle the \"good\" language context and \"bad\" language bias from the whole. In this paper, we investigate how to mitigate language bias in VQA. Motivated by causal effects, we proposed a novel counterfactual inference framework, which enables us to capture the language bias as the direct causal effect of questions on answers and reduce the language bias by subtracting the direct language effect from the total causal effect. Experiments demonstrate that our proposed counterfactual inference framework 1) is general to various VQA backbones and fusion strategies, 2) achieves competitive performance on the language-bias sensitive VQA-CP dataset while performs robustly on the balanced VQA v2 dataset without any augmented data. The code is available at https://github.com/yuleiniu/cfvqa."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Perturbing Inputs for Fragile Interpretations in Deep Natural Language Processing\nAbstract: Interpretability methods like Integrated Gradient and LIME are popular choices for explaining natural language model predictions with relative word importance scores. These interpretations need to be robust for trustworthy NLP applications in high-stake areas like medicine or finance. Our paper demonstrates how interpretations can be manipulated by making simple word perturbations on an input text. Via a small portion of word-level swaps, these adversarial perturbations aim to make the resulting text semantically and spatially similar to its seed input (therefore sharing similar interpretations). Simultaneously, the generated examples achieve the same prediction label as the seed yet are given a substantially different explanation by the interpretation methods. Our experiments generate fragile interpretations to attack two SOTA interpretation methods, across three popular Transformer models and on three different NLP datasets. We observe that the rank order correlation and top-K intersection score drops by over 20% when less than 10% of words are perturbed on average. Further, rank-order correlation keeps decreasing as more words get perturbed. Furthermore, we demonstrate that candidates generated from our method have good quality metrics."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: When Machine Learning Meets Privacy: A Survey and Outlook\nAbstract: The newly emerged machine learning (e.g. deep learning) methods have become a strong driving force to revolutionize a wide range of industries, such as smart healthcare, financial technology, and surveillance systems. Meanwhile, privacy has emerged as a big concern in this machine learning-based artificial intelligence era. It is important to note that the problem of privacy preservation in the context of machine learning is quite different from that in traditional data privacy protection, as machine learning can act as both friend and foe. Currently, the work on the preservation of privacy and machine learning (ML) is still in an infancy stage, as most existing solutions only focus on privacy problems during the machine learning process. Therefore, a comprehensive study on the privacy preservation problems and machine learning is required. This paper surveys the state of the art in privacy issues and solutions for machine learning. The survey covers three categories of interactions between privacy and machine learning: (i) private machine learning, (ii) machine learning aided privacy protection, and (iii) machine learning-based privacy attack and corresponding protection schemes. The current research progress in each category is reviewed and the key challenges are identified. Finally, based on our in-depth analysis of the area of privacy and machine learning, we point out future research directions in this field."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Limitations on the Achievable Repair Bandwidth of Piggybacking Codes with Low Substriping\nAbstract: The piggybacking framework for designing erasure codes for distributed storage has empirically proven to be very useful, and has been used to design codes with desirable properties, such as low repair bandwidth and complexity. However, the theoretical properties of this framework remain largely unexplored. We address this by adapting a general characterization of repair schemes (previously used for Reed Solomon codes) to analyze piggybacking codes with low substriping. With this characterization, we establish a separation between piggybacking and general erasure codes, and several impossibility results for subcategories of piggybacking codes; for certain parameters, we also present explicit, optimal constructions of piggybacking codes."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Linguistic Structures as Weak Supervision for Visual Scene Graph Generation\nAbstract: Prior work in scene graph generation requires categorical supervision at the level of triplets\u2014subjects and objects, and predicates that relate them, either with or without bounding box information. However, scene graph generation is a holistic task: thus holistic, contextual supervision should intuitively improve performance. In this work, we explore how linguistic structures in captions can benefit scene graph generation. Our method captures the information provided in captions about relations between individual triplets, and context for subjects and objects (e.g. visual properties are mentioned). Captions are a weaker type of supervision than triplets since the alignment between the exhaustive list of human-annotated subjects and objects in triplets, and the nouns in captions, is weak. However, given the large and diverse sources of multimodal data on the web (e.g. blog posts with images and captions), linguistic supervision is more scalable than crowdsourced triplets. We show extensive experimental comparisons against prior methods which leverage instance- and image-level supervision, and ablate our method to show the impact of leveraging phrasal and sequential context, and techniques to improve localization of subjects and objects."}, {"role": "assistant", "content": "2.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Mitigating Manipulation in Peer Review via Randomized Reviewer Assignments\nAbstract: We consider three important challenges in conference peer review: (i) reviewers maliciously attempting to get assigned to certain papers to provide positive reviews, possibly as part of quid-pro-quo arrangements with the authors; (ii) \"torpedo reviewing,\" where reviewers deliberately attempt to get assigned to certain papers that they dislike in order to reject them; (iii) reviewer de-anonymization on release of the similarities and the reviewer-assignment code. On the conceptual front, we identify connections between these three problems and present a framework that brings all these challenges under a common umbrella. We then present a (randomized) algorithm for reviewer assignment that can optimally solve the reviewer-assignment problem under any given constraints on the probability of assignment for any reviewer-paper pair. We further consider the problem of restricting the joint probability that certain suspect pairs of reviewers are assigned to certain papers, and show that this problem is NP-hard for arbitrary constraints on these joint probabilities but efficiently solvable for a practical special case. Finally, we experimentally evaluate our algorithms on datasets from past conferences, where we observe that they can limit the chance that any malicious reviewer gets assigned to their desired paper to 50% while producing assignments with over 90% of the total optimal similarity. Our algorithms still achieve this similarity while also preventing reviewers with close associations from being assigned to the same paper."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Efficient Neural Ranking using Forward Indexes\nAbstract: Neural document ranking approaches, specifically transformer models, have achieved impressive gains in ranking performance. However, query processing using such over-parameterized models is both resource and time intensive. In this paper, we propose the Fast-Forward index \u2013 a simple vector forward index that facilitates ranking documents using interpolation of lexical and semantic scores \u2013 as a replacement for contextual re-rankers and dense indexes based on nearest neighbor search. Fast-Forward indexes rely on efficient sparse models for retrieval and merely look up pre-computed dense transformer-based vector representations of documents and passages in constant time for fast CPU-based semantic similarity computation during query processing. We propose index pruning and theoretically grounded early stopping techniques to improve the query processing throughput. We conduct extensive large-scale experiments on TREC-DL datasets and show improvements over hybrid indexes in performance and query processing efficiency using only CPUs. Fast-Forward indexes can provide superior ranking performance using interpolation due to the complementary benefits of lexical and semantic similarities."}, {"role": "assistant", "content": "1.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Learning Optimal Conditional Priors For Disentangled Representations\nAbstract: A large part of the literature on learning disentangled representations focuses on variational autoencoders (VAEs). Recent developments demonstrate that disentanglement cannot be obtained in a fully unsupervised setting without inductive biases on models and data. As such, Khemakhem et al., AISTATS 2020, suggest employing a factorized prior distribution over the latent variables that is conditionally dependent on auxiliary observed variables complementing input observations. While this is a remarkable advancement toward model identifiability, the learned conditional prior only focuses on sufficiency, giving no guarantees on a minimal representation. Motivated by information theoretic principles, we propose a novel VAE-based generative model with theoretical guarantees on disentanglement. Our proposed model learns a sufficient and compact - thus optimal - conditional prior, which serves as regularization for the latent space. Experimental results indicate superior performance with respect to state-of-the-art methods, according to several established metrics proposed in the literature on disentanglement."}, {"role": "assistant", "content": "2.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Towards a critical race methodology in algorithmic fairness\nAbstract: We examine the way race and racial categories are adopted in algorithmic fairness frameworks. Current methodologies fail to adequately account for the socially constructed nature of race, instead adopting a conceptualization of race as a fixed attribute. Treating race as an attribute, rather than a structural, institutional, and relational phenomenon, can serve to minimize the structural aspects of algorithmic unfairness. In this work, we focus on the history of racial categories and turn to critical race theory and sociological work on race and ethnicity to ground conceptualizations of race for fairness research, drawing on lessons from public health, biomedical research, and social survey research. We argue that algorithmic fairness researchers need to take into account the multidimensionality of race, take seriously the processes of conceptualizing and operationalizing race, focus on social processes which produce racial inequality, and consider perspectives of those most affected by sociotechnical systems."}, {"role": "assistant", "content": "2.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Rationales for Sequential Predictions\nAbstract: Sequence models are a critical component of modern NLP systems, but their predictions are difficult to explain. We consider model explanations though rationales, subsets of context that can explain individual model predictions. We find sequential rationales by solving a combinatorial optimization: the best rationale is the smallest subset of input tokens that would predict the same output as the full sequence. Enumerating all subsets is intractable, so we propose an efficient greedy algorithm to approximate this objective. The algorithm, which is called greedy rationalization, applies to any model. For this approach to be effective, the model should form compatible conditional distributions when making predictions on incomplete subsets of the context. This condition can be enforced with a short fine-tuning step. We study greedy rationalization on language modeling and machine translation. Compared to existing baselines, greedy rationalization is best at optimizing the sequential objective and provides the most faithful rationales. On a new dataset of annotated sequential rationales, greedy rationales are most similar to human rationales."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Complexity of Computing Optimal Stackelberg Strategies in Security Resource Allocation Games\nAbstract: Recently, algorithms for computing game-theoretic solutions have been deployed in real-world security applications, such as the placement of checkpoints and canine units at Los Angeles International Airport. These algorithms assume that the defender (security personnel) can commit to a mixed strategy, a so-called Stackelberg model. As pointed out by Kiek-intveld et al. (2009), in these applications, generally, multiple resources need to be assigned to multiple targets, resulting in an exponential number of pure strategies for the defender. In this paper, we study how to compute optimal Stackelberg strategies in such games, showing that this can be done in polynomial time in some cases, and is NP-hard in others."}, {"role": "assistant", "content": "3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Deep Learning Through the Lens of Example Difficulty\nAbstract: Existing work on understanding deep learning often employs measures that compress all data-dependent information into a few numbers. In this work, we adopt a perspective based on the role of individual examples. We introduce a measure of the computational difficulty of making a prediction for a given input: the (effective) prediction depth. Our extensive investigation reveals surprising yet simple relationships between the prediction depth of a given input and the model\u2019s uncertainty, confidence, accuracy and speed of learning for that data point. We further categorize difficult examples into three interpretable groups, demonstrate how these groups are processed differently inside deep models and showcase how this understanding allows us to improve prediction accuracy. Insights from our study lead to a coherent view of a number of separately reported phenomena in the literature: early layers generalize while later layers memorize; early layers converge faster and networks learn easy data and simple functions first."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Convex optimization based on global lower second-order models\nAbstract: In this paper, we present new second-order algorithms for composite convex optimization, called Contracting-domain Newton methods. These algorithms are affine-invariant and based on global second-order lower approximation for the smooth component of the objective. Our approach has an interpretation both as a second-order generalization of the conditional gradient method, or as a variant of trust-region scheme. Under the assumption, that the problem domain is bounded, we prove $\\mathcal{O}(1/k^{2})$ global rate of convergence in functional residual, where $k$ is the iteration counter, minimizing convex functions with Lipschitz continuous Hessian. This significantly improves the previously known bound $\\mathcal{O}(1/k)$ for this type of algorithms. Additionally, we propose a stochastic extension of our method, and present computational results for solving empirical risk minimization problem."}, {"role": "assistant", "content": "3.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Model-based Policy Optimization with Unsupervised Model Adaptation\nAbstract: Model-based reinforcement learning methods learn a dynamics model with real data sampled from the environment and leverage it to generate simulated data to derive an agent. However, due to the potential distribution mismatch between simulated data and real data, this could lead to degraded performance. Despite much effort being devoted to reducing this distribution mismatch, existing methods fail to solve it explicitly. In this paper, we investigate how to bridge the gap between real and simulated data due to inaccurate model estimation for better policy optimization. To begin with, we first derive a lower bound of the expected return, which naturally inspires a bound maximization algorithm by aligning the simulated and real data distributions. To this end, we propose a novel model-based reinforcement learning framework AMPO, which introduces unsupervised model adaptation to minimize the integral probability metric (IPM) between feature distributions from real and simulated data. Instantiating our framework with Wasserstein-1 distance gives a practical model-based approach. Empirically, our approach achieves state-of-the-art performance in terms of sample efficiency on a range of continuous control benchmark tasks."}, {"role": "assistant", "content": "3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: A Density Ratio Approach to Language Model Fusion in End-to-End Automatic Speech Recognition\nAbstract: This article describes a density ratio approach to integrating external Language Models (LMs) into end-to-end models for Automatic Speech Recognition (ASR). Applied to a Recurrent Neural Network Transducer (RNN-T) ASR model trained on a given domain, a matched in-domain RNN-LM, and a target domain RNN-LM, the proposed method uses Bayes' Rule to define RNN-T posteriors for the target domain, in a manner directly analogous to the classic hybrid model for ASR based on Deep Neural Networks (DNNs) or LSTMs in the Hidden Markov Model (HMM) framework (Bourlard & Morgan, 1994). The proposed approach is evaluated in cross-domain and limited-data scenarios, for which a significant amount of target domain text data is used for LM training, but only limited (or no) {audio, transcript} training data pairs are used to train the RNN-T. Specifically, an RNN-T model trained on paired audio & transcript data from YouTube is evaluated for its ability to generalize to Voice Search data. The Density Ratio method was found to consistently outperform the dominant approach to LM and end-to-end ASR integration, Shallow Fusion."}, {"role": "assistant", "content": "4.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Leveraging Language to Learn Program Abstractions and Search Heuristics\nAbstract: Inductive program synthesis, or inferring programs from examples of desired behavior, offers a general paradigm for building interpretable, ro-bust, and generalizable machine learning systems. Effective program synthesis depends on two key ingredients: a strong library of functions from which to build programs, and an ef\ufb01cient search strategy for \ufb01nding programs that solve a given task. We introduce LAPS (Language for Abstraction and Program Search), a technique for using natural language annotations to guide joint learning of libraries and neurally-guided search models for synthesis. When integrated into a state-of-the-art library learning system (DreamCoder), LAPS produces higher-quality libraries and improves search ef\ufb01ciency and generalization on three domains \u2013 string editing, image composition, and abstract reasoning about scenes \u2013 even when no natural language hints are available at test time."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: ByT5: Towards a Token-Free Future with Pre-trained Byte-to-Byte Models\nAbstract: Abstract Most widely used pre-trained language models operate on sequences of tokens corresponding to word or subword units. By comparison, token-free models that operate directly on raw text (bytes or characters) have many benefits: They can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Because byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.1"}, {"role": "assistant", "content": "4.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Rethinking search\nAbstract: When experiencing an information need, users want to engage with a domain expert, but often turn to an information retrieval system, such as a search engine, instead. Classical information retrieval systems do not answer information needs directly, but instead provide references to (hopefully authoritative) answers. Successful question answering systems offer a limited corpus created on-demand by human experts, which is neither timely nor scalable. Pre-trained language models, by contrast, are capable of directly generating prose that may be responsive to an information need, but at present they are dilettantes rather than domain experts - they do not have a true understanding of the world, they are prone to hallucinating, and crucially they are incapable of justifying their utterances by referring to supporting documents in the corpus they were trained over. This paper examines how ideas from classical information retrieval and pre-trained language models can be synthesized and evolved into systems that truly deliver on the promise of domain expert advice."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Evaluating the Morphosyntactic Well-formedness of Generated Texts\nAbstract: Text generation systems are ubiquitous in natural language processing applications. However, evaluation of these systems remains a challenge, especially in multilingual settings. In this paper, we propose L\u2019AMBRE \u2013 a metric to evaluate the morphosyntactic well-formedness of text using its dependency parse and morphosyntactic rules of the language. We present a way to automatically extract various rules governing morphosyntax directly from dependency treebanks. To tackle the noisy outputs from text generation systems, we propose a simple methodology to train robust parsers. We show the effectiveness of our metric on the task of machine translation through a diachronic study of systems translating into morphologically-rich languages."}, {"role": "assistant", "content": "2.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Document-Level N-ary Relation Extraction with Multiscale Representation Learning\nAbstract: Most information extraction methods focus on binary relations expressed within single sentences. In high-value domains, however, n-ary relations are of great demand (e.g., drug-gene-mutation interactions in precision oncology). Such relations often involve entity mentions that are far apart in the document, yet existing work on cross-sentence relation extraction is generally confined to small text spans (e.g., three consecutive sentences), which severely limits recall. In this paper, we propose a novel multiscale neural architecture for document-level n-ary relation extraction. Our system combines representations learned over various text spans throughout the document and across the subrelation hierarchy. Widening the system\u2019s purview to the entire document maximizes potential recall. Moreover, by integrating weak signals across the document, multiscale modeling increases precision, even in the presence of noisy labels from distant supervision. Experiments on biomedical machine reading show that our approach substantially outperforms previous n-ary relation extraction methods."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies\nAbstract: Abstract A key limitation in current datasets for multi-hop reasoning is that the required steps for answering the question are mentioned in it explicitly. In this work, we introduce StrategyQA, a question answering (QA) benchmark where the required reasoning steps are implicit in the question, and should be inferred using a strategy. A fundamental challenge in this setup is how to elicit such creative questions from crowdsourcing workers, while covering a broad range of potential strategies. We propose a data collection procedure that combines term-based priming to inspire annotators, careful control over the annotator population, and adversarial filtering for eliminating reasoning shortcuts. Moreover, we annotate each question with (1) a decomposition into reasoning steps for answering it, and (2) Wikipedia paragraphs that contain the answers to each step. Overall, StrategyQA includes 2,780 examples, each consisting of a strategy question, its decomposition, and evidence paragraphs. Analysis shows that questions in StrategyQA are short, topic-diverse, and cover a wide range of strategies. Empirically, we show that humans perform well (87%) on this task, while our best baseline reaches an accuracy of \u223c 66%."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Group-Aware Threshold Adaptation for Fair Classification\nAbstract: The fairness in machine learning is getting increasing attention, as its applications in different fields continue to expand and diversify. To mitigate the discriminated model behaviors between different demographic groups, we introduce a novel post-processing method to optimize over multiple fairness constraints through group-aware threshold adaptation. We propose to learn adaptive classification thresholds for each demographic group by optimizing the confusion matrix estimated from the probability distribution of a classification model output. As we only need an estimated probability distribution of model output instead of the classification model structure, our post-processing model can be applied to a wide range of classification models and improve fairness in a model-agnostic manner and ensure privacy. This even allows us to post-process existing fairness methods to further improve the trade-off between accuracy and fairness. Moreover, our model has low computational cost. We provide rigorous theoretical analysis on the convergence of our optimization algorithm and the trade-off between accuracy and fairness of our method. Our method theoretically enables a better upper bound in near optimality than existing method under same condition. Experimental results demonstrate that our method outperforms state-of-the-art methods and obtains the result that is closest to the theoretical accuracy-fairness trade-off boundary."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Nonconvex-Nonconcave Min-Max Optimization with a Small Maximization Domain\nAbstract: We study the problem of finding approximate first-order stationary points in optimization problems of the form minx\u2208X maxy\u2208Y f(x, y), where the sets X,Y are convex and Y is compact. The objective function f is smooth, but assumed neither convex in x nor concave in y. Our approach relies upon replacing the function f(x, \u22c5) with its k-th order Taylor approximation (in y) and finding a near-stationary point in the resulting surrogate problem. To guarantee its success, we establish the following result: let the Euclidean diameter of Y be small in terms of the target accuracy \u03b5, namely O(\u03b5 2 k+1 ) for k \u2208 N and O(\u03b5) for k = 0, with the constant factors controlled by certain regularity parameters of f ; then any \u03b5-stationary point in the surrogate problem remains O(\u03b5)-stationary for the initial problem. Moreover, we show that these upper bounds are nearly optimal: the aforementioned reduction provably fails when the diameter of Y is larger. For 0 \u2a7d k \u2a7d 2 the surrogate function can be efficiently maximized in y; our general approximation result then leads to efficient algorithms for finding a near-stationary point in nonconvex-nonconcave min-max problems, for which we also provide convergence guarantees."}, {"role": "assistant", "content": "3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Control of Mobile Robots Using Barrier Functions Under Temporal Logic Specifications\nAbstract: In this article, we propose a framework for the control of mobile robots subject to temporal logic specifications using barrier functions. Complex task specifications can be conveniently encoded using linear temporal logic. In particular, we consider a fragment of linear temporal logic, which encompasses a large class of motion planning specifications for a robotic system. Control barrier functions have recently emerged as a convenient tool to guarantee reachability and safety for a system. In addition, they can be encoded as affine constraints in a quadratic program. In this article, a fully automatic framework that translates a user defined specification in temporal logic to a sequence of barrier function based quadratic programs is presented. In addition, with the aim of alleviating infeasibility scenarios, we propose methods for composition of barrier functions as well as a prioritization-based control method to guarantee feasibility of the controller. We prove that the resulting system trajectory synthesized by the proposed controller satisfies the given specification. Robotic simulation and experimental results are provided in addition to the theoretical framework."}, {"role": "assistant", "content": "3.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Efficient Content-Based Sparse Attention with Routing Transformers\nAbstract: Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1"}, {"role": "assistant", "content": "4.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: What Gets Echoed? Understanding the \u201cPointers\u201d in Explanations of Persuasive Arguments\nAbstract: Explanations are central to everyday life, and are a topic of growing interest in the AI community. To investigate the process of providing natural language explanations, we leverage the dynamics of the /r/ChangeMyView subreddit to build a dataset with 36K naturally occurring explanations of why an argument is persuasive. We propose a novel word-level prediction task to investigate how explanations selectively reuse, or echo, information from what is being explained (henceforth, explanandum). We develop features to capture the properties of a word in the explanandum, and show that our proposed features not only have relatively strong predictive power on the echoing of a word in an explanation, but also enhance neural methods of generating explanations. In particular, while the non-contextual properties of a word itself are more valuable for stopwords, the interaction between the constituent parts of an explanandum is crucial in predicting the echoing of content words. We also find intriguing patterns of a word being echoed. For example, although nouns are generally less likely to be echoed, subjects and objects can, depending on their source, be more likely to be echoed in the explanations."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Solving parametric PDE problems with artificial neural networks\nAbstract: The curse of dimensionality is commonly encountered in numerical partial differential equations (PDE), especially when uncertainties have to be modelled into the equations as random coefficients. However, very often the variability of physical quantities derived from PDE can be captured by a few features on the space of the coefficient fields. Based on such observation, we propose using neural network to parameterise the physical quantity of interest as a function of input coefficients. The representability of such quantity using a neural network can be justified by viewing the neural network as performing time evolution to find the solutions to the PDE. We further demonstrate the simplicity and accuracy of the approach through notable examples of PDEs in engineering and physics."}, {"role": "assistant", "content": "3.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Asking It All: Generating Contextualized Questions for any Semantic Role\nAbstract: Asking questions about a situation is an inherent step towards understanding it. To this end, we introduce the task of role question generation, which, given a predicate mention and a passage, requires producing a set of questions asking about all possible semantic roles of the predicate. We develop a two-stage model for this task, which first produces a context-independent question prototype for each role and then revises it to be contextually appropriate for the passage. Unlike most existing approaches to question generation, our approach does not require conditioning on existing answers in the text. Instead, we condition on the type of information to inquire about, regardless of whether the answer appears explicitly in the text, could be inferred from it, or should be sought elsewhere. Our evaluation demonstrates that we generate diverse and well-formed questions for a large, broad-coverage ontology of predicates and roles."}, {"role": "assistant", "content": "4.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Synchromesh: Reliable code generation from pre-trained language models\nAbstract: Large pre-trained language models have been used to generate code, providing a flexible interface for synthesizing programs from natural language specifications. However, they often violate syntactic and semantic rules of their output language, limiting their practical usability. In this paper, we propose SYNCHROMESH: a framework for substantially improving the reliability of pre-trained models for code generation. SYNCHROMESH comprises two components. First, it retrieves few-shot examples from a training bank using Target Similarity Tuning (TST), a novel method for semantic example selection. TST learns to recognize utterances that describe similar target programs despite differences in surface natural language features. Then, SYNCHROMESH feeds the examples to a pre-trained language model and samples programs using Constrained Semantic Decoding (CSD): a general framework for constraining the output to a set of valid programs in the target language. CSD leverages constraints on partial outputs to sample complete correct programs, and needs neither re-training nor fine-tuning of the language model. We evaluate our methods by synthesizing code from natural language descriptions using GPT-3 and Codex in three real-world languages: SQL queries, Vega-Lite visualizations and SMCalFlow programs. These domains showcase rich constraints that CSD is able to enforce, including syntax, scope, typing rules, and contextual logic. We observe substantial complementary gains from CSD and TST in prediction accuracy and in effectively preventing run-time errors."}, {"role": "assistant", "content": "3.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Few-shot Image Generation with Elastic Weight Consolidation\nAbstract: Few-shot image generation seeks to generate more data of a given domain, with only few available training examples. As it is unreasonable to expect to fully infer the distribution from just a few observations (e.g., emojis), we seek to leverage a large, related source domain as pretraining (e.g., human faces). Thus, we wish to preserve the diversity of the source domain, while adapting to the appearance of the target. We adapt a pretrained model, without introducing any additional parameters, to the few examples of the target domain. Crucially, we regularize the changes of the weights during this adaptation, in order to best preserve the information of the source dataset, while fitting the target. We demonstrate the effectiveness of our algorithm by generating high-quality results of different target domains, including those with extremely few examples (e.g., <10). We also analyze the performance of our method with respect to some important factors, such as the number of examples and the dissimilarity between the source and target domain."}, {"role": "assistant", "content": "3.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Contrastive Multi-View Representation Learning on Graphs\nAbstract: We introduce a self-supervised approach for learning node and graph level representations by contrasting structural views of graphs. We show that unlike visual representation learning, increasing the number of views to more than two or contrasting multi-scale encodings do not improve performance, and the best performance is achieved by contrasting encodings from first-order neighbors and a graph diffusion. We achieve new state-of-the-art results in self-supervised learning on 8 out of 8 node and graph classification benchmarks under the linear evaluation protocol. For example, on Cora (node) and Reddit-Binary (graph) classification benchmarks, we achieve 86.8% and 84.5% accuracy, which are 5.5% and 2.4% relative improvements over previous state-of-the-art. When compared to supervised baselines, our approach outperforms them in 4 out of 8 benchmarks. Source code is released at: this https URL"}, {"role": "assistant", "content": "3.25"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Revisiting Self-Training for Neural Sequence Generation\nAbstract: Self-training is one of the earliest and simplest semi-supervised methods. The key idea is to augment the original labeled dataset with unlabeled data paired with the model's prediction (i.e. the pseudo-parallel data). While self-training has been extensively studied on classification problems, in complex sequence generation tasks (e.g. machine translation) it is still unclear how self-training works due to the compositionality of the target space. In this work, we first empirically show that self-training is able to decently improve the supervised baseline on neural sequence generation tasks. Through careful examination of the performance gains, we find that the perturbation on the hidden states (i.e. dropout) is critical for self-training to benefit from the pseudo-parallel data, which acts as a regularizer and forces the model to yield close predictions for similar unlabeled inputs. Such effect helps the model correct some incorrect predictions on unlabeled data. To further encourage this mechanism, we propose to inject noise to the input space, resulting in a \"noisy\" version of self-training. Empirical study on standard machine translation and text summarization benchmarks shows that noisy self-training is able to effectively utilize unlabeled data and improve the performance of the supervised baseline by a large margin."}, {"role": "assistant", "content": "2.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Learning Differential Equations that are Easy to Solve\nAbstract: Differential equations parameterized by neural networks become expensive to solve numerically as training progresses. We propose a remedy that encourages learned dynamics to be easier to solve. Specifically, we introduce a differentiable surrogate for the time cost of standard numerical solvers, using higher-order derivatives of solution trajectories. These derivatives are efficient to compute with Taylor-mode automatic differentiation. Optimizing this additional objective trades model performance against the time cost of solving the learned dynamics. We demonstrate our approach by training substantially faster, while nearly as accurate, models in supervised classification, density estimation, and time-series modelling tasks."}, {"role": "assistant", "content": "5.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: You should evaluate your language model on marginal likelihood over tokenisations\nAbstract: Neural language models typically tokenise input text into sub-word units to achieve an open vocabulary. The standard approach is to use a single canonical tokenisation at both train and test time. We suggest that this approach is unsatisfactory and may bottleneck our evaluation of language model performance. Using only the one-best tokenisation ignores tokeniser uncertainty over alternative tokenisations, which may hurt model out-of-domain performance. In this paper, we argue that instead, language models should be evaluated on their marginal likelihood over tokenisations. We compare different estimators for the marginal likelihood based on sampling, and show that it is feasible to estimate the marginal likelihood with a manageable number of samples. We then evaluate a pretrained language model on both the one-best-tokenisation and marginal perplexities, and show that the marginal perplexity can be significantly better than the one best, especially on out-of-domain data. We link this difference in perplexity to the tokeniser uncertainty as measured by tokeniser entropy. We discuss some implications of our results for language model training and evaluation, particularly with regard to tokenisation robustness."}, {"role": "assistant", "content": "2.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: When Do You Need Billions of Words of Pretraining Data?\nAbstract: NLP is currently dominated by language models like RoBERTa which are pretrained on billions of words. But what exact knowledge or skills do Transformer LMs learn from large-scale pretraining that they cannot learn from less data? To explore this question, we adopt five styles of evaluation: classifier probing, information-theoretic probing, unsupervised relative acceptability judgments, unsupervised language model knowledge probing, and fine-tuning on NLU tasks. We then draw learning curves that track the growth of these different measures of model ability with respect to pretraining data volume using the MiniBERTas, a group of RoBERTa models pretrained on 1M, 10M, 100M and 1B words. We find that these LMs require only about 10M to 100M words to learn to reliably encode most syntactic and semantic features we test. They need a much larger quantity of data in order to acquire enough commonsense knowledge and other skills required to master typical downstream NLU tasks. The results suggest that, while the ability to encode linguistic features is almost certainly necessary for language understanding, it is likely that other, unidentified, forms of knowledge are the major drivers of recent improvements in language understanding among large pretrained models."}, {"role": "assistant", "content": "2.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: BARTScore: Evaluating Generated Text as Text Generation\nAbstract: A wide variety of NLP applications, such as machine translation, summarization, and dialog, involve text generation. One major challenge for these applications is how to evaluate whether such generated texts are actually fluent, accurate, or effective. In this work, we conceptualize the evaluation of generated text as a text generation problem, modeled using pre-trained sequence-to-sequence models. The general idea is that models trained to convert the generated text to/from a reference output or the source text will achieve higher scores when the generated text is better. We operationalize this idea using BART [27], an encoder-decoder based pre-trained model, and propose a metric BARTSCORE with a number of variants that can be flexibly applied in an unsupervised fashion to evaluation of text from different perspectives (e.g. informativeness, fluency, or factuality). BARTSCORE is conceptually simple and empirically effective. It can outperform existing top-scoring metrics in 16 of 22 test settings, covering evaluation of 16 datasets (e.g., machine translation, text summarization) and 7 different perspectives (e.g., informativeness, factuality). Code to calculate BARTScore is available at https://github.com/neulab/BARTScore, and we have released an interactive leaderboard for meta-evaluation at http: //explainaboard.nlpedia.ai/leaderboard/task-meval/ on the EXPLAINABOARD platform [32], which allows us to interactively understand the strengths, weaknesses, and complementarity of each metric."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: LOREN: Logic-Regularized Reasoning for Interpretable Fact Verification\nAbstract: Given a natural language statement, how to verify its veracity against a large-scale textual knowledge source like Wikipedia? Most existing neural models make predictions without giving clues about which part of a false claim goes wrong. In this paper, we propose LOREN, an approach for interpretable fact verification. We decompose the verification of the whole claim at phrase-level, where the veracity of the phrases serves as explanations and can be aggregated into the final verdict according to logical rules. The key insight of LOREN is to represent claim phrase veracity as three-valued latent variables, which are regularized by aggregation logical rules. The final claim verification is based on all latent variables. Thus, LOREN enjoys the additional benefit of interpretability \u2014 it is easy to explain how it reaches certain results with claim phrase veracity. Experiments on a public fact verification benchmark show that LOREN is competitive against previous approaches while enjoying the merit of faithful and accurate interpretability. The resources of LOREN are available at: https://github.com/jiangjiechen/LOREN."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Data-to-text Generation by Splicing Together Nearest Neighbors\nAbstract: We propose to tackle data-to-text generation tasks by directly splicing together retrieved segments of text from \u201cneighbor\u201d source-target pairs. Unlike recent work that conditions on retrieved neighbors but generates text token-by-token, left-to-right, we learn a policy that directly manipulates segments of neighbor text, by inserting or replacing them in partially constructed generations. Standard techniques for training such a policy require an oracle derivation for each generation, and we prove that finding the shortest such derivation can be reduced to parsing under a particular weighted context-free grammar. We find that policies learned in this way perform on par with strong baselines in terms of automatic and human evaluation, but allow for more interpretable and controllable generation."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Multimodal Conditional Image Synthesis with Product-of-Experts GANs\nAbstract: Existing conditional image synthesis frameworks generate images based on user inputs in a single modality, such as text, segmentation, sketch, or style reference. They are often unable to leverage multimodal user inputs when available, which reduces their practicality. To address this limitation, we propose the Product-of-Experts Generative Adversarial Networks (PoE-GAN) framework, which can synthesize images conditioned on multiple input modalities or any subset of them, even the empty set. PoE-GAN consists of a productof-experts generator and a multimodal multiscale projection discriminator. Through our carefully designed training scheme, PoE-GAN learns to synthesize images with high quality and diversity. Besides advancing the state of the art in multimodal conditional image synthesis, PoE-GAN also outperforms the best existing unimodal conditional image synthesis approaches when tested in the unimodal setting. The project website is available at this link."}, {"role": "assistant", "content": "2.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Adversarial Semantic Collisions\nAbstract: We study semantic collisions: texts that are semantically unrelated but judged as similar by NLP models. We develop gradient-based approaches for generating semantic collisions and demonstrate that state-of-the-art models for many tasks which rely on analyzing the meaning and similarity of texts-- including paraphrase identification, document retrieval, response suggestion, and extractive summarization-- are vulnerable to semantic collisions. For example, given a target query, inserting a crafted collision into an irrelevant document can shift its retrieval rank from 1000 to top 3. We show how to generate semantic collisions that evade perplexity-based filtering and discuss other potential mitigations. Our code is available at https://github.com/csong27/collision-bert."}, {"role": "assistant", "content": "3.5"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Improving Diversity of Neural Text Generation via Inverse Probability Weighting\nAbstract: The neural network based text generation suffers from the text degeneration issue such as repetition. Although top-k sampling and nucleus sampling outperform beam search based decoding methods, they only focus on truncating the \u201ctail\u201d of the distribution and do not address the \u201chead\u201d part, which we show might contain tedious or even repetitive candidates with high probability that lead to repetition loops. They also do not fully address the issue that human text does not always favor high probability words. To explore improved diversity for text generation, we propose a heuristic sampling method inspired by inverse probability weighting. We propose to use interquartile range of the predicted distribution to determine the \u201chead\u201d part, then permutate and rescale the \u201chead\u201d with inverse probability. This aims at decreasing the probability for the tedious and possibly repetitive candidates with higher probability, and increasing the probability for the rational but more surprising candidates with lower probability. The proposed algorithm provides a controllable variation on the predicted distribution which enhances diversity without compromising rationality of the distribution. We use pre-trained language model to compare our algorithm with nucleus sampling. Results show that our algorithm can effectively increase the diversity of generated samples while achieving close resemblance to human text."}, {"role": "assistant", "content": "4.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Progressive Transfer Learning\nAbstract: Model fine-tuning is a widely used transfer learning approach in person Re-identification (ReID) applications, which fine-tuning a pre-trained feature extraction model into the target scenario instead of training a model from scratch. It is challenging due to the significant variations inside the target scenario, e.g., different camera viewpoint, illumination changes, and occlusion. These variations result in a gap between each mini-batch\u2019s distribution and the whole dataset\u2019s distribution when using mini-batch training. In this paper, we study model fine-tuning from the perspective of the aggregation and utilization of the dataset\u2019s global information when using mini-batch training. Specifically, we introduce a novel network structure called Batch-related Convolutional Cell (BConv-Cell), which progressively collects the dataset\u2019s global information into a latent state and uses it to rectify the extracted feature. Based on BConv-Cells, we further proposed the Progressive Transfer Learning (PTL) method to facilitate the model fine-tuning process by jointly optimizing BConv-Cells and the pre-trained ReID model. Empirical experiments show that our proposal can greatly improve the ReID model\u2019s performance on MSMT17, Market-1501, CUHK03, and DukeMTMC-reID datasets. Moreover, we extend our proposal to the general image classification task. The experiments in several image classification benchmark datasets demonstrate that our proposal can significantly improve baseline models\u2019 performance. The code has been released at https://github.com/ZJULearning/PTL"}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: How much pretraining data do language models need to learn syntax?\nAbstract: Transformers-based pretrained language models achieve outstanding results in many well-known NLU benchmarks. However, while pretraining methods are very convenient, they are expensive in terms of time and resources. This calls for a study of the impact of pretraining data size on the knowledge of the models. We explore this impact on the syntactic capabilities of RoBERTa, using models trained on incremental sizes of raw text data. First, we use syntactic structural probes to determine whether models pretrained on more data encode a higher amount of syntactic information. Second, we perform a targeted syntactic evaluation to analyze the impact of pretraining data size on the syntactic generalization performance of the models. Third, we compare the performance of the different models on three downstream applications: part-of-speech tagging, dependency parsing and paraphrase identification. We complement our study with an analysis of the cost-benefit trade-off of training such models. Our experiments show that while models pretrained on more data encode more syntactic knowledge and perform better on downstream applications, they do not always offer a better performance across the different syntactic phenomena and come at a higher financial and environmental cost."}, {"role": "assistant", "content": "4.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Natural Language Understanding with Privacy-Preserving BERT\nAbstract: Privacy preservation remains a key challenge in data mining and Natural Language Understanding (NLU). Previous research shows that the input text or even text embeddings can leak private information. This concern motivates our research on effective privacy preservation approaches for pretrained Language Models (LMs). We investigate the privacy and utility implications of applying d\u03c7-privacy, a variant of Local Differential Privacy, to BERT fine-tuning in NLU applications. More importantly, we further propose privacy-adaptive LM pretraining methods and show that our approach can boost the utility of BERT dramatically while retaining the same level of privacy protection. We also quantify the level of privacy preservation and provide guidance on privacy configuration. Our experiments and findings lay the groundwork for future explorations of privacy-preserving NLU with pretrained LMs."}, {"role": "assistant", "content": "4.75"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: Increasing Network Visibility Using Coded Repetition Beacon Piggybacking\nAbstract: Vehicular Ad hoc Networks (VANET) is one of the most challenging research areas in the field of Mobile Ad Hoc Networks. In this research, we propose a new mechanism for increasing network visibility, by taking the information gained from periodic safety messages (beacons), and inserting it into a 'neighbor' table. The table will be propagated to all neighbors giving a wider vision for each vehicle belonging to the network. It will also decrease the risk of collision at road junctions as each vehicle will have prior knowledge oncoming vehicles before reaching the junction."}, {"role": "assistant", "content": "2.0"}]}
{"messages": [{"role": "system", "content": "You are a knowledgeable reviewer. Rate the expertise of the paper based on its title and abstract. Provide a rating as a single number, between 1 and 5, where 1 represents low expertise and 5 represents high expertise. The rating should be a floating-point number with up to two decimal places. Your response must only be the rating score."}, {"role": "user", "content": "Title: SimVLM: Simple Visual Language Model Pretraining with Weak Supervision\nAbstract: With recent progress in joint modeling of visual and textual representations, Vision-Language Pretraining (VLP) has achieved impressive performance on many multimodal downstream tasks. However, the requirement for expensive annotations including clean image captions and regional labels limits the scalability of existing approaches, and complicates the pretraining procedure with the introduction of multiple dataset-specific objectives. In this work, we relax these constraints and present a minimalist pretraining framework, named Simple Visual Language Model (SimVLM). Unlike prior work, SimVLM reduces the training complexity by exploiting large-scale weak supervision, and is trained end-to-end with a single prefix language modeling objective. Without utilizing extra data or task-specific customization, the resulting model significantly outperforms previous pretraining methods and achieves new state-of-the-art results on a wide range of discriminative and generative vision-language benchmarks, including VQA (+3.74% vqa-score), NLVR2 (+1.17% accuracy), SNLI-VE (+1.37% accuracy) and image captioning tasks (+10.1% average CIDEr score). Furthermore, we demonstrate that SimVLM acquires strong generalization and transfer ability, enabling zero-shot behavior including open-ended visual question answering and cross-modality transfer."}, {"role": "assistant", "content": "2.0"}]}
